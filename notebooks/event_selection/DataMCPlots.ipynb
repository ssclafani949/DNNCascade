{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# set env flags to catch BLAS used for scipy/numpy \n",
    "# to only use 1 cpu, n_cpus will be totally controlled by csky\n",
    "os.environ['MKL_NUM_THREADS'] = \"1\"\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = \"1\"\n",
    "os.environ['OMP_NUM_THREADS'] = \"1\"\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = \"1\"\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'w'\n",
    "mpl.rcParams['savefig.facecolor'] = 'w'\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "# suppress natural naming warnings\n",
    "import warnings\n",
    "from tables import NaturalNameWarning\n",
    "warnings.filterwarnings('ignore', category=NaturalNameWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-antarctica",
   "metadata": {},
   "source": [
    "## Defines Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_version = 'version-001-p01'\n",
    "\n",
    "plot_dir = '/home/mhuennefeld/public_html/analyses/DNNCascade/plots/data_mc/selection_{}'.format(selection_version)\n",
    "df_dir = '/data/ana/PointSource/DNNCascade/analysis/{}/'.format(selection_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_path in [plot_dir]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        print('Creating directory:', dir_path)\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-sunglasses",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "print('Loading BFRv1 ...')\n",
    "dfs['BFRv1'] = pd.read_hdf(\n",
    "    df_dir + '/MC_NuGen_bfrv1_2153x.hdf', key='df',\n",
    ")\n",
    "\n",
    "print('Loading SnowStorm ...')\n",
    "dfs['SnowStorm']  = pd.read_hdf(\n",
    "    df_dir + '/systematics/SnowStorm_Spice321/MC_NuGen_snowstorm_214xx.hdf', key='df',\n",
    ")\n",
    "\n",
    "print('Loading exp ...')\n",
    "df_exp_list = []\n",
    "for y in range(2011, 2021):\n",
    "     df_exp_list.append(pd.read_hdf(\n",
    "        '{}/IC86_{}_exp.hdf'.format(df_dir, y), key='df',\n",
    "    ))\n",
    "dfs['_exp']  = pd.concat(df_exp_list, ignore_index=True)\n",
    "\n",
    "print('Loading MuonGun ...')\n",
    "dfs['MuonGun']  = pd.read_hdf(\n",
    "    df_dir + '/MC_MuonGun_2131x.hdf', key='df',\n",
    ")\n",
    "\n",
    "print('Loading CORSIKA ...')\n",
    "dfs['CORSIKA']  = pd.read_hdf(\n",
    "    df_dir + '/MC_CORSIKA_20904.hdf', key='df',\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-weight",
   "metadata": {},
   "source": [
    "## Dataset Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "astro_weight = 'weights_cscd_hans'\n",
    "conv_weight = 'weights_MCEq_H3a_sibyll2_3c_conv'\n",
    "prompt_weight = 'weights_MCEq_H3a_sibyll2_3c_pr'\n",
    "atmo_weight = 'weights_MCEq_H3a_sibyll2_3c_total'\n",
    "atmo_pf = 'nuveto_pf_dnn_cascade_selection_H3a_SIBYLL2_3c_total'\n",
    "conv_pf = 'nuveto_pf_dnn_cascade_selection_H3a_SIBYLL2_3c_conv'\n",
    "prompt_pf = 'nuveto_pf_dnn_cascade_selection_H3a_SIBYLL2_3c_pr'\n",
    "\n",
    "\n",
    "def collect_hist_data(dfs, livetime):\n",
    "    weights_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for name, df in dfs.items():\n",
    "        if name == 'SnowStorm': continue\n",
    "        \n",
    "        if astro_weight in df:\n",
    "\n",
    "            label_list.append('NuGen conv ({})'.format(name))\n",
    "            weights_list.append(df[conv_weight] * df[conv_pf]*livetime)\n",
    "\n",
    "            label_list.append('NuGen prompt ({})'.format(name))\n",
    "            weights_list.append(df[prompt_weight] * df[prompt_pf]*livetime)\n",
    "\n",
    "            label_list.append('NuGen atmo ({})'.format(name))\n",
    "            weights_list.append(df[atmo_weight] * df[atmo_pf]*livetime)\n",
    "            \n",
    "            label_list.append('NuGen astro ({})'.format(name))\n",
    "            weights_list.append(df[astro_weight]*livetime)\n",
    "        \n",
    "        if 'weights' in df:\n",
    "            label_list.append(name)\n",
    "            weights_list.append(df['weights'])\n",
    "            \n",
    "    \n",
    "    # Total MC\n",
    "    num_events = 0\n",
    "    for mc_name in ['BFRv1', 'CORSIKA', 'MuonGun']:\n",
    "        num_events += np.sum(dfs[mc_name]['weights'])\n",
    "    label_list.append('Total MC')\n",
    "    weights_list.append(num_events)\n",
    "    \n",
    "    # Exp data\n",
    "    label_list.append('Exp. Data')\n",
    "    weights_list.append(np.ones_like(dfs['_exp']['runs']))\n",
    "        \n",
    "    return weights_list, label_list\n",
    "\n",
    "# get livetime\n",
    "livetime = dfs['BFRv1']['weights_livetime']\n",
    "assert len(np.unique(livetime)) == 1\n",
    "livetime = livetime[0]\n",
    "\n",
    "# get total MC\n",
    "mc_total = 0\n",
    "for mc_name in ['BFRv1', 'CORSIKA', 'MuonGun']:\n",
    "    mc_total += np.sum(dfs[mc_name]['weights'])\n",
    "    \n",
    "weights_list, label_list = collect_hist_data(dfs, livetime=livetime)\n",
    "\n",
    "\n",
    "print('Data Rates:')\n",
    "for weights, label in zip(weights_list, label_list):\n",
    "    weight_sum = np.sum(weights)\n",
    "    fraction = weight_sum / mc_total\n",
    "    print('  {:9.2f} events | {:3.3f} mHz | {:6.2f} %| {}'.format(\n",
    "        weight_sum, weight_sum * 1000 / livetime, \n",
    "        fraction * 100, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-marketing",
   "metadata": {},
   "source": [
    "## Livetime and Burnsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_burn = dfs['_exp']['I3EventHeader_Run'] % 10 == 0\n",
    "dfs['exp'] = dfs['_exp'][mask_burn]\n",
    "\n",
    "burnsample_fraction = np.sum(mask_burn) / len(dfs['_exp'])\n",
    "print('Burn sample fraction: {:3.3f}%'.format(burnsample_fraction * 100))\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    if 'exp' not in name:\n",
    "        print('Adjusting weights for: {}'.format(name))\n",
    "        df['weights_new'] = df['weights'] * burnsample_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "_livetime = dfs['BFRv1']['weights_livetime'].iloc[0]\n",
    "livetime = _livetime * burnsample_fraction\n",
    "print('Livetime: {} days'.format(_livetime / 60 / 60 / 24))\n",
    "print('Livetime [Burnsample]: {} days'.format(livetime / 60 / 60 / 24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-strength",
   "metadata": {},
   "source": [
    "## Snowstorm Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_dict = {\n",
    "    #'Absorption': [0.930, 1.070], #[0.9, 1.0],\n",
    "    #'Scattering': [0.953, 1.012], #[0.9, 1.1],\n",
    "    #'AnisotropyScale': [0, 2], #[0., 2.],\n",
    "    #'DOMEfficiency': [0.9, 1.1],\n",
    "    #'HoleIceForward_Unified_00': [-0.800, 0.800], #[-0.65, 0.65],\n",
    "    #'HoleIceForward_Unified_01': [-0.120, -0.040], #[-0.2, 0.2],\n",
    "}\n",
    "snowstorm_simulation_range = {\n",
    "    'Scattering': [0.9, 1.1],\n",
    "    'Absorption': [0.9, 1.1],\n",
    "    'AnisotropyScale': [0., 2.],\n",
    "    'DOMEfficiency': [0.9, 1.1],\n",
    "    'HoleIceForward_Unified_00': [-1.0, 1.0],\n",
    "    'HoleIceForward_Unified_01': [-0.2, 0.2],\n",
    "}\n",
    "\n",
    "def rename_snowstorm_params(df):\n",
    "    parameter_names=[\n",
    "        'Scattering', 'Absorption', 'AnisotropyScale', \n",
    "        'DOMEfficiency', 'HoleIceForward_Unified_00', \n",
    "        'HoleIceForward_Unified_01',\n",
    "    ]\n",
    "    for i, param in enumerate(parameter_names):\n",
    "        df[param] = df['SnowstormParameters_{:05d}'.format(i)]\n",
    "\n",
    "def gauss(x, mu, sigma):\n",
    "    \"\"\"Gaussian PDF\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        The input tensor.\n",
    "    mu : array_like\n",
    "        Mu parameter of Gaussian.\n",
    "    sigma : array_like\n",
    "        Sigma parameter of Gaussian.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        The Gaussian PDF evaluated at x\n",
    "    \"\"\"\n",
    "    return np.exp(-0.5*((x - mu) / sigma)**2) / (2*np.pi*sigma**2)**0.5\n",
    "\n",
    "def get_snowstorm_multiplier_gaussian(\n",
    "        df,\n",
    "        priors_dict,\n",
    "        simulation_range=snowstorm_simulation_range,\n",
    "    ):\n",
    "    \"\"\"Reweight snowstorm with Gaussian\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame or dict\n",
    "        The dataframe or dictionary containing the SnowStorm\n",
    "        parameters. \n",
    "    priors_dict : dict\n",
    "        A dictionary with uniform Snowstorm priors defined\n",
    "        as a tuple of (min, max). Gaussian will be placed in \n",
    "        center with 2-sigma corresponding to boundaries, i.e.\n",
    "        sigma = (max - min) / 4.\n",
    "    \"\"\"\n",
    "    w_multiplier = np.ones_like(df[list(simulation_range.keys())[0]])\n",
    "    for name, prior in priors_dict.items():\n",
    "        \n",
    "        prior_orig = simulation_range[name]\n",
    "        assert prior[1] >= prior[0] and prior_orig[1] >= prior_orig[0]\n",
    "        assert prior[0] >= prior_orig[0] and prior[0] <= prior_orig[1]\n",
    "        assert prior[1] >= prior_orig[0] and prior[1] <= prior_orig[1]\n",
    "        \n",
    "        range_sim = prior_orig[1] - prior_orig[0]\n",
    "        range_new = prior[1] - prior[0]\n",
    "        sigma = range_new / 4.\n",
    "        mu = np.mean(prior)\n",
    "        w_multiplier *= gauss(x=df[name], mu=mu, sigma=sigma)\n",
    "    \n",
    "    # normalize weights\n",
    "    w_multiplier = w_multiplier / np.sum(w_multiplier) * len(w_multiplier)\n",
    "    return w_multiplier\n",
    "\n",
    "def get_snowstorm_multiplier(\n",
    "        df,\n",
    "        priors_dict,\n",
    "        simulation_range=snowstorm_simulation_range,\n",
    "        verbose=False,\n",
    "    ):\n",
    "    \"\"\"Reweight snowstorm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame or dict\n",
    "        The dataframe or dictionary containing the SnowStorm\n",
    "        parameters. \n",
    "    priors_dict : dict\n",
    "        A dictionary with uniform Snowstorm priors defined\n",
    "        as a tuple of (min, max).\n",
    "    \"\"\"\n",
    "    w_multiplier = 1.0\n",
    "    mask = np.ones_like(df[list(simulation_range.keys())[0]], dtype=bool)\n",
    "    for name, prior in priors_dict.items():\n",
    "        \n",
    "        prior_orig = simulation_range[name]\n",
    "        assert prior[1] >= prior[0] and prior_orig[1] >= prior_orig[0]\n",
    "        assert prior[0] >= prior_orig[0] and prior[0] <= prior_orig[1]\n",
    "        assert prior[1] >= prior_orig[0] and prior[1] <= prior_orig[1]\n",
    "        \n",
    "        range_sim = prior_orig[1] - prior_orig[0]\n",
    "        range_new = prior[1] - prior[0]\n",
    "        w_multiplier *= range_sim / range_new\n",
    "        mask = np.logical_and(mask, df[name] >= prior[0])\n",
    "        mask = np.logical_and(mask, df[name] <= prior[1])\n",
    "    \n",
    "    \n",
    "    snowstorm_multiplier = np.ones_like(mask) * mask.astype(float) * w_multiplier\n",
    "    if verbose:\n",
    "        print(np.sum(mask) / float(len(mask)), 1./w_multiplier, w_multiplier)\n",
    "        print(np.sum(mask), len(mask), np.sum(snowstorm_multiplier))\n",
    "    return snowstorm_multiplier\n",
    "        \n",
    "if 'SnowStorm' in dfs:\n",
    "    print('Reweighting Snowstorm set')\n",
    "    rename_snowstorm_params(dfs['SnowStorm'])\n",
    "    dfs['SnowStorm']['snowstorm_multiplier'] = get_snowstorm_multiplier(\n",
    "        df=dfs['SnowStorm'],\n",
    "        priors_dict=priors_dict,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-celebrity",
   "metadata": {},
   "source": [
    "## Add Cos(Zenith) and sindec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs.values():\n",
    "    df['sindec'] = np.sin(df['dec'])\n",
    "    for key in df.keys():\n",
    "        if key[-17:] == 'I3Particle_zenith':\n",
    "            df[key[:-6] + 'cos_zen'] = np.cos(df[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-destruction",
   "metadata": {},
   "source": [
    "## Perform Pseudo Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "\n",
    "def power_law(energy, norm, gamma, e_pivot=1e5):\n",
    "    n_types = 2  # Dividing by n_types gives flux per flavor and per type\n",
    "    return norm * np.power(energy/e_pivot, gamma) / n_types\n",
    "\n",
    "def get_astro_weights(df, norm, gamma):\n",
    "    ow = df['I3MCWeightDict_OneWeight']\n",
    "    type_weight = df['I3MCWeightDict_TypeWeight']\n",
    "    energy = df['I3MCWeightDict_PrimaryNeutrinoEnergy']\n",
    "    n_events = df['I3MCWeightDict_NEvents']\n",
    "    n_files = df['weights_meta_info_n_files']\n",
    "    \n",
    "    flux_val = power_law(\n",
    "        energy=energy, \n",
    "        norm=norm, \n",
    "        gamma=-gamma, \n",
    "    )\n",
    "\n",
    "    astro_weights = flux_val * ow / (\n",
    "        type_weight * n_events * n_files)\n",
    "\n",
    "    return astro_weights\n",
    "\n",
    "\n",
    "sys_scale = 1e4\n",
    "w = 1.\n",
    "snowstorm_priors_width = {\n",
    "    'Scattering': 0.05*w,\n",
    "    'Absorption': 0.05*w,\n",
    "    'AnisotropyScale': 0.5*w,\n",
    "    'DOMEfficiency': 0.05*w,\n",
    "    'HoleIceForward_Unified_00': 0.5*w,\n",
    "    'HoleIceForward_Unified_01': 0.1*w,\n",
    "}\n",
    "\n",
    "def get_hists(\n",
    "            bins_energy, bins_sindec, params, \n",
    "            nugen_key='BFRv1', \n",
    "            priors_width=snowstorm_priors_width, \n",
    "            livetime=_livetime,\n",
    "            use_gaussian=True,\n",
    "        ):\n",
    "    hists = {}\n",
    "    for key, df in dfs.items():\n",
    "        hist_i = None\n",
    "        if key == '_exp':\n",
    "            hist_i, _, _ = np.histogram2d(\n",
    "                df['energy'], df['sindec'],\n",
    "                bins=[bins_energy, bins_sindec],\n",
    "            )\n",
    "        elif key in ['MuonGun', 'CORSIKA']:\n",
    "            hist_i, _, _ = np.histogram2d(\n",
    "                df['energy'], df['sindec'],\n",
    "                bins=[bins_energy, bins_sindec],\n",
    "                weights=df['weights'] * params[key],\n",
    "            )\n",
    "        elif key == 'BFRv1' and nugen_key == 'BFRv1':\n",
    "            conv_weights = df[conv_weight] * df[conv_pf] * livetime * params['conv']\n",
    "            prompt_weights = df[prompt_weight] * df[prompt_pf] * livetime * params['prompt']\n",
    "            astro_weights = get_astro_weights(df, norm=params['astro'], gamma=params['gamma']) * livetime\n",
    "            hist_i, _, _ = np.histogram2d(\n",
    "                df['energy'], df['sindec'],\n",
    "                bins=[bins_energy, bins_sindec],\n",
    "                weights=conv_weights + prompt_weights + astro_weights,\n",
    "            )\n",
    "        elif key == 'SnowStorm' and nugen_key == 'SnowStorm':\n",
    "            conv_weights = df[conv_weight] * df[conv_pf] * livetime * params['conv']\n",
    "            prompt_weights = df[prompt_weight] * df[prompt_pf] * livetime * params['prompt']\n",
    "            astro_weights = get_astro_weights(df, norm=params['astro'], gamma=params['gamma']) * livetime\n",
    "            snowstorm_weights = conv_weights + prompt_weights + astro_weights\n",
    "            \n",
    "            # extract SnowStorm priors\n",
    "            priors_dict = {}\n",
    "            for k, v in params.items():\n",
    "                if k in snowstorm_simulation_range:\n",
    "                    priors_dict[k] = [params[k] - priors_width[k], params[k] + priors_width[k]]\n",
    "            \n",
    "            # get multipliers for set of systematic paramters\n",
    "            if use_gaussian:\n",
    "                snowstorm_multiplier = get_snowstorm_multiplier_gaussian(\n",
    "                    df=df, priors_dict=priors_dict,\n",
    "                )\n",
    "            else:\n",
    "                snowstorm_multiplier = get_snowstorm_multiplier(\n",
    "                    df=df, priors_dict=priors_dict, verbose=False,\n",
    "                )\n",
    "            \n",
    "            hist_i, _, _ = np.histogram2d(\n",
    "                df['energy'], df['sindec'],\n",
    "                bins=[bins_energy, bins_sindec],\n",
    "                weights=snowstorm_weights * snowstorm_multiplier,\n",
    "            )\n",
    "        else:\n",
    "            pass\n",
    "            #print('Ignoring:', key)\n",
    "        \n",
    "        if hist_i is not None:\n",
    "            hists[key] = hist_i\n",
    "            \n",
    "    return hists\n",
    "\n",
    "\n",
    "def compute_likelihood(params_vector, bins_energy, bins_sindec, nugen_key, sys_names, priors_width, use_gaussian):\n",
    "    params = {\n",
    "        'MuonGun': params_vector[0],\n",
    "        'CORSIKA': params_vector[1],\n",
    "        'conv': params_vector[2],\n",
    "        'prompt': params_vector[3],\n",
    "        'astro': params_vector[4] * 1e-18,\n",
    "        'gamma': params_vector[5],\n",
    "    }\n",
    "    if nugen_key == 'SnowStorm':\n",
    "        for i, sys_name in enumerate(sys_names):\n",
    "            prior_range = 0.5 * (snowstorm_simulation_range[sys_name][1] - snowstorm_simulation_range[sys_name][0]) - priors_width[sys_name]\n",
    "            params[sys_name] = snowstorm_simulation_range[sys_name][0] + priors_width[sys_name] + params_vector[6 + i] * prior_range * sys_scale\n",
    "\n",
    "    hists = get_hists(\n",
    "        bins_energy=bins_energy, bins_sindec=bins_sindec, params=params, \n",
    "        nugen_key=nugen_key, priors_width=priors_width, use_gaussian=use_gaussian,\n",
    "    )\n",
    "\n",
    "    # accumulate MC contributions\n",
    "    hist_mc = hists['MuonGun'] + hists['CORSIKA'] + hists[nugen_key]\n",
    "    \n",
    "    # compute negative Poisson Likelihood (without constant terms)\n",
    "    neg_llh = hist_mc - hists['_exp'] * np.log(hist_mc)\n",
    "    print('params', params)\n",
    "    print(np.sum(neg_llh))\n",
    "    return np.sum(neg_llh)\n",
    "\n",
    "def perform_fit(\n",
    "            bins_energy, bins_sindec, \n",
    "            nugen_key='BFRv1', \n",
    "            sys_names=[], \n",
    "            priors_width=snowstorm_priors_width, \n",
    "            use_gaussian=True,\n",
    "            x0=None,\n",
    "            **minimize_kwargs,\n",
    "        ):\n",
    "    \n",
    "    bounds = [\n",
    "        [0., np.inf],  # MuonGun\n",
    "        [0., np.inf],  # CORSIKA\n",
    "        [0., np.inf],  # conv\n",
    "        [0., np.inf],  # prompt\n",
    "        [0., np.inf],  # astro\n",
    "        [1., 4.],      # gamma\n",
    "    ]\n",
    "    _x0 = [1., 1., 1., 1., 1., 2.6]\n",
    "    \n",
    "    if nugen_key == 'SnowStorm':\n",
    "        for i, sys_name in enumerate(sys_names):\n",
    "            # for whatever reason the minimizer makes very small steps.\n",
    "            # Therefore sys needs to be made more sensitive to small values\n",
    "            bounds.append([0, 2 / sys_scale]) \n",
    "            _x0.append(1./sys_scale)\n",
    "    \n",
    "    if x0 is None:\n",
    "        x0 = _x0\n",
    "        \n",
    "    res = optimize.minimize(\n",
    "        compute_likelihood, \n",
    "        x0=x0, \n",
    "        args=(bins_energy, bins_sindec, nugen_key, sys_names, priors_width, use_gaussian),\n",
    "        bounds=bounds,\n",
    "        **minimize_kwargs\n",
    "    )\n",
    "    \n",
    "    params = {\n",
    "        'MuonGun': res.x[0],\n",
    "        'CORSIKA': res.x[1],\n",
    "        'conv': res.x[2],\n",
    "        'prompt': res.x[3],\n",
    "        'astro': res.x[4] * 1e-18,\n",
    "        'gamma': res.x[5],\n",
    "    }\n",
    "    if nugen_key == 'SnowStorm':\n",
    "        for i, sys_name in enumerate(sys_names):\n",
    "            prior_range = 0.5 * (snowstorm_simulation_range[sys_name][1] - snowstorm_simulation_range[sys_name][0]) - priors_width[sys_name]\n",
    "            params[sys_name] = snowstorm_simulation_range[sys_name][0] + priors_width[sys_name] + res.x[6 + i] * prior_range * sys_scale\n",
    "\n",
    "    return res, params\n",
    "\n",
    "\n",
    "\n",
    "bins_sindec = np.linspace(-1, 1, 15)\n",
    "#bins_energy = 10**np.r_[2.5, 2.7:4.501:0.10, 4.6, 4.7, 4.8, 5., 5.5, 8.]\n",
    "bins_energy = 10**np.r_[2.5, 2.8:4.401:0.10, 4.7, 5, 8.]\n",
    "bins_energy = 10**np.r_[2.5, 2.8:4.401:0.30, 4.7, 5, 8.]\n",
    "#bins_energy = 10**np.r_[2.5, 8.]\n",
    "params_test = {\n",
    "    'MuonGun': 1.,\n",
    "    'CORSIKA': 1.,\n",
    "    'conv': 1.,\n",
    "    'prompt': 1.,\n",
    "    'astro': 1,\n",
    "    'gamma': 2.6,\n",
    "}\n",
    "sys_names = [\n",
    "    'Scattering',\n",
    "    'Absorption',\n",
    "    'AnisotropyScale',\n",
    "    'DOMEfficiency',\n",
    "    'HoleIceForward_Unified_00',\n",
    "    'HoleIceForward_Unified_01',   \n",
    "]\n",
    "print('bins_sindec', bins_sindec)\n",
    "print('bins_energy', bins_energy)\n",
    "print(get_hists(bins_energy, bins_sindec, params=params_test)['_exp'])\n",
    "#norm_list=[1.0e-18, 1.5e-18, 2.0e-18], \n",
    "#gamma_list=[2.6, 2.7, 2.8],\n",
    "#params_vector = [1, 1, 1, 1, 1, 2.6]\n",
    "p = 1./sys_scale\n",
    "params_vector = [1, 1, 1, 1, 1, 2.6, p, p, p, p, p, p]\n",
    "for b in [True, False]:\n",
    "    print(compute_likelihood(\n",
    "        params_vector=params_vector, bins_energy=bins_energy, bins_sindec=bins_sindec, \n",
    "        nugen_key='SnowStorm', sys_names=sys_names, priors_width=snowstorm_priors_width,\n",
    "        use_gaussian=b,\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-october",
   "metadata": {},
   "source": [
    "#### Check impact of SnowStorm Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-creativity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_sys_impact_plot(key_to_plot, bins, params=params_test, xscale=None, yscale=None, livetime=_livetime):\n",
    "    df = dfs['SnowStorm']\n",
    "    conv_weights = df[conv_weight] * df[conv_pf] * livetime * params['conv']\n",
    "    prompt_weights = df[prompt_weight] * df[prompt_pf] * livetime * params['prompt']\n",
    "    astro_weights = get_astro_weights(df, norm=params['astro'], gamma=params['gamma']) * livetime\n",
    "    snowstorm_weights = conv_weights + prompt_weights + astro_weights\n",
    "\n",
    "    for sys_name, bounds in snowstorm_simulation_range.items():\n",
    "\n",
    "        fig, (ax, ax_r) = plt.subplots(2, 1, sharex=True, figsize=(9, 6))\n",
    "\n",
    "        width = snowstorm_priors_width[sys_name]\n",
    "        mids = np.linspace(\n",
    "            bounds[0] + 0.5 * width, bounds[1] - 0.5 * width, 5\n",
    "        )\n",
    "        h_base, _, _ = ax.hist(\n",
    "            df[key_to_plot], bins=bins, weights=snowstorm_weights ,\n",
    "            histtype='step', label='Baseline', color='0.8', ls='--',\n",
    "        )\n",
    "        bin_mids = bins[:-1] + 0.5 * np.diff(bins)\n",
    "        print('Base', np.sum(snowstorm_weights))\n",
    "\n",
    "\n",
    "        for mid in mids:\n",
    "            priors_dict_i = {sys_name: [mid - 0.5 * width, mid + 0.5 * width]}\n",
    "\n",
    "            snowstorm_multiplier = get_snowstorm_multiplier(\n",
    "                df=df, priors_dict=priors_dict_i, verbose=False,\n",
    "            )\n",
    "            print(np.sum(snowstorm_weights * snowstorm_multiplier))\n",
    "            h, _, _ = ax.hist(\n",
    "                df[key_to_plot], bins=bins, weights=snowstorm_weights * snowstorm_multiplier,\n",
    "                histtype='step', label='{} = {:3.3f}'.format(sys_name, mid),\n",
    "            )\n",
    "            ax_r.plot(bin_mids, h / h_base, label='{} = {:3.3f}'.format(sys_name, mid))\n",
    "\n",
    "        ax.legend()\n",
    "        ax_r.legend()\n",
    "        ax.set_xscale(xscale)\n",
    "        ax.set_yscale(yscale)\n",
    "\n",
    "make_sys_impact_plot(key_to_plot='energy', bins=np.logspace(2, 7, 30), xscale='log', yscale='log')\n",
    "make_sys_impact_plot(key_to_plot='sindec', bins=np.linspace(-1, 1, 30), xscale='linear', yscale='linear')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-crawford",
   "metadata": {},
   "source": [
    "##### Perform Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-silicon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res, params = perform_fit(\n",
    "    bins_energy=bins_energy, bins_sindec=bins_sindec,\n",
    "    nugen_key='SnowStorm', sys_names=sys_names, priors_width=snowstorm_priors_width,\n",
    "    use_gaussian=True,\n",
    "    #method='Nelder-Mead',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-start",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res2, params2 = perform_fit(\n",
    "    bins_energy=bins_energy, bins_sindec=bins_sindec,\n",
    "    nugen_key='SnowStorm', sys_names=sys_names, priors_width=snowstorm_priors_width,\n",
    "    x0=res.x,\n",
    "    use_gaussian=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "params2, res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bfr, params_bfr = perform_fit(\n",
    "    bins_energy=bins_energy, bins_sindec=bins_sindec,\n",
    "    nugen_key='BFRv1', sys_names=sys_names, priors_width=snowstorm_priors_width,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_bfr, res_bfr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-heather",
   "metadata": {},
   "source": [
    "#### Add fitted weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiplier_from_params(df, params, use_gaussian=True, priors_width=snowstorm_priors_width):\n",
    "    \n",
    "    # extract SnowStorm priors\n",
    "    priors_dict = {}\n",
    "    for k, v in params.items():\n",
    "        if k in snowstorm_simulation_range:\n",
    "            priors_dict[k] = [params[k] - priors_width[k], params[k] + priors_width[k]]\n",
    "\n",
    "    # get multipliers for set of systematic paramters\n",
    "    if use_gaussian:\n",
    "        snowstorm_multiplier = get_snowstorm_multiplier_gaussian(\n",
    "            df=df, priors_dict=priors_dict,\n",
    "        )\n",
    "    else:\n",
    "        snowstorm_multiplier = get_snowstorm_multiplier(\n",
    "            df=df, priors_dict=priors_dict, verbose=False,\n",
    "        )\n",
    "    return snowstorm_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_use = params\n",
    "params_to_use = params2\n",
    "\n",
    "# NuGen\n",
    "for key in ['BFRv1', 'SnowStorm']:\n",
    "    df = dfs[key]\n",
    "    weights_conv =  df[conv_weight] * df[conv_pf] * _livetime * params_to_use['conv']\n",
    "    weights_prompt = df[prompt_weight] * df[prompt_pf] * _livetime * params_to_use['prompt']\n",
    "    weights_astro = get_astro_weights(df, norm=params['astro'], gamma=params_to_use['gamma']) * _livetime\n",
    "    weights_fit = weights_conv + weights_prompt + weights_astro\n",
    "    \n",
    "    if key == 'SnowStorm':\n",
    "        weights_fit *= get_multiplier_from_params(df, params=params_to_use)\n",
    "        \n",
    "    dfs[key]['weights_fit'] = weights_fit\n",
    "\n",
    "dfs['MuonGun']['weights_fit'] = dfs['MuonGun']['weights'] * params_to_use['MuonGun']\n",
    "dfs['CORSIKA']['weights_fit'] = dfs['CORSIKA']['weights'] * params_to_use['CORSIKA']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-payment",
   "metadata": {},
   "source": [
    "### Some distribution plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-failing",
   "metadata": {},
   "source": [
    "##### compute weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nugen_key = 'BFRv1'\n",
    "nugen_key = 'SnowStorm'\n",
    "use_gaussian = False\n",
    "\n",
    "df = dfs[nugen_key]\n",
    "weights_conv =  df[conv_weight] * df[conv_pf] * _livetime * params_to_use['conv']\n",
    "weights_prompt = df[prompt_weight] * df[prompt_pf] * _livetime * params_to_use['prompt']\n",
    "weights_astro = get_astro_weights(df, norm=params['astro'], gamma=params_to_use['gamma']) * _livetime\n",
    "\n",
    "if nugen_key == 'SnowStorm':\n",
    "    print('Using Gaussian fitting:', use_gaussian)\n",
    "    snowstorm_multiplier = get_multiplier_from_params(df, params=params_to_use, use_gaussian=use_gaussian)\n",
    "    weights_conv *= snowstorm_multiplier\n",
    "    weights_prompt *= snowstorm_multiplier\n",
    "    weights_astro *= snowstorm_multiplier\n",
    "    \n",
    "weights_muongun = dfs['MuonGun']['weights'] * params_to_use['MuonGun']\n",
    "weights_corsika = dfs['CORSIKA']['weights'] * params_to_use['CORSIKA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.logspace(np.log10(500), 6.3, 50)\n",
    "key = 'energy'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "\n",
    "ax.hist(dfs['_exp'][key], bins=bins, label='Exp Data', histtype='step', color='0.', lw=2)\n",
    "ax.hist(df[key], bins=bins, weights=weights_astro, label='Astrophysical', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_conv, label='Conv', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_prompt, label='Prompt', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_conv + weights_prompt + weights_astro, label='MC Neutrino', histtype='step')\n",
    "ax.hist(dfs['MuonGun'][key], bins=bins, weights=weights_muongun, label='MuonGun', histtype='step')\n",
    "ax.hist(dfs['CORSIKA'][key], bins=bins, weights=weights_corsika, label='CORSIKA', histtype='step')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(1e-1, 3e4)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "#fig.savefig('{}/energy_distribution.png'.format(\n",
    "#    '/data/user/mhuennefeld/data/analyses/DNNCascadeCodeReview/unblinding_checks/plots/unblinding/galactic_plane_checks'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-1., 1., 15)\n",
    "key = 'sindec'\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "ax.hist(dfs['_exp'][key], bins=bins, label='Exp Data', histtype='step', color='0.', lw=2)\n",
    "ax.hist(df[key], bins=bins, weights=weights_astro, label='Astrophysical', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_conv, label='Conv', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_prompt, label='Prompt', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_conv + weights_prompt + weights_astro, label='MC Neutrino', histtype='step')\n",
    "ax.hist(dfs['MuonGun'][key], bins=bins, weights=weights_muongun, label='MuonGun', histtype='step')\n",
    "ax.hist(dfs['CORSIKA'][key], bins=bins, weights=weights_corsika, label='CORSIKA', histtype='step')\n",
    "\n",
    "ax.legend(loc='lower center')\n",
    "#ax.set_ylim(1e-1, 3e4)\n",
    "#ax.set_yscale('log')\n",
    "#fig.savefig('{}/energy_distribution.png'.format(\n",
    "#    '/data/user/mhuennefeld/data/analyses/DNNCascadeCodeReview/unblinding_checks/plots/unblinding/galactic_plane_checks'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-bunny",
   "metadata": {},
   "source": [
    "## Create Combined MC DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "nugen_keys = [nugen_key]\n",
    "muon_keys = ['MuonGun', 'CORSIKA']\n",
    "\n",
    "shared_keys = None\n",
    "for name, df in dfs.items():\n",
    "    if 'exp' not in name:\n",
    "        if shared_keys is None:\n",
    "            shared_keys = set(df.columns.values)\n",
    "        else:\n",
    "            shared_keys = shared_keys.intersection(\n",
    "                set(df.columns.values))\n",
    "\n",
    "df_list = []\n",
    "for name in nugen_keys:\n",
    "    print('NuGen:', name)\n",
    "    df_red = dfs[name][list(shared_keys)]\n",
    "    df_red['mc_origin'] = 'NuGen_' + name\n",
    "    df_list.append(df_red)\n",
    "for name in muon_keys:\n",
    "    print('Muon:', name)\n",
    "    df_red = dfs[name][list(shared_keys)]\n",
    "    df_red['mc_origin'] = 'Muon_' + name\n",
    "    df_list.append(df_red)\n",
    "    \n",
    "df_mc = pd.concat(df_list, ignore_index=True)\n",
    "del df_list\n",
    "print(len(df_mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-being",
   "metadata": {},
   "source": [
    "### Energy Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.logspace(np.log10(500), 6.3, 30)\n",
    "key = 'EventGeneratorSelectedRecoNN_I3Particle_energy'\n",
    "mc_key = 'LabelsDeepLearning_PrimaryEnergy'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "df = dfs[nugen_key]\n",
    "weights_conv = df[conv_weight] * df[conv_pf]*livetime\n",
    "weights_prompt = df[prompt_weight] * df[prompt_pf]*livetime\n",
    "weights_atmo = df[atmo_weight] * df[atmo_pf]*livetime\n",
    "weights_astro = df[astro_weight]*livetime\n",
    "\n",
    "def powerlaw_weights(energy, gamma, norm, e_pivot):\n",
    "    n_types = 2.\n",
    "    return norm * np.power(energy/e_pivot, gamma) / n_types\n",
    "\n",
    "n_events_per_run = df['weights_meta_info_n_events_per_run']\n",
    "n_files = df['weights_meta_info_n_files']\n",
    "\n",
    "# csky GP flux is given in units of:\n",
    "# 1/(``unit`` * GeV)/cm2/s\n",
    "# need to do per solid angle, so divide by 4pi\n",
    "norm = 2.4366279686479028e-18\n",
    "norm = 2.18e-18 # bias corrected\n",
    "norm /= 4 * np.pi\n",
    "\n",
    "flux_pi0 = powerlaw_weights(\n",
    "    energy=df[mc_key], gamma=-2.7, norm=norm, e_pivot=1e5)\n",
    "weights_pi0 = flux_pi0 * df['I3MCWeightDict_OneWeight'] * livetime / (n_events_per_run * n_files)\n",
    "\n",
    "flux_pi0_gamma3 = powerlaw_weights(\n",
    "    energy=df[mc_key], gamma=-3.0, norm=norm, e_pivot=1e5)\n",
    "weights_pi0_gamma3 = flux_pi0_gamma3 * df['I3MCWeightDict_OneWeight'] * livetime / (n_events_per_run * n_files)\n",
    "print('Pi0 events: {}, {}'.format(np.sum(weights_pi0), np.sum(weights_pi0_gamma3)))\n",
    "            \n",
    "ax.hist(dfs['exp'][key], bins=bins, label='Burn Sample', histtype='step', color='0.', lw=2)\n",
    "ax.hist(df[key], bins=bins, weights=weights_atmo, label='Atmospheric', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_astro, label='Astrophysical', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_pi0, label=r'Fitted $\\pi^0$', histtype='step')\n",
    "ax.hist(df[key], bins=bins, weights=weights_pi0_gamma3, \n",
    "        label=r'Fitted $\\pi^0$ norm + $\\gamma=3.$', histtype='step', color='0.6', ls='--')\n",
    "ax.hist(df[key], bins=bins, weights=(weights_atmo + weights_pi0), label='Atmo + $\\pi^0$', \n",
    "        histtype='step', lw=2, ls='--')\n",
    "ax.hist(df[key], bins=bins, weights=(weights_atmo + weights_astro), label='Atmo + Astro', \n",
    "        histtype='step', lw=2, ls='-.')\n",
    "ax.hist(df[key], bins=bins, weights=(weights_atmo + weights_pi0_gamma3), label='Atmo + $\\pi^0 (\\gamma=3.0)$', \n",
    "        histtype='step', lw=2, ls='--')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(1e-1, 3e3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "fig.savefig('{}/energy_distribution.png'.format(\n",
    "    '/data/user/mhuennefeld/data/analyses/DNNCascadeCodeReview/unblinding_checks/plots/unblinding/galactic_plane_checks'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "livetime / 3600 / 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-collapse",
   "metadata": {},
   "source": [
    "## Data/MC Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disteval.visualization.comparison_plotter import ComparisonPlotter\n",
    "\n",
    "def get_binning_dict():\n",
    "    n_bins = 25\n",
    "    binning_dict = {}\n",
    "    for key in dfs['exp'].keys():\n",
    "        if 'energy' in key.lower():\n",
    "            binning_dict[key] = np.logspace(2, 8, n_bins)\n",
    "        elif key[:4] == 'BDT_' and 'astroness' in key:\n",
    "            binning_dict[key] = np.linspace(0, 1, 20)\n",
    "        elif 'azimuth' in key.lower():\n",
    "            binning_dict[key] = np.linspace(0, 2*np.pi, n_bins)\n",
    "        elif 'zenith' in key.lower():\n",
    "            binning_dict[key] = np.linspace(0, np.pi, n_bins)\n",
    "        elif 'cos_zen' in key.lower():\n",
    "            binning_dict[key] = np.linspace(-1, 1., 20)\n",
    "        elif '_q_' in key.lower():\n",
    "            binning_dict[key] = np.logspace(0, 5, n_bins)\n",
    "        elif '_p_is_veto_event' == key.lower()[-16:]:\n",
    "            binning_dict[key] = np.linspace(0, 1, 30)\n",
    "    return binning_dict\n",
    "        \n",
    "\n",
    "def data_mc_plot(key, df_mc, df_exp,\n",
    "                 weight_key='weights_new',\n",
    "                 mask_func=None, add_parts=True, \n",
    "                 bins=20, binning_dict=None, \n",
    "                 livetime=livetime,\n",
    "                 snowstorm_priors=None):\n",
    "    \"\"\"Draw Data/MC plot\n",
    "    \"\"\"\n",
    "    if binning_dict is None:\n",
    "        binning_dict = get_binning_dict()\n",
    "        \n",
    "    if 'energy' in key.lower() or '_q_' in key.lower():\n",
    "        xscale = 'log'\n",
    "    else:\n",
    "        xscale = 'linear'\n",
    "    \n",
    "    mc_origin = df_mc.mc_origin\n",
    "    values_sim = df_mc[key]\n",
    "    weight_sim = df_mc[weight_key]\n",
    "    values_exp = df_exp[key]\n",
    "    \n",
    "    if mask_func is not None:\n",
    "        mask_sim = mask_func(df_mc)\n",
    "        values_sim = values_sim[mask_sim]\n",
    "        weight_sim = weight_sim[mask_sim]\n",
    "        mc_origin = mc_origin[mask_sim]\n",
    "        mask = mask_func(df_exp)\n",
    "        values_exp = values_exp[mask]\n",
    "    \n",
    "    \n",
    "    if key not in binning_dict:\n",
    "        if isinstance(bins, int):\n",
    "            binning_dict[key] = np.linspace(\n",
    "                np.nanmin(values_exp), np.nanmax(values_exp), \n",
    "                bins)\n",
    "        else:\n",
    "            binning_dict[key] = bins\n",
    "    \n",
    "    plotter = ComparisonPlotter()\n",
    "    plotter.add_element('LimitedMCHisto',\n",
    "                        log_y=True,\n",
    "                        alpha=[0.68, 0.9, 0.99],\n",
    "                        y_label='Events in {:3.3f} days'.format(\n",
    "                                            livetime / (3600*24)),\n",
    "                        binning_dict=binning_dict)\n",
    "    plotter.add_element('LimitedMCRatio', zoomed=False,\n",
    "                        y_label='p-value',\n",
    "                        y_min_log_prob=-2.8,\n",
    "                        )\n",
    "\n",
    "    plotter.add_element('Normalization', normalize='test_livetime')\n",
    "    plotter.add_ref('MC simulation',\n",
    "                    values_sim,\n",
    "                    livetime=livetime,\n",
    "                    color='#1f77b4',\n",
    "                    weights=weight_sim,\n",
    "                    cmap='PuBu')\n",
    "    if add_parts:\n",
    "        for part_name in df_mc.mc_origin.unique():\n",
    "            mask_part = mc_origin == part_name\n",
    "            if np.sum(weight_sim[mask_part]) > 0.:\n",
    "                plotter.add_ref_part(part_name,\n",
    "                                     values_sim[mask_part],\n",
    "                                     livetime=livetime,\n",
    "                                     weights=weight_sim[mask_part])\n",
    "    \n",
    "    if snowstorm_priors is not None:\n",
    "        df_ss = df_nugen['SnowStorm']\n",
    "        values_ss = df_ss[key].values\n",
    "        weight_ss_orig = df_ss.weight_ss_orig.values\n",
    "        if mask_func is not None:\n",
    "            mask_ss = mask_func(df_ss)\n",
    "            values_ss = values_ss[mask_ss]\n",
    "            weight_ss_orig = weight_ss_orig[mask_ss]\n",
    "\n",
    "        for prior_name, prior_dict in snowstorm_priors.items():\n",
    "            w_multipler = get_snowstorm_multiplier(\n",
    "                df=df_ss,\n",
    "                priors_dict=prior_dict,\n",
    "            )\n",
    "            if mask_func is not None:\n",
    "                w_multipler = w_multipler[mask_ss]\n",
    "            if np.sum(mask_ss) > 0:\n",
    "                plotter.add_ref_part(prior_name,\n",
    "                                     values_ss,\n",
    "                                     livetime=livetime,\n",
    "                                     weights=weight_ss_orig*w_multipler)\n",
    "            \n",
    "    plotter.add_test('Data',\n",
    "                     values_exp,\n",
    "                     livetime=livetime,\n",
    "                     color='w')\n",
    "    fig, ax_dict, result_tray = plotter.draw(\n",
    "        x_label=key,\n",
    "        #max_ticks_per_side=2,\n",
    "    )\n",
    "    ax_dict['PlotHistAggerwal'].set_xscale(xscale)\n",
    "    ax_dict['PlotRatioAggerwal'].set_xscale(xscale)\n",
    "    #ax_dict['PlotHistAggerwal'].set_ylim(\n",
    "    #    max(1e-0, np.min(\n",
    "    #        result_tray.sum_w[result_tray.sum_w > 0])))\n",
    "\n",
    "    #mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    return fig, ax_dict, result_tray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-newsletter",
   "metadata": {},
   "source": [
    "#### Make example data/MC plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['angErr', 'energy', 'dec', 'sindec']:\n",
    "    fig, ax_dict, result_tray = data_mc_plot(key=key, df_mc=df_mc, df_exp=dfs['_exp'], livetime=_livetime, weight_key='weights_fit')\n",
    "    fig.savefig('{}/data_mc_fit_{}.png'.format(plot_dir, key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-mounting",
   "metadata": {},
   "source": [
    "#### Make collection of data/MC plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-amber",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "key_patterns = [\n",
    "    'CVMultiplicity',\n",
    "    'CVStatistics',\n",
    "    'MPEFit_azimuth',\n",
    "    'MPEFit_z',\n",
    "    'MPEFit_c',\n",
    "    'NN_',\n",
    "    'BDT_',\n",
    "    'EventGeneratorSelectedRecoNN',\n",
    "    #---'event_selection_egen_seed',\n",
    "    #'DNNCascadeSelectionRecoFeatures',\n",
    "    #'DeepLearningReco_event_selection_veto_classifier_01__test_p_is_veto_event',\n",
    "    #---'DeepLearningReco_event_selection_veto_classifier_vertex_early_01_p_is_veto_event',\n",
    "    #---'DeepLearningReco_event_selection',\n",
    "    #'event_selection_cascade_z',\n",
    "    #'EventGeneratorSelectedReco_I3Particle',\n",
    "    #---'event_selection_cascade_',\n",
    "    #'EventGenerator_cascade_7param_noise_tw_BFRv1Spice321_01_I3Particle_',\n",
    "    #'EventGenerator_cascade_7param_noise_tw_BFRv1Spice321_01__bfgs_gtol_10_I3Particle',\n",
    "    #'BDT_final_starting_300m_01',\n",
    "    #'BDT_bdt_max_depth_4_n_est_2000lr_0_02_seed_3_train_size_50',\n",
    "    #'BDT_bdt_max_depth_4_n_est_1000lr_0.01_seed_3_train_size_50',\n",
    "]\n",
    "avoid_patterns = [\n",
    "    'runtime',\n",
    "    'NN_unc',\n",
    "    'veto_classifier',\n",
    "    'DeepLearningReco_event_selection_cascade_dir_01',\n",
    "    'energy_fraction_2',\n",
    "    'DNNCascadeSelectionRecoFeatures_log',\n",
    "    'DeepLearningReco_event_selection_egen_seed_dir_01',\n",
    "    'BDT_astroness_bdt_mu0100_cscd0000_wo_energy_01_pred_000',\n",
    "    'BDT_astroness_bdt_mu0100_cscd0000_wo_energy_zenith_01_pred_000',\n",
    "]\n",
    "\n",
    "value_ranges = [\n",
    "    #[-1., 1.],\n",
    "    [-0.6, -0.3],\n",
    "    [-0.3, 0.],\n",
    "    [0., 0.3],\n",
    "    [0.3, 0.6],\n",
    "    #[-0.65, 0.6],\n",
    "]\n",
    "eff_ranges = [\n",
    "    [0.9, 1.1],\n",
    "    #[1., 1.1],\n",
    "    #[0.9, 1.0],\n",
    "]\n",
    "abs_ranges = [\n",
    "    #[0.9, 1.1],\n",
    "    [1., 1.1],\n",
    "    [0.9, 1.0],\n",
    "]\n",
    "scat_ranges = [\n",
    "    [0.9, 1.1],\n",
    "    #[1., 1.1],\n",
    "    #[0.9, 1.0],\n",
    "]\n",
    "snowstorm_priors = {}\n",
    "for value_range in value_ranges:\n",
    "    for abs_range in abs_ranges:\n",
    "        for scat_range in scat_ranges:\n",
    "            for eff_range in eff_ranges:\n",
    "                name = 'H0 [{:0.2f}, {:0.2}] | abs. [{:0.2f}, {:0.2}] | scat. [{:0.2f}, {:0.2}] | eff. [{:0.2f}, {:0.2}]'.format(\n",
    "                    *(value_range + abs_range + scat_range + eff_range))\n",
    "                snowstorm_priors[name] = {\n",
    "                    'Absorption': abs_range,\n",
    "                    'Scattering': scat_range,\n",
    "                    'DOMEfficiency': eff_range,\n",
    "                    'HoleIceForward_Unified_00': value_range,\n",
    "                }\n",
    "snowstorm_priors = None\n",
    "\n",
    "\n",
    "def mask_func(df):\n",
    "    mask = np.ones(len(df), dtype=bool)\n",
    "    return mask\n",
    "\n",
    "possible_keys = []\n",
    "for key in dfs['exp'].keys():\n",
    "    match_pattern = False\n",
    "    for avoid_pattern in avoid_patterns:\n",
    "        if avoid_pattern in key:\n",
    "            match_pattern = True\n",
    "            break\n",
    "    if not match_pattern:\n",
    "        possible_keys.append(key)\n",
    "\n",
    "data_mc_plot_dir = '{}/data_mc'.format(plot_dir)\n",
    "if not os.path.exists(data_mc_plot_dir):\n",
    "    print('Creating directory:', data_mc_plot_dir)\n",
    "    os.makedirs(data_mc_plot_dir)\n",
    "    \n",
    "for key in tqdm(sorted(set(possible_keys))):\n",
    "    for key_pattern in key_patterns:\n",
    "        if key_pattern in key:\n",
    "            if key in df_mc:\n",
    "                print(key)\n",
    "                fig, ax_dict, result_tray = data_mc_plot(\n",
    "                    key, df_mc=df_mc, df_exp=dfs['exp'],\n",
    "                    mask_func=mask_func, add_parts=True,\n",
    "                    snowstorm_priors=snowstorm_priors,\n",
    "                )\n",
    "                title = ''\n",
    "                for prior_key, value_range in priors_dict.items():\n",
    "                    title += '{}: [{:3.2f}, {:3.2f}] '.format(\n",
    "                        prior_key, *value_range)\n",
    "                ax_dict['PlotHistAggerwal'].set_title(title)\n",
    "                fig.savefig('{}/data_mc_{}.png'.format(\n",
    "                    data_mc_plot_dir, key))\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-mounting",
   "metadata": {},
   "source": [
    "### Masked Data MC plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-tactics",
   "metadata": {},
   "source": [
    "##### High-Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-wireless",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mask_func(df):\n",
    "    # mask = np.ones(len(df), dtype=bool)\n",
    "    mask = df['EventGeneratorSelectedRecoNN_I3Particle_energy'] > 1e5\n",
    "    return mask\n",
    "\n",
    "data_mc_plot_dir_masked = '{}/data_mc/masked'.format(plot_dir)\n",
    "if not os.path.exists(data_mc_plot_dir_masked):\n",
    "    print('Creating directory:', data_mc_plot_dir_masked)\n",
    "    os.makedirs(data_mc_plot_dir_masked)\n",
    "    \n",
    "for key in tqdm(sorted(set(possible_keys))):\n",
    "    for key_pattern in key_patterns:\n",
    "        if key_pattern in key:\n",
    "            if key in df_mc:\n",
    "                print(key)\n",
    "                fig, ax_dict, result_tray = data_mc_plot(\n",
    "                    key, df_mc=df_mc, df_exp=dfs['exp'],\n",
    "                    mask_func=mask_func, add_parts=True,\n",
    "                    snowstorm_priors=snowstorm_priors,\n",
    "                )\n",
    "                title = ''\n",
    "                for prior_key, value_range in priors_dict.items():\n",
    "                    title += '{}: [{:3.2f}, {:3.2f}] '.format(\n",
    "                        prior_key, *value_range)\n",
    "                ax_dict['PlotHistAggerwal'].set_title(title)\n",
    "                fig.savefig('{}/data_mc_{}.png'.format(\n",
    "                    data_mc_plot_dir_masked, key))\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-occasions",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-fault",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.3_py3-v4.1.0_csky",
   "language": "python",
   "name": "tensorflow2.3_py3-v4.1.0_csky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
