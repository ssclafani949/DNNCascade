{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#PDFs in BDT and sindec?\n",
    "import os\n",
    "\n",
    "# set env flags to catch BLAS used for scipy/numpy \n",
    "# to only use 1 cpu, n_cpus will be totally controlled by csky\n",
    "if False:\n",
    "    os.environ['MKL_NUM_THREADS'] = \"1\"\n",
    "    os.environ['NUMEXPR_NUM_THREADS'] = \"1\"\n",
    "    os.environ['OMP_NUM_THREADS'] = \"1\"\n",
    "    os.environ['OPENBLAS_NUM_THREADS'] = \"1\"\n",
    "    os.environ['VECLIB_MAXIMUM_THREADS'] = \"1\"\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'w'\n",
    "mpl.rcParams['savefig.facecolor'] = 'w'\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, cm\n",
    "import csky as cy\n",
    "from csky import cext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import astropy\n",
    "#from icecube import astro\n",
    "import histlite as hl\n",
    "import healpy\n",
    "import healpy as hp\n",
    "import socket\n",
    "import pickle\n",
    "import copy\n",
    "healpy.disable_warnings()\n",
    "plt.rc('figure', facecolor = 'w')\n",
    "plt.rc('figure', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-narrative",
   "metadata": {},
   "source": [
    "## Define Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_version = 'version-001-p01'\n",
    "\n",
    "host_name = socket.gethostname()\n",
    "\n",
    "if 'cobalt' in host_name:\n",
    "    print('Working on Cobalts')\n",
    "    data_prefix = '/data/user/ssclafani/data/cscd/final'\n",
    "    ana_dir = '/data/user/ssclafani/data/analyses/'\n",
    "    plot_dir = '/data/user/mhuennefeld/data/analyses/DNNCascadeCodeReview/unblinding_checks/plots/data_release/create_data/DataFluxMeasurement'\n",
    "    \n",
    "else:\n",
    "    raise ValueError('Unknown host:', host_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_path in [plot_dir]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        print('Creating directory:', dir_path)\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-neighbor",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = cy.selections.Repository()\n",
    "specs = cy.selections.DNNCascadeDataSpecs.DNNC_10yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ana = cy.get_analysis(\n",
    "    repo, selection_version, specs, \n",
    "    #gammas=np.r_[0.1:6.01:0.125],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ana.anas[0]\n",
    "a.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.bg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-convention",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycle\n",
    "from copy import deepcopy\n",
    "\n",
    "soft_colors = cy.plotting.soft_colors\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\n",
    "def get_bias_allt(tr, ntrials=200, n_sigs=np.r_[:101:10], quiet=False):\n",
    "    trials = [\n",
    "        (None if quiet else print(f'\\r{n_sig:4d} ...', end='', flush=True))\n",
    "        or\n",
    "        tr.get_many_fits(ntrials, n_sig=n_sig, logging=False, seed=n_sig)\n",
    "        for n_sig in n_sigs]\n",
    "    if not quiet:\n",
    "        print()\n",
    "    for (n_sig, t) in zip(n_sigs, trials):\n",
    "        t['ntrue'] = np.repeat(n_sig, len(t))\n",
    "    allt = cy.utils.Arrays.concatenate(trials)\n",
    "    return allt\n",
    "\n",
    "def get_color_cycler():\n",
    "    return cycle(colors)\n",
    "\n",
    "def plot_ns_bias(ax, tr, allt, label=''):\n",
    "\n",
    "    n_sigs = np.unique(allt.ntrue)\n",
    "    dns = np.mean(np.diff(n_sigs))\n",
    "    ns_bins = np.r_[n_sigs - 0.5*dns, n_sigs[-1] + 0.5*dns]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "\n",
    "    h = hl.hist((allt.ntrue, allt.ns), bins=(ns_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1),errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = ns_bins[[0, -1]]\n",
    "    ax.set_xlim(ax.set_ylim(lim))\n",
    "    ax.plot(lim, lim, **expect_kw)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ax.set_xlabel(r'$n_{inj}$')\n",
    "    ax.set_ylabel(r'$n_s$')\n",
    "    ax.grid()\n",
    "\n",
    "def plot_gamma_bias(ax, tr, allt, label=''):\n",
    "\n",
    "    n_sigs = np.unique(allt.ntrue)\n",
    "    dns = np.mean(np.diff(n_sigs))\n",
    "    ns_bins = np.r_[n_sigs - 0.5*dns, n_sigs[-1] + 0.5*dns]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "    expect_gamma = tr.sig_injs[0].flux[0].gamma\n",
    "\n",
    "    h = hl.hist((allt.ntrue, allt.gamma), bins=(ns_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1),errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = ns_bins[[0, -1]]\n",
    "    ax.set_xlim(lim)\n",
    "    ax.set_ylim(1, 4)\n",
    "    ax.axhline(expect_gamma, **expect_kw)\n",
    "\n",
    "    ax.set_xlabel(r'$n_{inj}$')\n",
    "    ax.set_ylabel(r'$\\gamma$')\n",
    "    ax.grid()\n",
    "\n",
    "def plot_bkg_trials(\n",
    "            bg, fig=None, ax=None, \n",
    "            label='{} bg trials', \n",
    "            label_fit=r'$\\chi^2[{:.2f}\\mathrm{{dof}},\\ \\eta={:.3f}]$', \n",
    "            color=colors[0],\n",
    "            density=False,\n",
    "            bins=50,\n",
    "        ):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    if density:\n",
    "        h = bg.get_hist(bins=bins).normalize()\n",
    "    else:\n",
    "        h = bg.get_hist(bins=bins)\n",
    "    if label is not None:\n",
    "        label = label.format(bg.n_total)\n",
    "    hl.plot1d(ax, h, crosses=True, color=color, label=label)\n",
    "\n",
    "    # compare with the chi2 fit:\n",
    "    if hasattr(bg, 'pdf'):\n",
    "        x = h.centers[0]\n",
    "        norm = h.integrate().values\n",
    "        if label_fit is not None:\n",
    "            label_fit = label_fit.format(bg.ndof, bg.eta)\n",
    "        if density:\n",
    "            ax.semilogy(x, bg.pdf(x), lw=1, ls='--', label=label_fit, color=color)\n",
    "        else:\n",
    "            ax.semilogy(x, norm * bg.pdf(x), lw=1, ls='--', label=label_fit, color=color)\n",
    "\n",
    "    ax.set_xlabel(r'TS')\n",
    "    if density:\n",
    "        ax.set_ylabel(r'Density')\n",
    "    else:\n",
    "        ax.set_ylabel(r'number of trials')\n",
    "    ax.legend()\n",
    "        \n",
    "    return fig, ax, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-acrobat",
   "metadata": {},
   "source": [
    "## Setup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../..')\n",
    "\n",
    "import config as cg\n",
    "\n",
    "cg.base_dir = '/data/user/mhuennefeld/data/analyses/unblinding_v1.0.1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gp_tr(template_str, cutoff=np.inf, gamma=None, cpus=20, ana=ana):\n",
    "    cutoff_GeV = cutoff * 1e3\n",
    "    gp_conf = cg.get_gp_conf(\n",
    "        template_str=template_str, gamma=gamma, cutoff_GeV=cutoff_GeV, base_dir=cg.base_dir)\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr\n",
    "\n",
    "def get_template_tr(template, gamma=2.7, cutoff_tev=np.inf, cpus=20):\n",
    "    cutoff_gev = cutoff_tev * 1000.\n",
    "    gp_conf = {\n",
    "        'template': template,\n",
    "        'flux': cy.hyp.PowerLawFlux(gamma, energy_cutoff=cutoff_gev),\n",
    "        'randomize': ['ra'],\n",
    "        'fitter_args': dict(gamma=gamma),\n",
    "        'sigsub': True,\n",
    "        'update_bg': True,\n",
    "        'fast_weight': False,\n",
    "    }\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-eligibility",
   "metadata": {},
   "source": [
    "#### Get TrialRunners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dict = {\n",
    "    'pi0': get_gp_tr('pi0'),\n",
    "    'kra5': get_gp_tr('kra5'),\n",
    "    'kra50': get_gp_tr('kra50'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-anime",
   "metadata": {},
   "source": [
    "#### Get Results for each template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for key in tr_dict.keys():\n",
    "    f_path = os.path.join(\n",
    "        cg.base_dir, \n",
    "        'gp/results/{}/{}_unblinded.npy'.format(key, key), \n",
    "    )\n",
    "    if os.path.exists(f_path):\n",
    "        res_dict[key] = np.load(f_path)\n",
    "    else:\n",
    "        print('File does not exist: {}'.format(f_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-basics",
   "metadata": {},
   "source": [
    "#### Print best fit fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_norm = tr_dict['kra5'].to_model_norm(ns=res_dict['kra5'][1])\n",
    "model_norm50 = tr_dict['kra50'].to_model_norm(ns=res_dict['kra50'][1])\n",
    "dNdE = tr_dict['pi0'].to_dNdE(ns=res_dict['pi0'][1], E0=1e5)\n",
    "E2dNdE = tr_dict['pi0'].to_E2dNdE(ns=res_dict['pi0'][1], E0=100, unit=1e3)\n",
    "print(dNdE, E2dNdE, model_norm, model_norm50, model_norm*3/2, model_norm50*3/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dict['kra5'].to_model_norm(ns=273)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-seventh",
   "metadata": {},
   "source": [
    "#### Get bkg fits for each template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_file_dict = {\n",
    "    'pi0': '{}/gp/trials/{}/{}/trials.dict'.format(cg.base_dir, 'DNNC', 'pi0'),\n",
    "    'kra5': '{}/gp/trials/{}/{}/trials.dict'.format(cg.base_dir, 'DNNC', 'kra5'),\n",
    "    'kra50': '{}/gp/trials/{}/{}/trials.dict'.format(cg.base_dir, 'DNNC', 'kra50'),\n",
    "}\n",
    "n_bkg_trials = 20000\n",
    "seed = 1337\n",
    "\n",
    "bkg_dict = {}\n",
    "for key, tr in tr_dict.items():\n",
    "    if 'fermibubbles' in key: continue\n",
    "    if key in bkg_file_dict:\n",
    "        print('Loading background trials for template {}'.format(key))\n",
    "        sig = np.load(bkg_file_dict[key], allow_pickle=True)\n",
    "        bkg_dict[key] = sig['poisson']['nsig'][0.0]['ts']\n",
    "    \n",
    "    else:\n",
    "        print('Running background trials for template {}'.format(key))\n",
    "        bkg_dict[key] = tr.get_many_fits(\n",
    "            n_trials=n_bkg_trials, seed=seed, mp_cpus=20)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, values in bkg_dict.items():\n",
    "    print(k, len(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-closing",
   "metadata": {},
   "source": [
    "#### Get ts distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-convenience",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bkg_hist_dict = {}\n",
    "bg_tsd_dict = {}\n",
    "for key, bg in bkg_dict.items():\n",
    "    bg_tsd = cy.dists.TSD(bg)\n",
    "    fig, ax, h = plot_bkg_trials(bg_tsd, bins=np.linspace(0, 35, 1001))\n",
    "    \n",
    "    bkg_hist_dict[key] = h\n",
    "    bg_tsd_dict[key] = bg_tsd\n",
    "    \n",
    "    ts = res_dict[key][0]\n",
    "    ns = res_dict[key][1]\n",
    "    ax.axvline(\n",
    "        ts, color='0.8', ls='--', lw=2,\n",
    "        label='TS: {:3.3f} | ns: {:3.1f}'.format(ts, ns), \n",
    "    )\n",
    "    ts_5sig = bg_tsd.isf_nsigma(5)\n",
    "    ax.axvline(\n",
    "        ts_5sig, ls='--', lw=1,\n",
    "        label='5-sigma TS: {:3.3f}'.format(ts_5sig), \n",
    "    )\n",
    "    ax.set_title('Template: {}'.format(key))\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    fig.savefig('{}/ts_dist_{}.png'.format(plot_dir, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-affiliation",
   "metadata": {},
   "source": [
    "#### Save bkg ts distribution to file\n",
    "\n",
    "We will save everything to human readible csv files. \n",
    "This will allow the end-user to easily load this data in any\n",
    "application/language of their choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins = pd.DataFrame()\n",
    "df_values = pd.DataFrame()\n",
    "for key, h in bkg_hist_dict.items():\n",
    "    df_bins[key] = h.bins[0]\n",
    "    df_values[key] = h.values\n",
    "\n",
    "df_bins = df_bins.reset_index()\n",
    "df_values = df_values.reset_index()\n",
    "\n",
    "df_bins.to_csv('{}/bkg_trials_binning.csv'.format(plot_dir), index=False)\n",
    "df_values.to_csv('{}/bkg_trials_values.csv'.format(plot_dir), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-decline",
   "metadata": {},
   "source": [
    "#### Compute Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def get_significance_from_hist(ts, bin_edges, bin_values):\n",
    "    \"\"\"Compute significance for a given test-statistic\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : float\n",
    "        The test-statistic value for which the significance will be calculated.\n",
    "    bin_edges : array_like\n",
    "        The bin edges for the background trial histogram.\n",
    "    bin_values : array_like\n",
    "        The bin heights for the background trial histogram.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The significance of rejecting the null hypothesis for the given\n",
    "        test-statistic value based on the background trials provided\n",
    "        via `bin_edges` and `bin_values`\n",
    "    \"\"\"\n",
    "    # get index for which is valid:\n",
    "    # edge[i-1] < ts <= edge[i]\n",
    "    # this correspdonds to the i-th entry in bin_values\n",
    "    index = np.searchsorted(bin_edges, ts)\n",
    "    \n",
    "    # get number of trials with larger ts values\n",
    "    n_larger = np.sum(bin_values[index:])\n",
    "    pval = 1.*n_larger/np.sum(bin_values)\n",
    "    nsigma = stats.norm.isf(pval)\n",
    "    \n",
    "    return pval, nsigma\n",
    "\n",
    "for key, h in bkg_hist_dict.items():\n",
    "    pval, nsgima = get_significance_from_hist(\n",
    "        ts=res_dict[key][0],\n",
    "        bin_edges=df_bins[key].values,\n",
    "        bin_values=df_values[key].values,\n",
    "    )\n",
    "    print('{:3.4f} | {:3.4f} (calc.) | {:3.4f} (hist) | {}'.format(\n",
    "        res_dict[key][3], \n",
    "        bg_tsd_dict[key].sf_nsigma(res_dict[key][0]),\n",
    "        nsgima,\n",
    "        key, \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-fiction",
   "metadata": {},
   "source": [
    "## Effective area and Acceptance Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-austin",
   "metadata": {},
   "source": [
    "#### Load Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "kra5, kra5_nu, kra5_nubar, ebins5 = np.load('/data/ana/analyses/NuSources/2021_DNNCascade_analyses/templates/KRA-gamma_5PeV_maps_energies.tuple.npy', allow_pickle = True, encoding='latin1')\n",
    "kra50, kra50_nu, kra50_nubar, ebins50 = np.load('/data/ana/analyses/NuSources/2021_DNNCascade_analyses/templates/KRA-gamma_maps_energies.tuple.npy', allow_pickle=True, encoding='latin1')\n",
    "pi0 = np.load('/data/ana/analyses/NuSources/2021_DNNCascade_analyses/templates/Fermi-LAT_pi0_map.npy', allow_pickle=True, encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-north",
   "metadata": {},
   "source": [
    "##### Some sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "kra5_sum = kra5_nu + kra5_nubar\n",
    "np.allclose(kra5, kra5_sum), np.allclose(kra5_nubar, kra5_nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    csky_kra5_template, csky_kra5_energy_bins = cg.template_repo.get_template(\n",
    "        'KRA-gamma_5PeV_maps_energies', per_pixel_flux=True)\n",
    "    print(np.allclose(csky_kra5_template, kra5 / 2.))\n",
    "    print(np.allclose(csky_kra5_template, kra5 / 3.))\n",
    "    print(np.allclose(csky_kra5_energy_bins, ebins5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if these match the ones uploaded on zenodo\n",
    "if True:\n",
    "    kra5_, kra5_nu_, kra5_nubar_, ebins5_ = np.load('/data/user/mhuennefeld/to_delete/templates/KRA-gamma_5PeV_maps_energies.tuple.npy', allow_pickle = True, encoding='latin1')\n",
    "    kra50_, kra50_nu_, kra50_nubar_, ebins50_ = np.load('/data/user/mhuennefeld/to_delete/templates/KRA-gamma_50PeV_maps_energies.tuple.npy', allow_pickle = True, encoding='latin1')\n",
    "    print(\n",
    "        np.allclose(kra5_, kra5), \n",
    "        np.allclose(kra5_nu_, kra5_nu), \n",
    "        np.allclose(kra5_nubar_, kra5_nubar), \n",
    "        np.allclose(ebins5_, ebins5),\n",
    "    )\n",
    "    print(\n",
    "        np.allclose(kra50_, kra50), \n",
    "        np.allclose(kra50_nu_, kra50_nu), \n",
    "        np.allclose(kra50_nubar_, kra50_nubar), \n",
    "        np.allclose(ebins50_, ebins50),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-ethernet",
   "metadata": {},
   "source": [
    "#### Compute Effective Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_eff_area(sindec_min=-1, sindec_max=1, dlogE=1*np.log10(1.1), unit='cm'):\n",
    "    \"\"\"Get effective area for given declination band\n",
    "    \"\"\"\n",
    "    \n",
    "    # we want to try and match the binning used in the \n",
    "    # KRA-gamma templates (dlogE=np.log10(1.1) from 10 GeV)\n",
    "    # in order to avoid too many binning artifacts later on.\n",
    "    # So we will construct the bins the same way\n",
    "    # (shifted by small epsilon) and then cut\n",
    "    # them to a smaller energy range later on.\n",
    "    bins = 10**np.arange(1 - 0.0001*dlogE, 8.01, dlogE)\n",
    "    bins = bins[np.logical_and(bins >= 1e2, bins <= 1.1e7)]\n",
    "    \n",
    "    # get events belonging to this declination band\n",
    "    sindec_true = np.sin(a.sig.true_dec)\n",
    "    mask = (sindec_true < sindec_max) & (sindec_true > sindec_min)\n",
    "    \n",
    "    # compute the solid angle for this declination band\n",
    "    solid_angle = 2 *np.pi * (sindec_max - sindec_min)\n",
    "    \n",
    "    # compute the effective area\n",
    "    if unit == 'm':\n",
    "        cm2_to_unit = 1e4\n",
    "    elif unit == 'cm':\n",
    "        cm2_to_unit = 1\n",
    "    else:\n",
    "        raise ValueError(unit)\n",
    "        \n",
    "    area_event = a.sig.oneweight[mask] / cm2_to_unit\n",
    "    \n",
    "    # normalize by solid angle\n",
    "    area_event /= solid_angle\n",
    "    \n",
    "    # normalize by energy bin width\n",
    "    # Energy bin ranges from\n",
    "    # 10^(log10E - 0.5*dlogE) to 10^(log10E + 0.5*dlogE)\n",
    "    # bin width is therefore:\n",
    "    # 10^(log10E + 0.5*dlogE) - 10^(log10E - 0.5*dlogE)\n",
    "    # = E * (10^(0.5*dlogE) - 10^(-0.5*dlogE))\n",
    "    area_event /= a.sig.true_energy[mask] * dlogE * np.log(10)\n",
    "    \n",
    "    effa, _ = np.histogram(\n",
    "        a.sig.true_energy[mask], \n",
    "        bins=bins, \n",
    "        weights=area_event,\n",
    "    )\n",
    "    \n",
    "    return effa, bins, area_event, a.sig.true_energy[mask]\n",
    "\n",
    "sindec_bins = np.linspace(-1., 1., 6)\n",
    "eff_area = []\n",
    "for i, sindec_max in enumerate(sindec_bins[1:]):\n",
    "    effa, energy_bins, area_event, energy = get_eff_area(\n",
    "        sindec_min=sindec_bins[i], sindec_max=sindec_max,\n",
    "    ) \n",
    "    eff_area.append(effa)\n",
    "\n",
    "eff_area = np.array(eff_area)\n",
    "print('eff_area', eff_area.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, sindec_max in enumerate(sindec_bins[1:]):\n",
    "    effa, energy_bins, area_event, energy = get_eff_area(\n",
    "        sindec_min=sindec_bins[i], sindec_max=sindec_max,\n",
    "    ) \n",
    "    \n",
    "    ax.step(energy_bins[1:], eff_area[i])\n",
    "    ax.hist(energy, bins=energy_bins, weights=area_event, histtype='step', ls='--')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-glance",
   "metadata": {},
   "source": [
    "#### Save effective area to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    '{}/effa_values.csv'.format(plot_dir),\n",
    "    eff_area, delimiter=','\n",
    ")\n",
    "np.savetxt(\n",
    "    '{}/effa_bins_sindec.csv'.format(plot_dir),\n",
    "    sindec_bins, delimiter=','\n",
    ")\n",
    "np.savetxt(\n",
    "    '{}/effa_bins_energy.csv'.format(plot_dir),\n",
    "    energy_bins, delimiter=','\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-clock",
   "metadata": {},
   "source": [
    "#### Compute acceptance correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def acceptance_correct_pi0(\n",
    "            ns, template, eff_area, sindec_bins, energy_bins,\n",
    "            livetime=304047105.0735066,\n",
    "        ):\n",
    "    \"\"\"Convert a number of signal events to the corresponding flux\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ns : array_like\n",
    "        The number of signal events that will be converted to the\n",
    "        corresponding flux.\n",
    "    template : array_like\n",
    "        The spatial template in units of sr^-1.\n",
    "        Shape: [npix]\n",
    "    eff_area : array_like\n",
    "        The effective area binned in sin(dec) (unitless) along first\n",
    "        dimension and and in energy (in GeV) along the second axis.\n",
    "        shape: [n_bins_sindec, n_bins_energy]\n",
    "    sindec_bins : array_like\n",
    "        The bin edges along the first dimension corresponding to sin(dec).\n",
    "        Shape: [n_bins_sindec + 1]\n",
    "    energy_bins : TYPE\n",
    "        The bin edges along the second dimension corresponding to\n",
    "        the energy in GeV.\n",
    "        Shape: [n_bins_energy + 1]\n",
    "    livetime : float\n",
    "        The livetime of the dataset in seconds.\n",
    "        For the 10-year cascade dataset, the livetime corresponds to\n",
    "        304047105.0735066 seconds, which is the default value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        The flux in terms of E^2 dN/dE at 100 TeV in units\n",
    "        of TeV cm^-2 s^-1.\n",
    "    \"\"\"\n",
    "\n",
    "    npix = len(template)\n",
    "    nside = hp.npix2nside(npix)\n",
    "    \n",
    "    # First we need to construct Phi(sindec, energy)\n",
    "    # in units of GeV^-1 cm^-2 s^-1 sr^-1.\n",
    "    # We will do this by splitting Phi into the spatial term (units of sr^-1)\n",
    "    # and the energy term (units of GeV^-1 cm^-2 s^-1).\n",
    "    # We can do this splitting for the pi0 model because the energy spectrum\n",
    "    # is assumed not to depend on the location in the sky\n",
    "    theta, phi = hp.pix2ang(nside, np.r_[:npix])\n",
    "    pix_dec = np.pi/2. - theta\n",
    "    pix_ra = phi\n",
    "    pix_sindec = np.sin(pix_dec)\n",
    "\n",
    "    # Phi(sindec), shape: [n_sindec], units of sr^-1\n",
    "    phi_sindec = np.zeros(len(sindec_bins)-1)\n",
    "\n",
    "    # walk through each declination band\n",
    "    for i, sindec_max in enumerate(sindec_bins[1:]):\n",
    "        sindec_min = sindec_bins[i]\n",
    "\n",
    "        # get all pixels belonging to this dec band\n",
    "        mask_pixels = np.logical_and(\n",
    "            pix_sindec >= sindec_min,\n",
    "            pix_sindec < sindec_max,\n",
    "        )\n",
    "        phi_sindec[i] = np.sum(template[mask_pixels]) \n",
    "\n",
    "    # Sky-integrated, per-flavor (nu + nubar) flux\n",
    "    # Choose an arbitrary flux normalization to scale\n",
    "    norm = 2.4368317158544137e-18  # in units of GeV^-1 cm^-2 s^-1\n",
    "    e0 = 1e5  # 100 TeV in units of GeV\n",
    "    gamma = 2.7\n",
    "\n",
    "    # compute average energy in each energy bin\n",
    "    energy_avg = (\n",
    "        (-gamma + 1) / (-gamma + 2) *\n",
    "        (energy_bins[1:]**(-gamma+2) - energy_bins[:-1]**(-gamma+2)) /\n",
    "        (energy_bins[1:]**(-gamma+1) - energy_bins[:-1]**(-gamma+1))\n",
    "    )\n",
    "\n",
    "    # Phi(energy) in units of GeV^-1 cm^-2 s^-1 sr^-1\n",
    "    # shape: [n_energy]\n",
    "    phi_e = norm * (energy_avg / e0) ** (-gamma)\n",
    "\n",
    "    # Phi(sindec, energy)\n",
    "    # shape: [n_sindec, n_energy],  GeV^-1 cm^-2 s^-1 sr^-1\n",
    "    phi = phi_e[np.newaxis] * phi_sindec[:, np.newaxis]\n",
    "\n",
    "    # \"integrate\" over energy bin width\n",
    "    # shape: [n_sindec, n_energy], cm^-2 s^-1 sr^-1\n",
    "    phi *= np.diff(energy_bins)[np.newaxis]\n",
    "\n",
    "    # \"integrate\" over solid angle\n",
    "    # shape: [n_pix, n_bins], s^-1 * cm^-2\n",
    "    phi *= hp.nside2pixarea(nside)\n",
    "\n",
    "    # Now we can compute the total number of expected events\n",
    "    # for the given flux.\n",
    "    # eff_area is in units of cm^-2 and livetime in units of s\n",
    "    total_acceptance = livetime * np.sum(eff_area * phi)\n",
    "\n",
    "    # we can then compute the ratio of the number of signal events versus\n",
    "    # the number of events one would have expected based on the previously\n",
    "    # defined flux. This can then be used to scale the arbitrary flux\n",
    "    # normalization that we chose as a reference point.\n",
    "    model_norm = ns / total_acceptance\n",
    "    print('model_norm', model_norm)\n",
    "    \n",
    "    # compute E^2 dN/dE at 100 TeV in units of TeV cm^-2 s^-1\n",
    "    norm_tev = norm * 1e3  # in units of TeV^-1 cm^-2 s^-1\n",
    "    norm_E2_tev = norm_tev * (100)**2\n",
    "    return model_norm*norm_E2_tev\n",
    "\n",
    "acceptance_correct_pi0(\n",
    "    ns=748,\n",
    "    template=pi0,\n",
    "    eff_area=eff_area,\n",
    "    sindec_bins=sindec_bins,\n",
    "    energy_bins=energy_bins,\n",
    "    livetime=a.livetime,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptance_correct_kra(ns, template, template_emids, eff_area, sindec_bins, energy_bins, livetime):\n",
    "    \n",
    "    npix = len(template)\n",
    "    nside = hp.npix2nside(npix)\n",
    "    pixarea = hp.nside2pixarea(nside)\n",
    "    \n",
    "    # compute energy bin widths\n",
    "    t_energy_bins = np.r_[template_emids, template_emids[-1]*1.1]\n",
    "    t_delta_e = np.diff(t_energy_bins)\n",
    "    \n",
    "    # compute Phi(sindec, energy)   \n",
    "    theta, phi = hp.pix2ang(nside, np.r_[:npix])\n",
    "    pix_dec = np.pi/2. - theta\n",
    "    pix_ra = phi\n",
    "    pix_sindec = np.sin(pix_dec)\n",
    "    \n",
    "    # Phi(sindec, energy), shape: [n_sindec, n_energy]\n",
    "    phi = np.zeros((len(sindec_bins)-1, len(energy_bins) -1))\n",
    "    \n",
    "    for i, sindec_max in enumerate(sindec_bins[1:]):\n",
    "        sindec_min = sindec_bins[i]\n",
    "        \n",
    "        # get all pixels belonging to this dec band\n",
    "        mask_pixels = np.logical_and(\n",
    "            pix_sindec >= sindec_min,\n",
    "            pix_sindec < sindec_max,\n",
    "        )\n",
    "        \n",
    "        # select bins corresponding to this dec band\n",
    "        template_decband = template[mask_pixels]\n",
    "        \n",
    "        for j, energy_max in enumerate(energy_bins[1:]):\n",
    "            \n",
    "            # get all bins belonging to this energy band\n",
    "            mask_bins = np.logical_and(\n",
    "                template_emids >= energy_bins[j],\n",
    "                template_emids < energy_max,\n",
    "            )\n",
    "            \n",
    "            # Template files are in GeV^-1 * sr^-1 * s^-1 * cm^-2\n",
    "            # shape: [n_pix, n_bins], GeV^-1 * sr^-1 * s^-1 * cm^-2\n",
    "            flux_bins = template_decband[:, mask_bins]\n",
    "            \n",
    "            # \"integrate\" over energy range\n",
    "            # shape: [n_pix, n_bins], sr^-1 * s^-1 * cm^-2\n",
    "            flux_bins *= t_delta_e[mask_bins]\n",
    "            \n",
    "            # \"integrate\" over solid angle\n",
    "            # shape: [n_pix, n_bins], s^-1 * cm^-2\n",
    "            flux_bins *= pixarea\n",
    "            \n",
    "            # shape: [], s^-1 * cm^-2\n",
    "            phi[i, j] = np.sum(flux_bins)\n",
    "            \n",
    "            # template is for sum of all neutrino flavors\n",
    "            # including nu and nubar\n",
    "            # correct to per-flavor flux (nu+nubar)\n",
    "            phi[i, j] /= 3.\n",
    "    \n",
    "    print('eff_area.shape', eff_area.shape)\n",
    "    total_acceptance = livetime * np.sum(eff_area * phi)\n",
    "    print('total_acceptance', total_acceptance)\n",
    "    model_norm = ns / total_acceptance\n",
    "    print('model_norm', model_norm)\n",
    "    \n",
    "    return model_norm\n",
    "\n",
    "ns = 273\n",
    "model_norm = acceptance_correct_kra(\n",
    "    ns=ns,\n",
    "    template=kra5,\n",
    "    template_emids=ebins5,\n",
    "    eff_area=eff_area,\n",
    "    sindec_bins=sindec_bins,\n",
    "    energy_bins=energy_bins,\n",
    "    livetime=a.livetime,\n",
    ")\n",
    "model_norm_tr5 = tr_dict['kra5'].to_model_norm(ns)\n",
    "model_norm_tr50 = tr_dict['kra50'].to_model_norm(ns)\n",
    "print(model_norm, model_norm_tr5, model_norm_tr50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flux_from_template(ra, dec, energy, template, template_emids, csky_indexing=False):\n",
    "    \n",
    "    npix = len(template)\n",
    "    nside = hp.npix2nside(npix)\n",
    "\n",
    "    # compute bin edges\n",
    "    t_energy_bins = np.r_[template_emids, template_emids[-1]*1.1]\n",
    "    \n",
    "    phi = ra\n",
    "    theta = np.pi/2. - dec\n",
    "    ipix = hp.ang2pix(nside=nside, theta=theta, phi=phi)\n",
    "    \n",
    "    # compute energy bin\n",
    "    if csky_indexing:\n",
    "        energy_idx = np.searchsorted(template_emids, energy) + 1 # correct would be -1 instead of +1\n",
    "        energy_idx[energy_idx >= template_emids.size] = template_emids.size - 1\n",
    "    else:\n",
    "        energy_idx = np.searchsorted(t_energy_bins, energy) - 1\n",
    "    \n",
    "    print('Order matches up [mid interpretation]:', np.logical_and(\n",
    "        energy > template_emids[np.clip(energy_idx-1, 0, template_emids.size-1)], \n",
    "        energy < template_emids[np.clip(energy_idx+1, 0, template_emids.size-1)], \n",
    "    ).all())\n",
    "    print('Order matches up [edge interpretation]:', np.logical_and(\n",
    "        energy >= template_emids[np.clip(energy_idx, 0, template_emids.size-1)], \n",
    "        energy < template_emids[np.clip(energy_idx+1, 0, template_emids.size-1)], \n",
    "    ).all())\n",
    "    if False:\n",
    "        for e, idx in zip(energy, energy_idx[:10]):\n",
    "            print(template_emids[idx], e, template_emids[idx+1])\n",
    "            #print(e, t_energy_bins[idx], t_energy_bins[idx+1])\n",
    "            #print(e, template_emids[idx-1], template_emids[idx], t_energy_bins[idx+1])\n",
    "            #print(np.logical_and(e > template_emids[idx-1], e < template_emids[idx+1]))\n",
    "    \n",
    "    # select corresponding flux\n",
    "    flux = template[ipix, energy_idx]\n",
    "    \n",
    "    return flux\n",
    "\n",
    "n_tota_dict = {}\n",
    "for csky_indexing in [True, False]:\n",
    "    print('Using csky indexing:', csky_indexing)\n",
    "    \n",
    "    n_samples = 1\n",
    "    flux = 0\n",
    "    for i in range(n_samples):\n",
    "        rng = np.random.RandomState(42 + i)\n",
    "        flux += get_flux_from_template(\n",
    "            ra=rng.uniform(0., 2*np.pi, size=len(a.sig.true_ra)),\n",
    "            dec=a.sig.true_dec,\n",
    "            energy=a.sig.true_energy,\n",
    "            #template=kra5,\n",
    "            template=kra5/3.,\n",
    "            #template=csky_kra5_template * 2/3,\n",
    "            #template=csky_kra5_template,\n",
    "            template_emids=ebins5,\n",
    "            csky_indexing=csky_indexing,\n",
    "        )\n",
    "    flux /= n_samples\n",
    "    n_total = np.sum(a.sig.oneweight * a.livetime * flux)\n",
    "    print(csky_indexing, n_total)\n",
    "    n_tota_dict[csky_indexing] = n_total\n",
    "    print()\n",
    "\n",
    "for key, tr in tr_dict.items():\n",
    "    print('csky n_total : {} [{}]'.format(tr._sig_injs[0].acc_total, key))\n",
    "print(n_tota_dict[False] / n_tota_dict[True], 1/(1.1**2)**(-2.5))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-packing",
   "metadata": {},
   "source": [
    "## Contribution Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-guide",
   "metadata": {},
   "source": [
    "#### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from dnn_cascade_selection.utils.notebook import ps_pdf\n",
    "\n",
    "def get_energy_and_space_contribution(trial, tr, gamma=2.7):\n",
    "    L = tr.get_one_llh_from_trial(trial)\n",
    "    \n",
    "    res = L.fit(**tr.fitter_args)\n",
    "    ns = res[1]['ns']\n",
    "    N = float(len(trial.evss[0][0]))\n",
    "    print(res, ns, N)\n",
    "    \n",
    "    space_eval = cy.inspect.get_space_eval(L, -1, 0) # 0: background events (1 would be for signal events)\n",
    "    energy_eval = cy.inspect.get_energy_eval(L, -1, 0)\n",
    "    SoB_space_ss = space_eval(gamma=gamma)[1] \n",
    "    SoB_energy = energy_eval(gamma=gamma)[0]\n",
    "    SoB_space = space_eval(gamma=gamma)[0] \n",
    "    #w = (SoB_space - SoB_space_ss) * SoB_energy\n",
    "    #lr = w * ns/N + 1.\n",
    "    return SoB_space, SoB_space_ss, SoB_energy, ns, N\n",
    "\n",
    "def get_lr_from_trial(trial, tr, gamma=2.7):\n",
    "    \"\"\"Get event likelihood-ratio value of signal-subtracted likelihood\n",
    "    \n",
    "    Info here:\n",
    "        https://wiki.icecube.wisc.edu/index.php/\n",
    "        Cascade_7yr_PS_GP/Galactic_Source_Search_Methods#Signal-Subtracted_Likelihood\n",
    "    \"\"\"\n",
    "    L = tr.get_one_llh_from_trial(trial)\n",
    "    \n",
    "    res = L.fit(**tr.fitter_args)\n",
    "    ns = res[1]['ns']\n",
    "    N = float(len(trial.evss[0][0]))\n",
    "    print(res, ns, N)\n",
    "    \n",
    "    space_eval = cy.inspect.get_space_eval(L, -1, 0) # 0: background events (1 would be for signal events)\n",
    "    energy_eval = cy.inspect.get_energy_eval(L, -1, 0)\n",
    "    StoB_space_ss = space_eval(gamma=gamma)[1] \n",
    "    SoB_energy = energy_eval(gamma=gamma)[0]\n",
    "    SoB_space = space_eval(gamma=gamma)[0] \n",
    "    w = (SoB_space - StoB_space_ss) * SoB_energy\n",
    "    lr = w * ns/N + 1.\n",
    "    return lr\n",
    "    \n",
    "def get_lr(template_str, gamma=2.7, seed=None, TRUTH=True):\n",
    "    \"\"\"Get event likelihood-ratio value of signal-subtracted likelihood\n",
    "    \n",
    "    Info here:\n",
    "        https://wiki.icecube.wisc.edu/index.php/\n",
    "        Cascade_7yr_PS_GP/Galactic_Source_Search_Methods#Signal-Subtracted_Likelihood\n",
    "    \"\"\"\n",
    "    trial = tr_dict[template_str].get_one_trial(seed=seed, TRUTH=TRUTH)\n",
    "    return get_lr_from_trial(trial=trial, tr=tr_dict[template_str], gamma=gamma)\n",
    "\n",
    "def get_contribution_list(template_str, N, w, ws, seed=None, TRUTH=True):\n",
    "    trial = tr_dict[template_str].get_one_trial(seed=seed, TRUTH=TRUTH)\n",
    "    \n",
    "    ci_list = []\n",
    "    ra_list = []\n",
    "    dec_list = []\n",
    "    sigma_list = []\n",
    "    for a in tqdm(ws[::-1][:N]):\n",
    "        mask = trial.evss[0][0].idx == a\n",
    "        dec = trial.evss[0][0][mask].dec[0]\n",
    "        ra = trial.evss[0][0][mask].ra[0]\n",
    "        sigma = trial.evss[0][0][mask].sigma[0]\n",
    "        w_i = w[a]\n",
    "        \n",
    "        #ci_list.append(10**(w_i))\n",
    "        ci_list.append(w_i)\n",
    "        ra_list.append(ra)\n",
    "        dec_list.append(dec)\n",
    "        sigma_list.append(sigma)\n",
    "    \n",
    "    ci_list = np.array(ci_list)\n",
    "    ra_list = np.array(ra_list)\n",
    "    dec_list = np.array(dec_list)\n",
    "    sigma_list = np.array(sigma_list)\n",
    "    return ci_list, ra_list, dec_list, sigma_list\n",
    "\n",
    "def compute_pixel(args):\n",
    "    ipix, nside, ci_list, ra_list, dec_list, sigma_list = args\n",
    "    theta, phi = hp.pix2ang(nside=nside, ipix=ipix)\n",
    "    dec = np.pi/2.  - theta\n",
    "    ra = phi\n",
    "\n",
    "    ang_dist = cy.coord.delta_angle(\n",
    "        zenith1=dec,\n",
    "        azimuth1=ra,\n",
    "        zenith2=dec_list,\n",
    "        azimuth2=ra_list,\n",
    "        latlon=True,\n",
    "    )\n",
    "    space_pdf = ps_pdf.von_mises_pdf(ang_dist, sigma=sigma_list)\n",
    "\n",
    "    pixel_value = np.sum(space_pdf * ci_list)\n",
    "    return pixel_value\n",
    "    \n",
    "def get_contribution_map(template_str, N, w, ws, nside=64, cpus=25, normalize=False, seed=None, TRUTH=True):\n",
    "    ci_list, ra_list, dec_list, sigma_list = get_contribution_list(\n",
    "        template_str=template_str, N=N, w=w, ws=ws, seed=seed, TRUTH=TRUTH)\n",
    "    \n",
    "    npix = hp.nside2npix(nside)\n",
    "    arg_list = [(i, nside, ci_list, ra_list, dec_list, sigma_list) for i in range(npix)]\n",
    "    \n",
    "    if cpus > 1:\n",
    "        print('Running pool with {} cpus'.format(cpus))\n",
    "        \n",
    "\n",
    "        with Pool(cpus) as p:\n",
    "            skymap = list(tqdm(p.imap(compute_pixel, arg_list), total=npix))\n",
    "        skymap = np.array(skymap)\n",
    "        p.close()\n",
    "    else:\n",
    "        skymap = np.zeros(npix)\n",
    "        for i in tqdm(range(npix), total=npix):\n",
    "            skymap[i] = compute_pixel(arg_list[i])\n",
    "    \n",
    "    # normalize skymap\n",
    "    if normalize:\n",
    "        skymap = np.array(skymap) / np.sum(skymap) / hp.nside2pixarea(nside=nside)\n",
    "    return skymap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-operator",
   "metadata": {},
   "source": [
    "#### Space vs Energy Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "SoB_space, SoB_space_ss, SoB_energy, ns, N = get_energy_and_space_contribution(\n",
    "    trial=tr_dict['pi0'].get_one_trial(TRUTH=True),\n",
    "    tr=tr_dict['pi0'],\n",
    ")\n",
    "def get_ts(SoB_space, SoB_space_ss, SoB_energy, ns, N):\n",
    "    \n",
    "    # shape: [1, n_events]\n",
    "    SoB_space = SoB_space[np.newaxis, ]\n",
    "    SoB_space_ss = SoB_space_ss[np.newaxis, ]\n",
    "    SoB_energy = SoB_energy[np.newaxis, ]\n",
    "    \n",
    "    # shape: [n_ns_vals, 1]\n",
    "    ns = np.atleast_1d(ns)\n",
    "    ns = ns[:, np.newaxis]\n",
    "    \n",
    "    ns_over_N = ns/N\n",
    "    \n",
    "    # shape: [1, n_events] * [n_ns_vals, 1] = [n_ns_vals, n_events]\n",
    "    ts_events = 2*np.log((SoB_space - SoB_space_ss) * SoB_energy * ns_over_N + 1)\n",
    "    \n",
    "    # shape: [n_ns_vals]\n",
    "    ts = np.sum(ts_events, axis=1)\n",
    "    \n",
    "    return ts\n",
    "\n",
    "\n",
    "x = np.linspace(0., 2000, 1000)\n",
    "ts_vals = get_ts(\n",
    "    SoB_space=SoB_space, \n",
    "    SoB_space_ss=SoB_space_ss, \n",
    "    SoB_energy=SoB_energy, \n",
    "    ns=x, \n",
    "    N=N,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, ts_vals)\n",
    "ax.axvline(ns, ls='--', color='0.7')\n",
    "ax.set_xlabel('$n_s$')\n",
    "ax.set_ylabel('TS')\n",
    "\n",
    "ts = get_ts(\n",
    "    SoB_space=SoB_space, \n",
    "    SoB_space_ss=SoB_space_ss, \n",
    "    SoB_energy=SoB_energy, \n",
    "    ns=ns, \n",
    "    N=N,\n",
    ")\n",
    "print('ts', ts)\n",
    "print('ns, N', ns, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "bins = np.linspace(-20, 30, 100)\n",
    "ax.hist(SoB_space, bins=bins, histtype='step', label='SoB_space')\n",
    "ax.hist(SoB_space_ss, bins=bins, histtype='step', label='SoB_space_ss')\n",
    "ax.hist(SoB_energy, bins=bins, histtype='step', label='SoB_energy')\n",
    "ax.hist((SoB_space - SoB_space_ss)*SoB_energy, bins=bins, histtype='step', label='Combined Contribution')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-moderator",
   "metadata": {},
   "source": [
    "### Save event contributions for data release table\n",
    "\n",
    "This creates a table where each row corresponds to an event.\n",
    "Columns include contributions from \n",
    "- $(S/B)_\\mathrm{space}$\n",
    "- $(\\tilde{S}/B)_\\mathrm{space}$\n",
    "- $(S/B)_\\mathrm{energy}$\n",
    "\n",
    "\n",
    "for each of the three Galactic Plane models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.DataFrame()\n",
    "for key, tr in tr_dict.items():\n",
    "    SoB_space, SoB_space_ss, SoB_energy, ns, N = get_energy_and_space_contribution(\n",
    "        trial=tr.get_one_trial(TRUTH=True),\n",
    "        tr=tr,\n",
    "    )\n",
    "    df_events['{}_SoB_space'.format(key)] = SoB_space\n",
    "    df_events['{}_SoB_space_ss'.format(key)] = SoB_space_ss\n",
    "    df_events['{}_SoB_energy'.format(key)] = SoB_energy\n",
    "\n",
    "# add reconstructed energy\n",
    "df_events['energy'] = a.bg_data.energy\n",
    "\n",
    "df_events = df_events.reset_index()\n",
    "\n",
    "# save to file\n",
    "df_events.to_csv(\n",
    "    '{}/event_contributions_gp.csv'.format(plot_dir),\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "df_events "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-crime",
   "metadata": {},
   "source": [
    "#### Compute event contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dict = {\n",
    "    'pi0': get_lr('pi0'),\n",
    "    'kra5': get_lr('kra5', None),\n",
    "    'kra50': get_lr('kra50', None),\n",
    "}\n",
    "\n",
    "ts_dict = {}\n",
    "for key, lr in lr_dict.items():\n",
    "    ts = 2 * np.log(lr)\n",
    "    print(key, np.sum(ts))\n",
    "    ts_dict[key] = ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_scrample_seed = 4\n",
    "lr_bkg_dict = {\n",
    "    'pi0': get_lr('pi0', seed=bkg_scrample_seed, TRUTH=False),\n",
    "    'kra5': get_lr('kra5', None, seed=bkg_scrample_seed, TRUTH=False),\n",
    "    'kra50': get_lr('kra50', None, seed=bkg_scrample_seed, TRUTH=False),\n",
    "}\n",
    "\n",
    "ts_bkg_dict = {}\n",
    "for key, lr in lr_bkg_dict.items():\n",
    "    ts = 2 * np.log(lr)\n",
    "    print(key, np.sum(ts))\n",
    "    ts_bkg_dict[key] = ts\n",
    "    \n",
    "    try:\n",
    "        bg_tsd = cy.dists.TSD(bkg_dict[key])\n",
    "        total_ts = np.sum(ts)\n",
    "        print('  {} | ts: {:3.3f} | p-value: {:3.3f} | n-sigma: {:3.3f}'.format(\n",
    "            key, total_ts, bg_tsd.sf(total_ts), bg_tsd.sf_nsigma(total_ts)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "print('bkg_scrample_seed', bkg_scrample_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-association",
   "metadata": {},
   "source": [
    "#### Compute contribution Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-antique",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nside = 64\n",
    "\n",
    "skymaps_all = {}\n",
    "for key, ts_values in ts_dict.items():\n",
    "    print('Creating contributions for {}'.format(key))\n",
    "    \n",
    "    sorted_idx = np.argsort(ts_values)\n",
    "    skymaps_all[key] = get_contribution_map(\n",
    "        template_str=key, N=len(ts_values), w=ts_values, ws=sorted_idx, nside=nside)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "skymaps_all_bkg = {}\n",
    "for key, ts_values in ts_bkg_dict.items():\n",
    "    print('Creating contributions for {}'.format(key))\n",
    "    \n",
    "    sorted_idx = np.argsort(ts_values)\n",
    "    skymaps_all_bkg[key] = get_contribution_map(\n",
    "        template_str=key, N=len(ts_values), w=ts_values, ws=sorted_idx, nside=nside, \n",
    "        seed=bkg_scrample_seed, TRUTH=False,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-curve",
   "metadata": {},
   "source": [
    "#### Save contribution maps to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contribution = pd.DataFrame()\n",
    "\n",
    "df_contribution['ipix'] = np.arange(hp.nside2npix(nside))\n",
    "theta, phi = hp.pix2ang(nside=nside, ipix=df_contribution['ipix'])\n",
    "df_contribution['dec'] = np.pi/2.  - theta\n",
    "df_contribution['ra'] = phi\n",
    "    \n",
    "for key, skymap in skymaps_all.items():\n",
    "    df_contribution[key] = skymap\n",
    "\n",
    "for key, skymap in skymaps_all_bkg.items():\n",
    "    df_contribution['bkg_trial_' + key] = skymap\n",
    "\n",
    "df_contribution.to_csv(\n",
    "    '{}/contribution_map.csv'.format(plot_dir),\n",
    "    index=False,\n",
    ")\n",
    "df_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how these are saved\n",
    "if True:\n",
    "    df_contribution_ = pd.read_csv('{}/contribution_map.csv'.format(plot_dir))\n",
    "    for key, _ in skymaps_all.items():\n",
    "\n",
    "        # load from csv\n",
    "        skymap = df_contribution_[key]\n",
    "\n",
    "        hp.mollview(skymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-robertson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-wrestling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "traditional-organic",
   "metadata": {},
   "source": [
    "# Scratch Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-martial",
   "metadata": {},
   "source": [
    "#### Check bkg trial distributions after csky bug fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gp_tr_mod(template, template_str, ana, cutoff=np.inf, gamma=None, cpus=20):\n",
    "    cutoff_GeV = cutoff * 1e3\n",
    "    gp_conf = cg.get_gp_conf(\n",
    "        template_str=template_str, gamma=gamma, \n",
    "        cutoff_GeV=cutoff_GeV, base_dir=cg.base_dir)\n",
    "    \n",
    "    gp_conf.pop('dir')\n",
    "    gp_conf['extra_keeps'] = ['azimuth']\n",
    "    gp_conf['template'] = template\n",
    "\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr\n",
    "\n",
    "def get_kra5_tr_mod(ana, template, energy_bins, cpus=30):\n",
    "    kra_flux = cy.hyp.BinnedFlux(\n",
    "        bins_energy=energy_bins,\n",
    "        flux=template.sum(axis=0)*hp.nside2pixarea(hp.npix2nside(len(template))))\n",
    "    gp_conf = {\n",
    "        'template': template,\n",
    "        'bins_energy': energy_bins,\n",
    "        'randomize': ['ra'],\n",
    "        'update_bg': True,\n",
    "        'fast_weight': False,\n",
    "        'sigsub': True,\n",
    "        cy.pdf.CustomFluxEnergyPDFRatioModel: dict(\n",
    "            hkw=dict(bins=(\n",
    "                   np.linspace(-1, 1, 20),\n",
    "                   np.linspace(np.log10(500), 8.001, 20)\n",
    "                   )),\n",
    "            flux=kra_flux,\n",
    "            features=['sindec', 'log10energy'],\n",
    "            normalize_axes=([1])),\n",
    "        'energy': False,\n",
    "    }\n",
    "\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr\n",
    "\n",
    "tr_kra5_mod = get_kra5_tr_mod(ana=ana, template=kra5/3., energy_bins=ebins5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_trials_kra5 = tr_kra5_mod.get_many_fits(cpus=30, n_trials=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-headquarters",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_bkg_trials_tsd = cy.dists.Chi2TSD(bkg_trials_kra5)\n",
    "plot_bkg_trials_tsd_pre = cy.dists.Chi2TSD(bkg_dict['kra5'][:10000000])\n",
    "bins = np.linspace(0, 30, 50)\n",
    "fig, ax, h = plot_bkg_trials(plot_bkg_trials_tsd, density=True, bins=bins)\n",
    "plot_bkg_trials(plot_bkg_trials_tsd_pre, density=True, bins=bins, ax=ax, fig=fig, color='orange')\n",
    "ax.set_xlabel('TS [KRA-$\\gamma$ 5]')\n",
    "fig.savefig('{}/ts_comparison_kra5.png'.format(plot_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-breath",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.3_py3-v4.1.0_csky",
   "language": "python",
   "name": "tensorflow2.3_py3-v4.1.0_csky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
