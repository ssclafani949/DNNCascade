{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "helpful-thong",
   "metadata": {},
   "source": [
    "# Data Release Example Scripts: GP Flux Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-location",
   "metadata": {},
   "source": [
    "This notebooks illustrates how the Galactic Plane analysis can be reproduced with the provided data release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-freight",
   "metadata": {},
   "source": [
    "## Import necessary python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats, optimize\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import healpy as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-import",
   "metadata": {},
   "source": [
    "## Define some global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directory that contains the data release files\n",
    "data_dir = '/data/user/mhuennefeld/data/analyses/DNNCascadeCodeReview/unblinding_checks/plots/data_release/create_data/DataFluxMeasurement/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-cheese",
   "metadata": {},
   "source": [
    "## Load event-level data from data release\n",
    "\n",
    "The event-level data contains the contributions to each of the likelihood-ratio terms and it is provided via csv files. We will load these via the python library pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_events = pd.read_csv(os.path.join(data_dir, 'event_contributions_gp.csv'))\n",
    "df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-customs",
   "metadata": {},
   "source": [
    "## Define Likelihood-ratio and optimization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-provision",
   "metadata": {},
   "source": [
    "As stated in the paper, the \"signal-subtracted\" likelihood is given by:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    " \\mathcal{L} (n_s, \\gamma) = \\prod_{i=1}^N \\frac{n_s}{N} S_i(\\gamma, \\delta_i, \\alpha_i, \\sigma_i, E_i) + \\tilde{D_i} (\\delta_i, E_i) - \\frac{n_s}{N}\\tilde{S_i} (\\delta_i, E_i) \n",
    "    \\label{eq:ssllh}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "with the PDF $\\tilde{D}$ obtained from the scrambled experimental data and \n",
    "$\\tilde{S}$ the scrambled signal PDF as obtained from MC.\n",
    "\n",
    "The test-statistic (TS) is defined by the ratio of the maximized likelihood and the likelihood for the null hypothesis:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "TS = -2 \\ln \\frac{ \\mathcal{L} (n_s = 0)}{ \\mathcal{L}(\\hat{n}_s,\\gamma)}\n",
    "    \\label{eq:TS}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Inserting the definitions for the likelihood in the above TS equation (and omitting the function arguments) results in\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "TS(n_s) = -2 \\ln \\frac{\n",
    "        \\prod_{i=1}^N \\frac{n_s}{N} S_i + \\tilde{D_i} - \\frac{n_s}{N}\\tilde{S_i}\n",
    "    }{ \n",
    "       \\prod_{i=1}^N \\tilde{D_i}\n",
    "    }\n",
    "   = -2 \\ln \\prod_{i=1}^N  \\left( \\frac{n_s}{N} \\cdot \n",
    "        \\frac{S_i -\\tilde{S_i}}{\\tilde{D_i}}\n",
    "    + 1\n",
    "    \\right)\n",
    "   = -2 \\sum_{i=1}^N \\ln \\left( \\frac{n_s}{N} \\cdot \n",
    "        \\frac{S_i -\\tilde{S_i}}{\\tilde{D_i}}\n",
    "    + 1\n",
    "    \\right)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Directly optimizing the test-statistic as a function of $n_s$ is equivalent to first optimizing the likelihood $\\mathcal L (n_s)$ and subsquently computing the test-statistic. The analysis software directly optimizes the test-statistic as a function of $n_s$. This has the added benefit that individual analysis PDFs for signal and background only need to be defined as ratios.\n",
    "Furthermore, the PDFs can be split up into an energy and spacial term. We can therefore also write:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "TS(n_s) \n",
    "   = -2 \\sum_{i=1}^N \\ln \\left( \\frac{n_s}{N} \\cdot \n",
    "        \\left(\n",
    "            \\frac{S_i^\\mathrm{space}}{\\tilde{D_i}^\\mathrm{space}}\n",
    "            - \\frac{\\tilde{S_i}^\\mathrm{space}}{\\tilde{D_i}^\\mathrm{space}}\n",
    "        \\right) \\cdot \n",
    "        \\frac{S_i^\\mathrm{energy}}{\\tilde{D_i}^\\mathrm{energy}}\n",
    "    + 1\n",
    "    \\right)\n",
    "   %= -2 \\sum_{i=1}^N \\ln \\left( \\frac{n_s}{N} \\cdot \n",
    "   %     \\frac{\\left(S_i^\\mathrm{space} -\\tilde{S_i}^\\mathrm{space}\\right) \\cdot S_i^\\mathrm{energy}}{\\tilde{D_i}^\\mathrm{space}\\cdot\\tilde{D_i}^\\mathrm{energy}}\n",
    "   % + 1\n",
    "   % \\right)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Here we used the fact that the signal energy PDF remains the same for scrambled and un-scrambled events, e.g. $S_i^\\mathrm{energy}$ = $\\tilde{S_i}^\\mathrm{energy}$.\n",
    "\n",
    "The individual ratio terms for each event are provided in the data release. In terms of the column names in the data file, this results in :\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "TS(n_s) \n",
    "   = -2 \\sum_{i=1}^N \\ln \\left( \\frac{n_s}{N} \\cdot \n",
    "        \\left(\n",
    "            \\mathrm{SoB\\_space}_i\n",
    "            - \\mathrm{SoB\\_space\\_ss}_i\n",
    "        \\right) \\cdot \n",
    "        \\mathrm{SoB\\_energy}_i\n",
    "    + 1\n",
    "    \\right)\n",
    "\\end{equation}\n",
    "$\n",
    "We will use this equation to define the python function `get_test_statistic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_test_statistic(ns, N, SoB_space, SoB_space_ss, SoB_energy):\n",
    "    \"\"\"Compute test-statistic\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ns : float\n",
    "        The number of signal events.\n",
    "    N : int\n",
    "        The total number of events.\n",
    "    SoB_space : array_like\n",
    "        The ratio of the signal and background space PDF.\n",
    "    SoB_space_ss : array_like\n",
    "        The ratio of the scrambled signal and background space PDF.\n",
    "    SoB_energy : array_like\n",
    "        The ratio of the signal and background energy PDF.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The test-statistic of the maximized likelihood ratio.\n",
    "    \"\"\"\n",
    "    ts = np.sum(2*np.log((SoB_space - SoB_space_ss) * SoB_energy * ns/N + 1))\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-pocket",
   "metadata": {},
   "source": [
    "We can now optimize the test-statistic. We will use scipy for the optimization and we will loop through each of the GP models. The best fit $\\hat{n}_s$ is then saved in the dictionary `res_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function we need to minimize\n",
    "def get_negative_test_statistic(*args, **kwargs):\n",
    "    return -get_test_statistic(*args, **kwargs)\n",
    "\n",
    "# define a dictionary which will hold our results\n",
    "res_dict = {}\n",
    "\n",
    "# loop through each GP model\n",
    "for key in ['pi0', 'kra5', 'kra50']:\n",
    "    \n",
    "    # Minimize negative test-statistic function via scipy\n",
    "    print('Now optimizing the test-statistic for the GP model {}'.format(key))\n",
    "    result = optimize.minimize(\n",
    "        get_negative_test_statistic, \n",
    "        x0=0, \n",
    "        args=(\n",
    "            len(df_events),  # N\n",
    "            df_events[key + '_SoB_space'],  # SoB_space\n",
    "            df_events[key + '_SoB_space_ss'],  # SoB_space_ss\n",
    "            df_events[key + '_SoB_energy'],  # SoB_energy\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # save fitted ns and ts to the dictionary\n",
    "    res_dict[key] = {\n",
    "        'ns': result.x[0],\n",
    "        'ts': -result.fun,\n",
    "    } \n",
    "    print('  ns: {:3.1f} | ts: {:3.2f}'.format(result.x[0], -result.fun))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-housing",
   "metadata": {},
   "source": [
    "## Compute Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-bradley",
   "metadata": {},
   "source": [
    "In order to compute the significance of rejecting the null-hypothesis, we need to compare the computed test-statistic value against background trials. Background trials are obtained by randomizing the experimental data by scrambling the right ascension value of the events. \n",
    "The test-statistic distribution of the background trials is provided in the data release via two files that define the bin edges and the bin heights of each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins = pd.read_csv(os.path.join(data_dir, 'bkg_trials_binning.csv'))\n",
    "df_values = pd.read_csv(os.path.join(data_dir, 'bkg_trials_values.csv'))\n",
    "df_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-paintball",
   "metadata": {},
   "source": [
    "The function `get_significance_from_hist` computes the significance to reject the null-hypothesis for a given test-statistic value by evaluating the fraction of background trials with an equally large or larger test-statist value. This fraction corresponds to the probability of falsely rejecting the null-hypothesis and is therefore considered as the `p-value`. We can then convert the p-value to \"multiples of sigma\" by checking what number of standard deviations such a p-value would correspond to for a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significance_from_hist(ts, bin_edges, bin_values):\n",
    "    \"\"\"Compute significance for a given test-statistic\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : float\n",
    "        The test-statistic value for which the significance will be calculated.\n",
    "    bin_edges : array_like\n",
    "        The bin edges for the background trial histogram.\n",
    "    bin_values : array_like\n",
    "        The bin heights for the background trial histogram.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The significance of rejecting the null hypothesis for the given\n",
    "        test-statistic value based on the background trials provided\n",
    "        via `bin_edges` and `bin_values`\n",
    "    \"\"\"\n",
    "    # get index for which is valid:\n",
    "    # edge[i-1] < ts <= edge[i]\n",
    "    # this correspdonds to the i-th entry in bin_values\n",
    "    index = np.searchsorted(bin_edges, ts)\n",
    "    \n",
    "    # get number of trials with larger ts values\n",
    "    n_larger = np.sum(bin_values[index:])\n",
    "    pval = 1.*n_larger/np.sum(bin_values)\n",
    "    nsigma = stats.norm.isf(pval)\n",
    "    \n",
    "    return pval, nsigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-round",
   "metadata": {},
   "source": [
    "Now we will compute the significance for each of the tested GP models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['pi0', 'kra5', 'kra50']:\n",
    "    pval, nsigma = get_significance_from_hist(\n",
    "        ts=res_dict[key]['ts'],\n",
    "        bin_edges=df_bins[key].values,\n",
    "        bin_values=df_values[key].values,\n",
    "    )\n",
    "    res_dict[key]['nsigma'] = nsigma\n",
    "    print('{:3.2f} sigma | GP model: {}'.format(nsigma, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-coast",
   "metadata": {},
   "source": [
    "## Convert $\\hat{n}_s$ to Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-stupid",
   "metadata": {},
   "source": [
    "In order to compute the flux for each of the GP models, the fitted number of signal events needs to be converted by utilizing the effective area of the event sample.\n",
    "\n",
    "We'll start by loading the effective area, which is provided as a 2D array with the first axis in sin(dec) and the second axis in energy (units of GeV). The binning edges are provided in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "effa_values = np.loadtxt(os.path.join(data_dir, 'effa_values.csv'), delimiter=',')\n",
    "print('Shape of 2D effective area:', effa_values.shape)\n",
    "\n",
    "effa_bins_sindec = np.loadtxt(os.path.join(data_dir, 'effa_bins_sindec.csv'), delimiter=',')\n",
    "print('Number of provided bin edges in sin(dec):', len(effa_bins_sindec))\n",
    "\n",
    "effa_bins_energy = np.loadtxt(os.path.join(data_dir, 'effa_bins_energy.csv'), delimiter=',')\n",
    "print('Number of provided bin edges in energy:', len(effa_bins_energy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-fault",
   "metadata": {},
   "source": [
    "In addition to the effective area, the galactic plane templates are also required. The KRA-$\\gamma$ templates are provided on zenodo at this location: https://zenodo.org/record/7070823 and the $\\pi^0$ is located here: https://galprop.stanford.edu/PaperIISuppMaterial/SS_Z4_R20_T150_C5.tar .\n",
    "If downloaded from the provided links and placed in the defined `data_dir`, these can be loaded via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_kra5, template_kra5_nu, template_kra5_nubar, template_ebins5 = np.load(\n",
    "    os.path.join(data_dir, 'KRA-gamma_5PeV_maps_energies.tuple.npy'), \n",
    "    allow_pickle = True, \n",
    "    encoding='latin1',\n",
    ")\n",
    "template_kra50, template_kra50_nu, template_kra50_nubar, template_ebins50 = np.load(\n",
    "    os.path.join(data_dir, 'KRA-gamma_maps_energies.tuple.npy'), \n",
    "    allow_pickle = True, \n",
    "    encoding='latin1',\n",
    ")\n",
    "template_pi0 = np.load(\n",
    "    os.path.join(data_dir, 'Fermi-LAT_pi0_map.npy'), \n",
    "    allow_pickle=True, \n",
    "    encoding='latin1',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-transport",
   "metadata": {},
   "source": [
    "#### Acceptance Correction for $\\pi^0$ model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-contrary",
   "metadata": {},
   "source": [
    "The $\\pi^0$ template is provided as a healpix map normalized over the sky. The units are $\\mathrm{sr}^-1$. The flux at each pixel is then simply the sky-integrated power-law flux multiplied by this spatial PDF. When combined with the effective area, the total number of expected events (`total_acceptance`) can be computed for a given reference normalization. The ratio to the number of observed signal events is then used to convert to a flux. This conversion is outlined in the function `acceptance_correct_pi0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptance_correct_pi0(\n",
    "            ns, template, eff_area, sindec_bins, energy_bins,\n",
    "            livetime=304047105.0735066,\n",
    "        ):\n",
    "    \"\"\"Convert a number of signal events to the corresponding flux\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ns : array_like\n",
    "        The number of signal events that will be converted to the\n",
    "        corresponding flux.\n",
    "    template : array_like\n",
    "        The spatial template in units of sr^-1.\n",
    "        Shape: [npix]\n",
    "    eff_area : array_like\n",
    "        The effective area binned in sin(dec) (unitless) along first\n",
    "        dimension and and in energy (in GeV) along the second axis.\n",
    "        shape: [n_bins_sindec, n_bins_energy]\n",
    "    sindec_bins : array_like\n",
    "        The bin edges along the first dimension corresponding to sin(dec).\n",
    "        Shape: [n_bins_sindec + 1]\n",
    "    energy_bins : TYPE\n",
    "        The bin edges along the second dimension corresponding to\n",
    "        the energy in GeV.\n",
    "        Shape: [n_bins_energy + 1]\n",
    "    livetime : float\n",
    "        The livetime of the dataset in seconds.\n",
    "        For the 10-year cascade dataset, the livetime corresponds to\n",
    "        304047105.0735066 seconds, which is the default value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        The flux in terms of E^2 dN/dE at 100 TeV in units\n",
    "        of TeV cm^-2 s^-1.\n",
    "    \"\"\"\n",
    "\n",
    "    npix = len(template)\n",
    "    nside = hp.npix2nside(npix)\n",
    "    \n",
    "    # First we need to construct Phi(sindec, energy)\n",
    "    # in units of GeV^-1 cm^-2 s^-1 sr^-1.\n",
    "    # We will do this by splitting Phi into the spatial term (units of sr^-1)\n",
    "    # and the energy term (units of GeV^-1 cm^-2 s^-1).\n",
    "    # We can do this splitting for the pi0 model because the energy spectrum\n",
    "    # is assumed not to depend on the location in the sky\n",
    "    theta, phi = hp.pix2ang(nside, np.r_[:npix])\n",
    "    pix_dec = np.pi/2. - theta\n",
    "    pix_ra = phi\n",
    "    pix_sindec = np.sin(pix_dec)\n",
    "\n",
    "    # Phi(sindec), shape: [n_sindec], units of sr^-1\n",
    "    phi_sindec = np.zeros(len(sindec_bins)-1)\n",
    "\n",
    "    # walk through each declination band\n",
    "    for i, sindec_max in enumerate(sindec_bins[1:]):\n",
    "        sindec_min = sindec_bins[i]\n",
    "\n",
    "        # get all pixels belonging to this dec band\n",
    "        mask_pixels = np.logical_and(\n",
    "            pix_sindec >= sindec_min,\n",
    "            pix_sindec < sindec_max,\n",
    "        )\n",
    "        phi_sindec[i] = np.sum(template[mask_pixels]) \n",
    "\n",
    "    # Sky-integrated, per-flavor (nu + nubar) flux\n",
    "    # Choose an arbitrary flux normalization to scale\n",
    "    norm = 1e-18  # in units of GeV^-1 cm^-2 s^-1\n",
    "    e0 = 1e5  # 100 TeV in units of GeV\n",
    "    gamma = 2.7\n",
    "\n",
    "    # compute average energy in each energy bin\n",
    "    energy_avg = (\n",
    "        (-gamma + 1) / (-gamma + 2) *\n",
    "        (energy_bins[1:]**(-gamma+2) - energy_bins[:-1]**(-gamma+2)) /\n",
    "        (energy_bins[1:]**(-gamma+1) - energy_bins[:-1]**(-gamma+1))\n",
    "    )\n",
    "\n",
    "    # Phi(energy) in units of GeV^-1 cm^-2 s^-1 sr^-1\n",
    "    # shape: [n_energy]\n",
    "    phi_e = norm * (energy_avg / e0) ** (-gamma)\n",
    "\n",
    "    # Phi(sindec, energy)\n",
    "    # shape: [n_sindec, n_energy],  GeV^-1 cm^-2 s^-1 sr^-1\n",
    "    phi = phi_e[np.newaxis] * phi_sindec[:, np.newaxis]\n",
    "\n",
    "    # \"integrate\" over energy bin width\n",
    "    # shape: [n_sindec, n_energy], cm^-2 s^-1 sr^-1\n",
    "    phi *= np.diff(energy_bins)[np.newaxis]\n",
    "\n",
    "    # \"integrate\" over solid angle\n",
    "    # shape: [n_pix, n_bins], s^-1 * cm^-2\n",
    "    phi *= hp.nside2pixarea(nside)\n",
    "\n",
    "    # Now we can compute the total number of expected events\n",
    "    # for the given flux.\n",
    "    # eff_area is in units of cm^-2 and livetime in units of s\n",
    "    total_acceptance = livetime * np.sum(eff_area * phi)\n",
    "\n",
    "    # we can then compute the ratio of the number of signal events versus\n",
    "    # the number of events one would have expected based on the previously\n",
    "    # defined flux. This can then be used to scale the arbitrary flux\n",
    "    # normalization that we chose as a reference point.\n",
    "    model_norm = ns / total_acceptance\n",
    "\n",
    "    # compute E^2 dN/dE at 100 TeV in units of TeV cm^-2 s^-1\n",
    "    norm_tev = norm * 1e3  # in units of TeV^-1 cm^-2 s^-1\n",
    "    norm_E2_tev = norm_tev * (100)**2\n",
    "    return model_norm*norm_E2_tev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-protection",
   "metadata": {},
   "source": [
    "We are now almost ready to convert the best-fit $\\hat{n}_s$ to the corresponding flux in terms of $E^2 \\frac{dN}{dE}$ at 100 TeV in units of $\\mathrm{TeV}^{-1} \\mathrm{cm}^{-2} \\mathrm{s}^{-1}$.\n",
    "The last remaining step is to bias-correct the best-fit $\\hat{n}_s$. In case of mis-modeling of the PDF ratios, the fitted number of signal events may have a systematic shift and thus not properly recover the true underlying signal events. This bias can be evaluated from MC injection trials. A number of signal trials are performed in which a given number of signal events $n_\\mathrm{inj}$ are injected on top of the scrambled background distribution. For each of these trials, the analysis is run and the best-fit $\\hat{n}_s$ is recorded. The bias can then be evaluated by comparing the median of the best-fit $\\hat{n}_s$ distribution in comparison to the true number of injected signal events $n_\\mathrm{inj}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from signal trials\n",
    "df_ns_bias_pi0 = pd.read_csv(os.path.join(data_dir, 'ns_bias_pi0.csv'))\n",
    "\n",
    "# plot the ns-bias together with the correction function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_ns_bias_pi0['n_inj'], df_ns_bias_pi0['median_ns'], label=r'$\\pi^0$')\n",
    "ax.set_title('Model: {}'.format(key))\n",
    "ax.set_xlabel('$n_\\mathrm{inj}$')\n",
    "ax.set_ylabel('$\\hat{n}_s$')\n",
    "max_val = np.max(df_ns_bias_pi0['median_ns'])\n",
    "ax.plot((0., max_val), (0., max_val), ls='--', color='0.7')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-british",
   "metadata": {},
   "source": [
    "As seen above, a slight bias is visible: the fitted number of events $\\hat{n}_s$ tend to overestimate the true injected number of events. We will correct for this bias in order to obtain a better estimate of the true number of signal events by defining a bias-correction function `bias_corr_func_pi0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a spline function to the value pairs\n",
    "# we will use this later to bias-correct the best-fit ns\n",
    "sorted_idx = np.argsort(df_ns_bias_pi0['median_ns'])\n",
    "bias_corr_func_pi0 = UnivariateSpline(\n",
    "    x=df_ns_bias_pi0['median_ns'][sorted_idx], \n",
    "    y=df_ns_bias_pi0['n_inj'][sorted_idx], \n",
    "    # choose an appropriate smoothing for the spline\n",
    "    s=len(df_ns_bias_pi0['n_inj'])*500,\n",
    ")\n",
    "\n",
    "print('Bias corrected ns: {:3.1f}'.format(bias_corr_func_pi0(res_dict['pi0']['ns'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-safety",
   "metadata": {},
   "source": [
    "Now we can convert the bias-corrected $\\hat{n}_s$ to the corresponding flux in terms of $E^2 \\frac{dN}{dE}$ at 100 TeV in units of $\\mathrm{TeV}^{-1} \\mathrm{cm}^{-2} \\mathrm{s}^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "E2dNdE_pi0 = acceptance_correct_pi0(\n",
    "    ns=bias_corr_func_pi0(res_dict['pi0']['ns']),\n",
    "    template=template_pi0,\n",
    "    eff_area=effa_values,\n",
    "    sindec_bins=effa_bins_sindec,\n",
    "    energy_bins=effa_bins_energy,\n",
    ")\n",
    "E2dNdE_pi0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-local",
   "metadata": {},
   "source": [
    "The remaining <1% difference to the flux value stated in the paper is due to the necessary discretization of the effective area, while the analysis directly uses MC simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-flesh",
   "metadata": {},
   "source": [
    "## ToDo\n",
    "\n",
    "- add acceptance correction for KRA-$\\gamma$ models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-mistress",
   "metadata": {},
   "source": [
    "## Plot flux (simplified version of Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlaw(energy, E2dNdE_at_100TeV, gamma=2.7):\n",
    "    \"\"\"Powerlaw flux\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    energy : array_like\n",
    "        The energy in GeV at which to evaluate the powerlaw flux.\n",
    "    E2dNdE_at_100TeV : array_like\n",
    "        The powerlaw normalization in terms of E^2 dN/dE at 100 TeV\n",
    "        in units of TeV^-1 cm^-2 s^-1.\n",
    "    gamma : float, optional\n",
    "        The spectral index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        The powerlaw flux evaluated at the provided energies in terms\n",
    "        of E^2 dN/dE in units of GeV^-1 cm^-2 s^-1.\n",
    "    \"\"\"\n",
    "    e0 = 1e5  # 100 TeV\n",
    "\n",
    "    # get normalization in dN/dE in units of GeV^-1 cm^-2 s^-1\n",
    "    norm = E2dNdE_at_100TeV / (e0)**2 * 1e3\n",
    "    dNdE = norm * (energy/e0)**(-gamma)\n",
    "    return dNdE * energy**2\n",
    "\n",
    "energies = np.logspace(np.log10(500), 6.5, 100)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(energies, powerlaw(energies, E2dNdE_at_100TeV=E2dNdE_pi0), label=r'$\\pi^0$ best-fit $\\nu$ flux')\n",
    "ax.set_ylabel(r'E$_{\\nu}^2$ $\\frac{dN}{dE_\\nu}$ [GeV s$^{-1}$ cm$^{-2}$]')\n",
    "ax.set_xlabel(R'E$_\\nu$ [GeV]')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-subject",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
