{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# set env flags to catch BLAS used for scipy/numpy \n",
    "# to only use 1 cpu, n_cpus will be totally controlled by csky\n",
    "if False:\n",
    "    os.environ['MKL_NUM_THREADS'] = \"1\"\n",
    "    os.environ['NUMEXPR_NUM_THREADS'] = \"1\"\n",
    "    os.environ['OMP_NUM_THREADS'] = \"1\"\n",
    "    os.environ['OPENBLAS_NUM_THREADS'] = \"1\"\n",
    "    os.environ['VECLIB_MAXIMUM_THREADS'] = \"1\"\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'w'\n",
    "mpl.rcParams['savefig.facecolor'] = 'w'\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, cm\n",
    "import csky as cy\n",
    "from csky import cext\n",
    "import numpy as np\n",
    "import astropy\n",
    "#from icecube import astro\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import histlite as hl\n",
    "import healpy\n",
    "import healpy as hp\n",
    "import socket\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import copy\n",
    "healpy.disable_warnings()\n",
    "plt.rc('figure', facecolor = 'w')\n",
    "plt.rc('figure', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-nevada",
   "metadata": {},
   "source": [
    "## Define Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_version = 'version-001-p00'\n",
    "\n",
    "host_name = socket.gethostname()\n",
    "\n",
    "if 'cobalt' in host_name:\n",
    "    print('Working on Cobalts')\n",
    "    #data_prefix = '/data/user/ssclafani/data/cscd/final'\n",
    "    #ana_dir = '/data/user/ssclafani/data/analyses/'\n",
    "    plot_dir = cy.utils.ensure_dir('/data/user/mhuennefeld/data/analyses/DNNCascadeCodeReview/unblinding_checks/plots/unblinding/confidence_intervals')\n",
    "    \n",
    "else:\n",
    "    raise ValueError('Unknown host:', host_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_path in [plot_dir]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        print('Creating directory:', dir_path)\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-mystery",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = cy.selections.Repository()\n",
    "specs = cy.selections.DNNCascadeDataSpecs.DNNC_10yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ana = cy.get_analysis(\n",
    "    repo, selection_version, specs, \n",
    "    #gammas=np.r_[0.1:6.01:0.125],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ana.anas[0]\n",
    "a.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.bg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-decade",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycle\n",
    "from copy import deepcopy\n",
    "\n",
    "soft_colors = cy.plotting.soft_colors\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\n",
    "def get_bias_allt(tr, n_trials=200, n_sigs=np.r_[:101:10], quiet=False):\n",
    "    trials = [\n",
    "        (None if quiet else print(f'\\r{n_sig:4d} ...', end='', flush=True))\n",
    "        or\n",
    "        tr.get_many_fits(n_trials, n_sig=n_sig, logging=False, seed=n_sig)\n",
    "        for n_sig in n_sigs]\n",
    "    if not quiet:\n",
    "        print()\n",
    "    for (n_sig, t) in zip(n_sigs, trials):\n",
    "        t['ntrue'] = np.repeat(n_sig, len(t))\n",
    "    allt = cy.utils.Arrays.concatenate(trials)\n",
    "    return allt\n",
    "\n",
    "def get_color_cycler():\n",
    "    return cycle(colors)\n",
    "\n",
    "def plot_ns_bias(ax, tr, allt, label=''):\n",
    "\n",
    "    n_sigs = np.unique(allt.ntrue)\n",
    "    dns = np.mean(np.diff(n_sigs))\n",
    "    ns_bins = np.r_[n_sigs - 0.5*dns, n_sigs[-1] + 0.5*dns]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "\n",
    "    h = hl.hist((allt.ntrue, allt.ns), bins=(ns_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1),errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = ns_bins[[0, -1]]\n",
    "    ax.set_xlim(ax.set_ylim(lim))\n",
    "    ax.plot(lim, lim, **expect_kw)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ax.set_xlabel(r'$n_{inj}$')\n",
    "    ax.set_ylabel(r'$n_s$')\n",
    "    ax.grid()\n",
    "\n",
    "def plot_gamma_bias(ax, tr, allt, label=''):\n",
    "\n",
    "    n_sigs = np.unique(allt.ntrue)\n",
    "    dns = np.mean(np.diff(n_sigs))\n",
    "    ns_bins = np.r_[n_sigs - 0.5*dns, n_sigs[-1] + 0.5*dns]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "    expect_gamma = tr.sig_injs[0].flux[0].gamma\n",
    "\n",
    "    h = hl.hist((allt.ntrue, allt.gamma), bins=(ns_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1),errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = ns_bins[[0, -1]]\n",
    "    ax.set_xlim(lim)\n",
    "    ax.set_ylim(1, 4)\n",
    "    ax.axhline(expect_gamma, **expect_kw)\n",
    "\n",
    "    ax.set_xlabel(r'$n_{inj}$')\n",
    "    ax.set_ylabel(r'$\\gamma$')\n",
    "    ax.grid()\n",
    "\n",
    "def plot_bkg_trials(\n",
    "            bg, fig=None, ax=None, \n",
    "            label='{} bg trials', \n",
    "            label_fit=r'$\\chi^2[{:.2f}\\mathrm{{dof}},\\ \\eta={:.3f}]$', \n",
    "            color=colors[0],\n",
    "            density=False,\n",
    "            bins=50,\n",
    "        ):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    if density:\n",
    "        h = bg.get_hist(bins=bins).normalize()\n",
    "    else:\n",
    "        h = bg.get_hist(bins=bins)\n",
    "    if label is not None:\n",
    "        label = label.format(bg.n_total)\n",
    "    hl.plot1d(ax, h, crosses=True, color=color, label=label)\n",
    "\n",
    "    # compare with the chi2 fit:\n",
    "    if hasattr(bg, 'pdf'):\n",
    "        x = h.centers[0]\n",
    "        norm = h.integrate().values\n",
    "        if label_fit is not None:\n",
    "            label_fit = label_fit.format(bg.ndof, bg.eta)\n",
    "        if density:\n",
    "            ax.semilogy(x, bg.pdf(x), lw=1, ls='--', label=label_fit, color=color)\n",
    "        else:\n",
    "            ax.semilogy(x, norm * bg.pdf(x), lw=1, ls='--', label=label_fit, color=color)\n",
    "\n",
    "    ax.set_xlabel(r'TS')\n",
    "    if density:\n",
    "        ax.set_ylabel(r'Density')\n",
    "    else:\n",
    "        ax.set_ylabel(r'number of trials')\n",
    "    ax.legend()\n",
    "        \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-bridges",
   "metadata": {},
   "source": [
    "## Setup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import config as cg\n",
    "\n",
    "cg.base_dir = '/data/user/mhuennefeld/data/analyses/unblinding_v1.0.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gp_tr(template_str, cutoff=np.inf, gamma=None, cpus=20):\n",
    "    cutoff_GeV = cutoff * 1e3\n",
    "    gp_conf = cg.get_gp_conf(\n",
    "        template_str=template_str, gamma=gamma, \n",
    "        cutoff_GeV=cutoff_GeV, base_dir=cg.base_dir)\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr\n",
    "\n",
    "def get_template_tr(template, gamma=2.7, cutoff_tev=np.inf, cpus=20):\n",
    "    cutoff_gev = cutoff_tev * 1000.\n",
    "    gp_conf = {\n",
    "        'template': template,\n",
    "        'flux': cy.hyp.PowerLawFlux(gamma, energy_cutoff=cutoff_gev),\n",
    "        'randomize': ['ra'],\n",
    "        'fitter_args': dict(gamma=gamma),\n",
    "        'sigsub': True,\n",
    "        'update_bg': True,\n",
    "        'fast_weight': False,\n",
    "    }\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr\n",
    "\n",
    "def get_gp_tr_sys(ana, template_str, cutoff=np.inf, gamma=None, cpus=20, sigmas=[0]):\n",
    "    cutoff_GeV = cutoff * 1e3\n",
    "    gp_conf = cg.get_gp_conf(\n",
    "        template_str=template_str, gamma=gamma, \n",
    "        cutoff_GeV=cutoff_GeV, base_dir=cg.base_dir)\n",
    "    gp_conf.pop('dir')\n",
    "    gp_conf['sigmas'] = sigmas\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-converter",
   "metadata": {},
   "source": [
    "#### SnowStorm Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_dir = '/data/ana/PointSource/DNNCascade/analysis/{}/'.format(selection_version)\n",
    "df = pd.read_hdf(\n",
    "    df_dir + '/systematics/SnowStorm_Spice321/MC_NuGen_snowstorm_214xx.hdf', key='df',\n",
    ")\n",
    "df = df[['SnowstormParameters_{:05d}'.format(i) for i in range(6)] + ['run', 'energy', 'ow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from IPython.utils import io\n",
    "\n",
    "sim_ranges = {\n",
    "    'Scattering': [0.9, 1.1],\n",
    "    'Absorption': [0.9, 1.1],\n",
    "    'AnisotropyScale': [0., 2.],\n",
    "    'DOMEfficiency': [0.9, 1.1],\n",
    "    'HoleIceForward_Unified_00': [-1.0, 1.0],\n",
    "    'HoleIceForward_Unified_01': [-0.2, 0.2],\n",
    "}\n",
    "\n",
    "allowed_ranges = {\n",
    "    'Scattering': [0.9, 1.1],\n",
    "    'Absorption': [0.9, 1.1],\n",
    "    'AnisotropyScale': [0., 2.],\n",
    "    'DOMEfficiency': [0.9, 1.1],\n",
    "    \n",
    "    # slightly increase range from recommendation to not have too little stats\n",
    "    'HoleIceForward_Unified_00': [-0.75, 0.45], #[-0.5, 0.3],\n",
    "    'HoleIceForward_Unified_01': [-0.15, 0.075], #[-0.1, 0.05],\n",
    "}\n",
    "\n",
    "\n",
    "def get_snowstorm_ana(sys_ranges, sim_ranges=sim_ranges):\n",
    "    \n",
    "    # define SnowStorm dataset with reduced range\n",
    "    class DNNCascade_10yr_sys_reduced(cy.selections.DNNCascadeDataSpecs.DNNCascade_10yr_snowstorm_fullrange):\n",
    "        def dataset_modifications(self, ds):\n",
    "            print('Adding SnowStorm Parameters to MC')\n",
    "            path_sig_df = (\n",
    "                '/data/ana/PointSource/DNNCascade/analysis/' + \n",
    "                self._path_sig.format(version=self._version).replace('dnn_cascades/', '').replace('.npy', '.hdf')\n",
    "            )\n",
    "            # (use global df to avoid loading multiple times)\n",
    "            #if df is None:\n",
    "            #    df = pd.read_hdf(path_sig_df, key='df')\n",
    "            assert np.allclose(df['run'], ds.sig.run)\n",
    "            assert np.allclose(df['energy'], ds.sig.energy)\n",
    "            assert np.allclose(df['ow'], ds.sig.oneweight)\n",
    "\n",
    "            # load and rename SnowStorm parameters\n",
    "            parameter_names=[\n",
    "                'Scattering', 'Absorption', 'AnisotropyScale', \n",
    "                'DOMEfficiency', 'HoleIceForward_Unified_00', \n",
    "                'HoleIceForward_Unified_01',\n",
    "            ]\n",
    "            for i, param in enumerate(parameter_names):\n",
    "                ds.sig[param] = np.array(df['SnowstormParameters_{:05d}'.format(i)])\n",
    "            \n",
    "            print('Reducing Dataset')\n",
    "            mask = np.ones(len(ds.sig), dtype=bool)\n",
    "            factor = 1.\n",
    "            for param, sys_range in sys_ranges.items():\n",
    "                if sys_range != sim_ranges[param]:\n",
    "                    assert sys_range[0] < sys_range[1], sys_range\n",
    "                    factor *= (sys_range[1] - sys_range[0]) / (sim_ranges[param][1] - sim_ranges[param][0])\n",
    "                    mask_i = np.logical_and(\n",
    "                        ds.sig[param] >= sys_range[0],\n",
    "                        ds.sig[param] < sys_range[1],\n",
    "                    )\n",
    "                    mask = np.logical_and(mask, mask_i)\n",
    "            \n",
    "            print('Reduction factor: {:3.3f}'.format(factor))\n",
    "            ds.sig = ds.sig._subsample(mask)\n",
    "            ds.sig.oneweight[:] = ds.sig.oneweight/factor\n",
    "            \n",
    "    ana_sys = cy.get_analysis(\n",
    "        cy.selections.Repository(), selection_version, [DNNCascade_10yr_sys_reduced], \n",
    "        #_quiet=True,\n",
    "    )\n",
    "    return ana_sys\n",
    "\n",
    "def sample_snowstorm_ranges(\n",
    "            seed=None, \n",
    "            sim_ranges=sim_ranges, \n",
    "            allowed_ranges=allowed_ranges, \n",
    "            min_red_factor=0.05,\n",
    "            max_k=3,\n",
    "        ):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # sample number of parameters to perturb\n",
    "    k = rng.randint(1, 1 + max_k)\n",
    "    \n",
    "    # sample which parameters to perturb\n",
    "    parameter_names=[\n",
    "        'Scattering', 'Absorption', 'AnisotropyScale', \n",
    "        'DOMEfficiency', 'HoleIceForward_Unified_00', \n",
    "        'HoleIceForward_Unified_01',\n",
    "    ]\n",
    "    params = rng.choice(parameter_names, size=k, replace=False)\n",
    "    \n",
    "    # compute reduction fraction from allowed range\n",
    "    fractions = []\n",
    "    allowed_fraction = 1.\n",
    "    for param, allowed_range in allowed_ranges.items():\n",
    "        if allowed_range != sim_ranges[param]:\n",
    "            fraction_i = (allowed_range[1] - allowed_range[0]) / (sim_ranges[param][1] - sim_ranges[param][0])\n",
    "        else:\n",
    "            fraction_i = 1.\n",
    "        allowed_fraction *= fraction_i\n",
    "        if param in params:\n",
    "            fractions.append(fraction_i)\n",
    "            \n",
    "    # define relative reduction fraction of allowed range\n",
    "    rel_fr = np.power(min_red_factor / allowed_fraction , 1./k)\n",
    "    \n",
    "    # sample intervals\n",
    "    sys_range = deepcopy(allowed_ranges)\n",
    "\n",
    "    current_factor = 1.\n",
    "    for param, fraction_i in zip(params, fractions):\n",
    "        allowed_range = allowed_ranges[param]\n",
    "        \n",
    "        interval_width = (allowed_range[1] - allowed_range[0]) * rel_fr / 2.\n",
    "        sample_range = [allowed_range[0] + interval_width, allowed_range[1] - interval_width]\n",
    "        \n",
    "        assert sample_range[1] > sample_range[0], sample_range\n",
    "        \n",
    "        mid_point = rng.uniform(*sample_range)\n",
    "        sys_range[str(param)] = [mid_point - interval_width, mid_point + interval_width]\n",
    "    \n",
    "    return sys_range, params\n",
    "\n",
    "def get_snowstorm_tr(\n",
    "            template_str,\n",
    "            seed=None, \n",
    "            sim_ranges=sim_ranges, \n",
    "            allowed_ranges=allowed_ranges, \n",
    "            min_red_factor=0.05,\n",
    "            max_k=3,\n",
    "            sigmas=[0],\n",
    "        ):\n",
    "    \n",
    "    # sample SnowStorm parameters\n",
    "    sys_ranges, params = sample_snowstorm_ranges(seed=seed, min_red_factor=min_red_factor, max_k=max_k)\n",
    "    \n",
    "    # get snowstorm ana object\n",
    "    with io.capture_output() as captured:\n",
    "        ana_sys = get_snowstorm_ana(sys_ranges=sys_ranges)\n",
    "\n",
    "        # get trial runner\n",
    "        tr_sys = get_gp_tr_sys(ana=ana_sys, template_str=template_str, sigmas=sigmas)\n",
    "    \n",
    "    return tr_sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-secret",
   "metadata": {},
   "source": [
    "##### Test Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "min_red_factor = 0.02\n",
    "max_k = 3\n",
    "\n",
    "mids = {k: [] for k in sim_ranges.keys()}\n",
    "mids_all = {k: [] for k in sim_ranges.keys()}\n",
    "for i in tqdm(range(n_samples), total=n_samples):\n",
    "    sys_ranges, params = sample_snowstorm_ranges(min_red_factor=min_red_factor, max_k=max_k)\n",
    "    for k, sys_range in sys_ranges.items():\n",
    "        if k in params:\n",
    "            mids[k].append(np.mean(sys_range))\n",
    "        mids_all[k].append(np.mean(sys_range))\n",
    "    \n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    key = sorted(sim_ranges.keys())[i]\n",
    "    ax.set_xlabel(key)\n",
    "    ax.set_ylabel('Number of samples')\n",
    "    ax.hist(mids[key], bins=30)\n",
    "    ax.axvline(sim_ranges[key][0], color='0.3', ls='--', label='Simulation Range')\n",
    "    ax.axvline(sim_ranges[key][1], color='0.3', ls='--')\n",
    "    ax.axvline(allowed_ranges[key][0], color='0.7', ls='-', label='Allowed Range')\n",
    "    ax.axvline(allowed_ranges[key][1], color='0.7', ls='-')\n",
    "axes[0, 0].legend()\n",
    "fig.suptitle('Min Reduction: {:1.3f} | Max k: {}'.format(min_red_factor, max_k))\n",
    "fig.tight_layout()\n",
    "fig.savefig('{}/snowstorm_sampling_check.png'.format(plot_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_ranges, params = sample_snowstorm_ranges(min_red_factor=0.02)\n",
    "sys_ranges, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ana_sys = get_snowstorm_ana(\n",
    "    #sys_ranges={\n",
    "    #    'Scattering': [1.0, 1.1],\n",
    "    #    'Absorption': [0.9, 1.0],\n",
    "    #    'AnisotropyScale': [0., 1.],\n",
    "    #},\n",
    "    sys_ranges=sys_ranges,\n",
    "    #sys_ranges=sample_snowstorm_ranges(),\n",
    ")\n",
    "tr_sys = get_gp_tr_sys(\n",
    "    ana=ana_sys, template_str='pi0', \n",
    "    #sigmas=np.radians(np.r_[3:20, 20:40:2, 40:60:4, 60:91:5]),\n",
    ")\n",
    "print(len(ana_sys.anas[0].sig)/len(df), len(ana_sys.anas[0].sig))\n",
    "print('ana', np.sum(a.sig.oneweight * a.sig.true_energy**-2.5))\n",
    "print('sys', np.sum(ana_sys.anas[0].sig.oneweight * ana_sys.anas[0].sig.true_energy**-2.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ana_sys.anas[0].sig) / len(ana.anas[0].sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-tutorial",
   "metadata": {},
   "source": [
    "#### Get TrialRunners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dict = {\n",
    "    'pi0': get_gp_tr('pi0'),\n",
    "    'kra5': get_gp_tr('kra5'),\n",
    "    'kra50': get_gp_tr('kra50'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-matter",
   "metadata": {},
   "source": [
    "#### Get Results for each template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for key in tr_dict.keys():\n",
    "    f_path = os.path.join(\n",
    "        cg.base_dir, \n",
    "        'gp/results/{}/{}_unblinded.npy'.format(key, key), \n",
    "    )\n",
    "    res_dict[key] = np.load(f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-clothing",
   "metadata": {},
   "source": [
    "#### Get ns-bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_poisson = False\n",
    "add_sys = False\n",
    "\n",
    "if add_sys:\n",
    "    sys_suffix = '_sys'\n",
    "else:\n",
    "    sys_suffix = ''\n",
    "    \n",
    "if use_poisson:\n",
    "    ns_bias_file = os.path.join(plot_dir, 'ns_bias_poisson{}.pkl'.format(sys_suffix))\n",
    "else:\n",
    "    ns_bias_file = os.path.join(plot_dir, 'ns_bias{}.pkl'.format(sys_suffix))\n",
    "\n",
    "\n",
    "def get_ns_bias_dict_from_sys(ns_bias_dict_sys):\n",
    "    ns_bias_dict = {}\n",
    "    for key, ns_trials_dict in ns_bias_dict_sys.items():\n",
    "\n",
    "        # get ns_values\n",
    "        ns_values = [sorted(ns_trials_dict[trial_i].keys()) for trial_i in ns_trials_dict.keys()]\n",
    "        for ns_values_i in ns_values:\n",
    "            assert np.allclose(ns_values_i, ns_values[0])\n",
    "        ns_values = ns_values[0]\n",
    "\n",
    "        trials_dict = {ns: [] for ns in ns_values}\n",
    "        for trial_i in sorted(ns_trials_dict.keys()):\n",
    "            for ns in ns_values:\n",
    "                trials_dict[ns].append(ns_trials_dict[trial_i][ns])\n",
    "\n",
    "        ns_bias_dict[key] = {\n",
    "            ns: cy.utils.Arrays.concatenate(trials_dict[ns]) for ns in ns_values\n",
    "        }\n",
    "    return ns_bias_dict\n",
    "\n",
    "if os.path.exists(ns_bias_file):\n",
    "    print('Loading from file')\n",
    "    if add_sys:\n",
    "        with open(ns_bias_file, 'rb') as handle:\n",
    "            ns_bias_dict_sys = pickle.load(handle)\n",
    "        \n",
    "        # restructure dict\n",
    "        ns_bias_dict = get_ns_bias_dict_from_sys(ns_bias_dict_sys)\n",
    "    else:\n",
    "        with open(ns_bias_file, 'rb') as handle:\n",
    "            ns_bias_dict = pickle.load(handle)\n",
    "        \n",
    "        ns_bias_dict_sys = {}\n",
    "else:\n",
    "    print('Creating new dict')\n",
    "    ns_bias_dict = {}\n",
    "    ns_bias_dict_sys = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import pool\n",
    "\n",
    "ns_bias_range = {\n",
    "    'kra5': [0, 600],\n",
    "    'kra50': [0, 600],\n",
    "    'pi0': [0, 1500],\n",
    "}\n",
    "n_trials = 100\n",
    "recalculate = False\n",
    "cpus = 20\n",
    "\n",
    "for key, ns_range in ns_bias_range.items():\n",
    "    \n",
    "    for dictionary in [ns_bias_dict, ns_bias_dict_sys]:\n",
    "        if key not in dictionary:\n",
    "            dictionary[key] = {}\n",
    "        \n",
    "    print('Submitting values for {} from {} to {}'.format(key, *ns_range))\n",
    "    \n",
    "    if add_sys:\n",
    "        for trial_i in tqdm(range(n_trials), total=n_trials):\n",
    "            \n",
    "            if trial_i not in ns_bias_dict_sys[key] or recalculate:\n",
    "                ns_bias_dict_sys[key][trial_i] = {}\n",
    "                \n",
    "                print('Getting trial runner for {}'.format(key))\n",
    "                tr = get_snowstorm_tr(\n",
    "                    key, seed=trial_i, \n",
    "                    sigmas=np.radians(np.r_[3:20, 20:40:2, 40:60:4, 60:91:5]),\n",
    "                )\n",
    "                \n",
    "                print('Starting pool with {} cpus'.format(cpus))\n",
    "                def compute_trial_i(ns):\n",
    "                    trials = tr.get_many_fits(1, n_sig=ns, logging=False, seed=ns, poisson=use_poisson, cpus=1)\n",
    "                    trials['ntrue'] = np.repeat(ns, len(trials))\n",
    "                    return trials\n",
    "                    \n",
    "                arg_list = list(range(*ns_range))\n",
    "                with Pool(cpus) as p:\n",
    "                    trials = list(tqdm(p.imap(compute_trial_i, arg_list), total=len(arg_list)))\n",
    "                \n",
    "                for j, ns in enumerate(range(*ns_range)):\n",
    "                    ns_bias_dict_sys[key][trial_i][ns] = trials[j]\n",
    "\n",
    "                with open(ns_bias_file, 'wb') as f:\n",
    "                    pickle.dump(ns_bias_dict_sys, f, protocol=-1)\n",
    "    \n",
    "    else:\n",
    "        tr = tr_dict[key]\n",
    "        \n",
    "        for ns in tqdm(range(*ns_range), total=len(range(*ns_range))):\n",
    "\n",
    "            if ns not in ns_bias_dict[key] or recalculate:\n",
    "                trials = tr.get_many_fits(n_trials, n_sig=ns, logging=False, seed=ns, cpus=cpus, poisson=use_poisson)\n",
    "                trials['ntrue'] = np.repeat(ns, len(trials))\n",
    "\n",
    "                ns_bias_dict[key][ns] = trials\n",
    "\n",
    "                with open(ns_bias_file, 'wb') as f:\n",
    "                    pickle.dump(ns_bias_dict, f, protocol=-1)\n",
    "\n",
    "if add_sys:\n",
    "    ns_bias_dict = get_ns_bias_dict_from_sys(ns_bias_dict_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "bias_corr_funcs = {}\n",
    "\n",
    "for key, ns_bias_dict_i in ns_bias_dict.items():\n",
    "    \n",
    "    # create bias correction function\n",
    "    x = []\n",
    "    y = []\n",
    "    for n_inj in sorted(ns_bias_dict_i.keys()):\n",
    "        x.append(n_inj)\n",
    "        y.append(np.median(ns_bias_dict_i[n_inj].ns))\n",
    "    bias_corr_funcs[key] = UnivariateSpline(x=x, y=y, s=len(ns_bias_dict_i)*500)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    tr = tr_dict[key]\n",
    "    \n",
    "    allt = cy.utils.Arrays.concatenate([t for t in ns_bias_dict_i.values()])\n",
    "    plot_ns_bias(ax=ax, tr=tr, allt=allt)\n",
    "    ax.plot(x, bias_corr_funcs[key](x), label='Spline Fit')\n",
    "    ax.set_title('Model: {}'.format(key))\n",
    "    ax.set_xlabel('$n_\\mathrm{inj}$')\n",
    "    ax.set_ylabel('$n_s$')\n",
    "    ax.legend()\n",
    "    fig.savefig('{}/ns_bias_{}{}.png'.format(plot_dir, key, sys_suffix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0, 1500, 1000)\n",
    "for key, func in bias_corr_funcs.items():\n",
    "    ax.plot(x, func(x), label=key)\n",
    "ax.plot(x, x, color='0.7', ls='--')\n",
    "ax.set_xlabel('$n_\\mathrm{inj}$')\n",
    "ax.set_ylabel('$n_s$')\n",
    "ax.legend()\n",
    "fig.savefig('{}/ns_bias_comparison{}.png'.format(plot_dir, sys_suffix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-diana",
   "metadata": {},
   "source": [
    "#### Get Critical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "def model_norm_to_ns(tr, model_norm, correction_factor=1.5):\n",
    "    return model_norm * tr.sig_inj_acc_total / correction_factor\n",
    "    \n",
    "def get_critical_value_trial(\n",
    "            E2dNdE_or_modelnorm, tr, tr_inj, bias_corr_func=None,\n",
    "            E0=100, unit=1e3, seed=None, TRUTH=False, is_model_norm=False,\n",
    "        ):\n",
    "    \n",
    "    # get number of ns corresponding to flux\n",
    "    if is_model_norm:\n",
    "        n_sig = model_norm_to_ns(tr=tr_inj, model_norm=E2dNdE_or_modelnorm)\n",
    "    else:\n",
    "        n_sig = tr_inj.to_ns(E2dNdE_or_modelnorm, E0=E0, unit=unit)\n",
    "    \n",
    "    if TRUTH:\n",
    "        n_inj = 0\n",
    "    else:\n",
    "        n_inj = n_sig\n",
    "    \n",
    "    # get trial\n",
    "    trial = tr_inj.get_one_trial(n_sig=n_inj, poisson=True, seed=seed, TRUTH=TRUTH)\n",
    "    \n",
    "    # get best fit ts and ns for this trial\n",
    "    fit = tr.get_one_fit_from_trial(trial)\n",
    "    ts_fit, ns_fit = fit\n",
    "    \n",
    "    # apply bias correction for tested nsig?\n",
    "    if bias_corr_func is not None:\n",
    "        n_sig = bias_corr_func(n_sig)\n",
    "        \n",
    "    # get Likelihood object\n",
    "    L = tr.get_one_llh_from_trial(trial)\n",
    "    ts_test = L.get_ts(ns=n_sig, **tr.fitter_args)\n",
    "    \n",
    "    # compute test-statistic tau for critical value definition\n",
    "    # tau = -2 log llh-ratio = - 2 log {L_0(ns_test) / L_1(ns=n_fit)}\n",
    "    # In this case, we want to test against ns_test = ns(E2dNdE)\n",
    "    # tau = (-2 log LR(ns=0) - (-2 log LR(ns=ns_test))\n",
    "    #     = -2 log L(ns=0) + 2 log L(ns=n_fit) + 2 log L(ns=0) - 2 log L(ns=ns_test)\n",
    "    #     = -2 log L(ns=ns_test) + 2 log L(ns=ns_fit)\n",
    "    #     = -2 log {L(ns_ns_test) / L(ns=ns_fit)}\n",
    "    tau = ts_fit - ts_test\n",
    "    \n",
    "    return tau\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# define global functions for multiprocessing (pickle has issues with local functions)\n",
    "# ------------------------------------------------------------------------------------\n",
    "def run_trial_pi0(args):\n",
    "    i, E2dNdEs, E0, unit, bias_corr_func, sys_seed, min_red_factor, max_k = args\n",
    "    if sys_seed is None:\n",
    "        tr_inj = tr_dict['pi0']\n",
    "    else:\n",
    "        tr_inj = get_snowstorm_tr('pi0', seed=sys_seed, min_red_factor=min_red_factor, max_k=max_k)\n",
    "    \n",
    "    tau_values = [] \n",
    "    for E2dNdE in E2dNdEs:\n",
    "        tau_values_i = get_critical_value_trial(E2dNdE_or_modelnorm=E2dNdE, tr=tr_dict['pi0'], tr_inj=tr_inj, bias_corr_func=bias_corr_func, E0=E0, unit=unit, seed=i)\n",
    "        tau_values.append(tau_values_i)\n",
    "    return tau_values\n",
    "\n",
    "def run_trial_kra5(args):\n",
    "    i, modelnorms, E0, unit, bias_corr_func, sys_seed, min_red_factor, max_k = args\n",
    "    if sys_seed is None:\n",
    "        tr_inj = tr_dict['kra5']\n",
    "    else:\n",
    "        tr_inj = get_snowstorm_tr('kra5', seed=sys_seed, min_red_factor=min_red_factor, max_k=max_k)\n",
    "    \n",
    "    tau_values = [] \n",
    "    for modelnorm in modelnorms:\n",
    "        tau_values_i = get_critical_value_trial(E2dNdE_or_modelnorm=modelnorm, tr=tr_dict['kra5'], tr_inj=tr_inj, bias_corr_func=bias_corr_func, E0=E0, unit=unit, seed=i, is_model_norm=True)\n",
    "        tau_values.append(tau_values_i)\n",
    "    return tau_values\n",
    "\n",
    "def run_trial_kra50(args):\n",
    "    i, modelnorms, E0, unit, bias_corr_func, sys_seed, min_red_factor, max_k = args\n",
    "    if sys_seed is None:\n",
    "        tr_inj = tr_dict['kra50']\n",
    "    else:\n",
    "        tr_inj = get_snowstorm_tr('kra50', seed=sys_seed, min_red_factor=min_red_factor, max_k=max_k)\n",
    "    \n",
    "    tau_values = [] \n",
    "    for modelnorm in modelnorms:\n",
    "        tau_values_i = get_critical_value_trial(E2dNdE_or_modelnorm=modelnorm, tr=tr_dict['kra50'], tr_inj=tr_inj, bias_corr_func=bias_corr_func, E0=E0, unit=unit, seed=i, is_model_norm=True)\n",
    "        tau_values.append(tau_values_i)\n",
    "    return tau_values\n",
    "\n",
    "function_dict = {\n",
    "    'pi0': run_trial_pi0,\n",
    "    'kra5': run_trial_kra5,\n",
    "    'kra50': run_trial_kra50,\n",
    "}\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "def run_critical_value_trials(\n",
    "            n_trials, E2dNdE_or_modelnorm_list, key, add_systematics=False, \n",
    "            bias_corr_funcs=None, min_red_factor=0.02, max_k=3,\n",
    "            E0=100, unit=1e3, seed=0, cpus=20,\n",
    "        ):\n",
    "    \n",
    "    if bias_corr_funcs is not None:\n",
    "        bias_corr_func = bias_corr_funcs[key]\n",
    "    else:\n",
    "        bias_corr_func = None\n",
    "        \n",
    "    tau_values = [[] for norm in E2dNdE_or_modelnorm_list]\n",
    "    seed_values = list(range(seed, seed + n_trials))\n",
    "    \n",
    "    if add_systematics:\n",
    "        arg_list = [(i, E2dNdE_or_modelnorm_list, E0, unit, bias_corr_func, i, min_red_factor, max_k) \n",
    "                    for i in seed_values]\n",
    "    else:\n",
    "        arg_list = [(i, E2dNdE_or_modelnorm_list, E0, unit, bias_corr_func, None, min_red_factor, max_k) \n",
    "                    for i in seed_values]\n",
    "\n",
    "    compute_trial_i = function_dict[key]\n",
    "    \n",
    "    if cpus > 1:\n",
    "        print('Running pool with {} cpus'.format(cpus))\n",
    "        \n",
    "        with Pool(cpus) as p:\n",
    "            tau_values_map = list(tqdm(p.imap(compute_trial_i, arg_list), total=n_trials))\n",
    "        print('tau_values_map.shape', np.array(tau_values_map).shape)\n",
    "        for j, tau_values_i in enumerate(tau_values_map):\n",
    "            for i, values in enumerate(tau_values_i):\n",
    "                tau_values[i].append(values)\n",
    "        p.close()\n",
    "    else:\n",
    "        for args in tqdm(arg_list, total=n_trials):\n",
    "            tau_values_i = compute_trial_i(args)\n",
    "            for i, values in enumerate(tau_values_i):\n",
    "                tau_values[i].append(values)\n",
    "            \n",
    "    return np.array(tau_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-lodge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed = 4000\n",
    "n_trials = 1000\n",
    "apply_correction = True\n",
    "add_systematics = False\n",
    "min_red_factor = 0.02\n",
    "max_k = 3\n",
    "\n",
    "cpus = 20\n",
    "recalculate = False\n",
    "\n",
    "E2dNdE_or_modelnorm_dict = {\n",
    "    'pi0': np.linspace(1e-11, 4e-11, 25),\n",
    "    'kra5': np.linspace(0.3, 1.7, 25),\n",
    "    'kra50': np.linspace(0.2, 1.2, 25),\n",
    "}\n",
    "\n",
    "    \n",
    "tau_dict = {}\n",
    "for key in ['pi0', 'kra5', 'kra50']:\n",
    "#for key in ['pi0']:\n",
    "        \n",
    "    print('Running {} trials for {} with {} different normalizations'.format(n_trials, key, len(E2dNdE_or_modelnorm_dict[key])))\n",
    "    if apply_correction:\n",
    "        print('Applying correction')\n",
    "        bias_corr_funcs_kw = bias_corr_funcs\n",
    "    else:\n",
    "        bias_corr_funcs_kw = None\n",
    "    print('Adding Systematic:', add_systematics)\n",
    "    \n",
    "    if add_systematics:\n",
    "        sys_str = '{}_red_{:0.3f}_k_{}'.format(add_systematics, min_red_factor, max_k)\n",
    "    else:\n",
    "        sys_str = '{}'.format(add_systematics)\n",
    "        \n",
    "    file_path = os.path.join(plot_dir, 'trials_{}_corr_{}_sys_{}_seeds_{}_{}.pkl'.format(\n",
    "        key, apply_correction, sys_str, seed, seed+n_trials))\n",
    "    \n",
    "    if not os.path.exists(file_path) or recalculate:\n",
    "        tau_values = run_critical_value_trials(\n",
    "            n_trials, seed=seed, E2dNdE_or_modelnorm_list=E2dNdE_or_modelnorm_dict[key], key=key, add_systematics=add_systematics, cpus=cpus, bias_corr_funcs=bias_corr_funcs_kw)\n",
    "        \n",
    "        # save trials\n",
    "        with open(file_path, 'wb') as f:\n",
    "            seeds = list(range(seed, seed+n_trials))\n",
    "            pickle.dump((E2dNdE_or_modelnorm_dict, tau_values, seeds), f, protocol=-1)\n",
    "    else:\n",
    "        print('Skipping because file already exists...')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-august",
   "metadata": {},
   "source": [
    "#### Load trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "tau_dict = {}\n",
    "for key in ['pi0', 'kra5', 'kra50']:\n",
    "#for key in ['pi0']:\n",
    "    \n",
    "    print('Loading trials for {} with {} different normalizations'.format(key, len(E2dNdE_or_modelnorm_dict[key])))\n",
    "    key_s = (key, apply_correction)\n",
    "    \n",
    "    if key_s not in tau_dict:\n",
    "        tau_dict[key_s] = {norm: [] for norm in E2dNdE_or_modelnorm_dict[key]}\n",
    "    \n",
    "    # find a list of files\n",
    "    if add_systematics:\n",
    "        sys_str = '{}_red_{:0.3f}_k_{}'.format(add_systematics, min_red_factor, max_k)\n",
    "    else:\n",
    "        sys_str = '{}'.format(add_systematics)\n",
    "        \n",
    "    file_pattern = os.path.join(plot_dir, 'trials_{}_corr_{}_sys_{}_seeds_*_*.pkl'.format(\n",
    "        key, apply_correction, sys_str))\n",
    "    file_list = sorted(glob.glob(file_pattern))\n",
    "    print('Found {} files...'.format(len(file_list)))\n",
    "    \n",
    "    # load files and check for overlapping seeds\n",
    "    seed_values = set([])\n",
    "    for file_i in file_list:\n",
    "        with open(file_i, 'rb') as handle:\n",
    "            E2dNdE_or_modelnorm_dict_loaded, tau_values, seeds = pickle.load(handle)\n",
    "        \n",
    "        # make sure model norms match\n",
    "        assert sorted(E2dNdE_or_modelnorm_dict_loaded.keys()) == sorted(E2dNdE_or_modelnorm_dict.keys())\n",
    "        for k, norms in E2dNdE_or_modelnorm_dict_loaded.items():\n",
    "            assert np.allclose(norms, E2dNdE_or_modelnorm_dict[k]), (norms, E2dNdE_or_modelnorm_dict[k])\n",
    "\n",
    "        # make sure seeds do not overlap\n",
    "        overlapping_seeds = seed_values.intersection(set(seeds))\n",
    "        if overlapping_seeds:\n",
    "            raise ValueError('Found overlapping seeds: {}!'.format(overlapping_seeds))\n",
    "        seed_values = seed_values.union(set(seeds))\n",
    "        \n",
    "        # append tau values from this file\n",
    "        for i, E2dNdE_or_modelnorm in enumerate(E2dNdE_or_modelnorm_dict[key]):\n",
    "            tau_dict[key_s][E2dNdE_or_modelnorm].append(tau_values[i])\n",
    "    \n",
    "    # concatenate into single array\n",
    "    for i, E2dNdE_or_modelnorm in enumerate(E2dNdE_or_modelnorm_dict[key]):\n",
    "        tau_dict[key_s][E2dNdE_or_modelnorm] = np.concatenate(tau_dict[key_s][E2dNdE_or_modelnorm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-correspondence",
   "metadata": {},
   "source": [
    "#### Make critical value plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from itertools import cycle\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy import optimize\n",
    "\n",
    "    \n",
    "def make_critical_value_plot(\n",
    "            key, tau_dict, confidence_levels=[0.68, 0.9, 0.95], \n",
    "            norm_bins=np.linspace(1e-11, 4e-11, 25), \n",
    "            tau_bins=np.linspace(0., 7, 30),\n",
    "            bias_corr_funcs=None,\n",
    "            E0=100, unit=1e3,\n",
    "            ls_list=['-', '--', '-.'],\n",
    "            plot_splines=False,\n",
    "            n_eval_points=100,  \n",
    "        ):\n",
    "    if bias_corr_funcs is None:\n",
    "        print('Not applying correction')\n",
    "        apply_correction = False\n",
    "        bias_corr_func_kw = None\n",
    "    else:\n",
    "        print('Applying correction')\n",
    "        apply_correction = True\n",
    "        bias_corr_func_kw = bias_corr_funcs[key]\n",
    "        \n",
    "    key_s = (key, apply_correction)\n",
    "            \n",
    "    tau_values = []\n",
    "    norm_values = []\n",
    "    for norm in sorted(tau_dict[key_s].keys()):\n",
    "        tau_values.append(tau_dict[key_s][norm])\n",
    "        norm_values.append(norm)\n",
    "    \n",
    "    tau_values = np.array(tau_values)\n",
    "    norm_values = np.array(norm_values)\n",
    "    \n",
    "    if norm_bins is None:\n",
    "        assert np.allclose(np.diff(norm_values), np.diff(norm_values)[0]) \n",
    "        width = np.diff(norm_values)[0] / 2.\n",
    "        norm_bins = np.r_[norm_values - width, np.max(norm_values) + width]\n",
    "    else:\n",
    "        norm_bins = np.array(norm_bins)\n",
    "    \n",
    "    eps = 0.01 * np.diff(norm_bins)[0]\n",
    "    norm_bins[0] -= eps\n",
    "    norm_bins[-1] += eps\n",
    "    \n",
    "    n_bins = len(norm_bins) - 1\n",
    "    n_taus = len(tau_bins) - 1\n",
    "    \n",
    "    if key in ['kra5', 'kra50']:\n",
    "        is_model_norm = True\n",
    "    else:\n",
    "        is_model_norm = False\n",
    "            \n",
    "    # get tau values for observed data\n",
    "    tau_observed = []\n",
    "    norm_values_obs = np.linspace(norm_bins[0], norm_bins[-1], n_eval_points)\n",
    "    for norm in tqdm(norm_values_obs, total=len(norm_values_obs)):\n",
    "        tau_observed.append(get_critical_value_trial(\n",
    "            E2dNdE_or_modelnorm=norm, tr=tr_dict[key], tr_inj=tr_dict[key], E0=E0, unit=unit, seed=42, TRUTH=True, \n",
    "            is_model_norm=is_model_norm, bias_corr_func=bias_corr_func_kw,\n",
    "        ))\n",
    "    \n",
    "    # create histogram\n",
    "    hist = np.zeros((n_bins, n_taus)) * np.nan\n",
    "    critical_value_dict = {c: [] for c in confidence_levels}\n",
    "    for norm, taus in zip(norm_values, tau_values):\n",
    "        idx = np.searchsorted(norm_bins, norm) - 1\n",
    "        assert idx >= 0 and idx <= n_bins - 1, idx\n",
    "        \n",
    "        hist_col, _ = np.histogram(taus, bins=tau_bins, density=True)\n",
    "        hist[idx] = hist_col\n",
    "        \n",
    "        for c in confidence_levels:\n",
    "            critical_value_dict[c].append(np.quantile(taus, q=c))\n",
    "            \n",
    "    # ---------------------\n",
    "    # Compute best fit flux\n",
    "    # ---------------------\n",
    "    spl_observed = UnivariateSpline(norm_values_obs, tau_observed)\n",
    "    norm_obs_best = optimize.minimize(\n",
    "        spl_observed, x0=norm_values_obs[np.argmin(tau_observed)],\n",
    "        bounds=[(np.min(norm_values_obs), np.max(norm_values_obs))],\n",
    "    ).x[0]\n",
    "    \n",
    "    # ---------------------------\n",
    "    # compute intersection points\n",
    "    # ---------------------------\n",
    "    \n",
    "    tau_observed_tested = []\n",
    "    for norm in tqdm(norm_values, total=len(norm_values)):\n",
    "        tau_observed_tested.append(get_critical_value_trial(\n",
    "            E2dNdE_or_modelnorm=norm, tr=tr_dict[key], tr_inj=tr_dict[key], E0=E0, unit=unit, seed=42, TRUTH=True,\n",
    "            is_model_norm=is_model_norm, bias_corr_func=bias_corr_func_kw,\n",
    "        ))\n",
    "    tau_observed_tested = np.array(tau_observed_tested)\n",
    "    \n",
    "    cf_mask_dict = {}\n",
    "    intervalls_binned = {}\n",
    "    for c, critical_values in critical_value_dict.items():\n",
    "        mask = np.array(critical_values) > tau_observed_tested\n",
    "        intervalls_binned[c] = [np.min(norm_values[mask]), np.max(norm_values[mask])]\n",
    "    \n",
    "    # compute intersection points based on fitted splines      \n",
    "    c_splines_dict = {}\n",
    "    for c, c_values in critical_value_dict.items():\n",
    "        # fit spline to critical values\n",
    "        spl = UnivariateSpline(norm_values, c_values)\n",
    "        c_splines_dict[c] = spl\n",
    "    \n",
    "    def find_intersection(c, x0):\n",
    "        \"\"\"Find intersection points based on splines\"\"\"\n",
    "        def fun(x):\n",
    "            return spl_observed(x) - c_splines_dict[c](x)\n",
    "        sol = optimize.root(fun, x0=x0)\n",
    "        return np.sort(sol.x)\n",
    "    \n",
    "    intervalls_fitted = {}\n",
    "    for c, x0 in intervalls_binned.items():\n",
    "        interval = find_intersection(c, x0=x0)\n",
    "        assert len(interval) == 2, interval\n",
    "        intervalls_fitted[c] = interval\n",
    "    # ---------------------------\n",
    "        \n",
    "    # make plot\n",
    "    if is_model_norm:\n",
    "        units = 1\n",
    "    else:\n",
    "        units = 1e-11\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    X, Y = np.meshgrid(norm_bins / units, tau_bins)\n",
    "    im = ax.pcolor(X, Y, hist.T, norm=matplotlib.colors.LogNorm())\n",
    "    \n",
    "    ax.plot(\n",
    "        norm_values_obs / units, tau_observed, color='1.0', \n",
    "        label=r'Observed $\\tau(\\mathrm{flux})$' + '[Minimum at: {:3.2e}]'.format(norm_obs_best),\n",
    "    )\n",
    "    \n",
    "    if plot_splines:\n",
    "        ax.plot(\n",
    "            norm_values_obs / units, spl_observed(norm_values_obs), ls='--', lw=2., color='0.3',\n",
    "            label=r'Observed $\\tau(\\mathrm{flux})$ [Spline-Fit]',\n",
    "        )\n",
    "        ls_cycler = cycle(ls_list)\n",
    "        for c, critical_values in critical_value_dict.items():\n",
    "            ax.plot(\n",
    "                norm_values_obs / units, c_splines_dict[c](norm_values_obs), \n",
    "                color='1.0', lw=2., ls=next(ls_cycler),\n",
    "                label='Critical values ({:3.1f}% [Spline-Fit]'.format(c * 100),\n",
    "            )\n",
    "            \n",
    "    ls_cycler = cycle(ls_list)\n",
    "    for c, critical_values in critical_value_dict.items():\n",
    "        ax.plot(\n",
    "            norm_values / units, critical_values, color='r', ls=next(ls_cycler),\n",
    "            label='Critical values ({:3.1f}% CL [{:3.2e}, {:3.2e}])'.format(c * 100, *intervalls_fitted[c]),\n",
    "        )\n",
    "    \n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(r'$\\log_{10} \\, P(\\tau | \\mathrm{flux})$')\n",
    "    if units == 1e-11:\n",
    "        units_label = ' $\\cdot 10^{-11}$'\n",
    "    elif units == 1.:\n",
    "        units_label = ''\n",
    "    else:\n",
    "        units_label = ' $\\cdot {:.0e}$'.format(units)\n",
    "    \n",
    "    if is_model_norm:\n",
    "        ax.set_xlabel('Model Normalization')\n",
    "    else:\n",
    "        ax.set_xlabel(\n",
    "            '$\\mathrm{E}^2 \\cdot \\mathrm{dN/dE}$'+ units_label + ' at {:.0f} TeV'.format(E0)  + \n",
    "            ' [$\\mathrm{TeV} \\, \\mathrm{s}^{-1} \\, \\mathrm{cm}^{-2}$]')\n",
    "    ax.set_ylabel(r'$P(\\tau | \\mathrm{flux})$ (test-statistic $\\tau$)')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(np.min(tau_bins), np.max(tau_bins))\n",
    "    \n",
    "    return fig, ax, norm_obs_best, intervalls_binned, intervalls_fitted\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_correction:\n",
    "    bias_corr_funcs_kw = bias_corr_funcs\n",
    "    file_suffix = '_corrected'\n",
    "else:\n",
    "    bias_corr_funcs_kw = None\n",
    "    file_suffix = ''\n",
    "\n",
    "if add_systematics:\n",
    "    file_suffix += '_sys_red_{:1.3f}_k_{}'.format(min_red_factor, max_k)\n",
    "\n",
    "for key, _ in tau_dict.keys():\n",
    "    fig, ax, norm_obs_best, intervalls_binned, intervalls_fitted = make_critical_value_plot(\n",
    "        key, tau_dict=tau_dict, plot_splines=False, norm_bins=E2dNdE_or_modelnorm_dict[key], \n",
    "        bias_corr_funcs=bias_corr_funcs_kw)\n",
    "    ax.set_title('Model: {}'.format(key))\n",
    "    fig.savefig('{}/confidence_intervals_{}{}.png'.format(plot_dir, key, file_suffix))\n",
    "\n",
    "    fig, ax, norm_obs_best, intervalls_binned, intervalls_fitted = make_critical_value_plot(\n",
    "        key, tau_dict=tau_dict, plot_splines=True, norm_bins=E2dNdE_or_modelnorm_dict[key], \n",
    "        bias_corr_funcs=bias_corr_funcs_kw)\n",
    "    ax.set_title('Model: {}'.format(key))\n",
    "    fig.savefig('{}/confidence_intervals_{}{}_splines.png'.format(plot_dir, key, file_suffix))\n",
    "\n",
    "    print(key, norm_obs_best) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-insulation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-plaza",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "E2dNdE_obs_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.to_E2dNdE(748.113, E0=100, unit=1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.to_E2dNdE(678, E0=100, unit=1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-distinction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-walker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-cambodia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.3_py3-v4.1.0_csky",
   "language": "python",
   "name": "tensorflow2.3_py3-v4.1.0_csky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
