{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# set env flags to catch BLAS used for scipy/numpy \n",
    "# to only use 1 cpu, n_cpus will be totally controlled by csky\n",
    "if False:\n",
    "    os.environ['MKL_NUM_THREADS'] = \"1\"\n",
    "    os.environ['NUMEXPR_NUM_THREADS'] = \"1\"\n",
    "    os.environ['OMP_NUM_THREADS'] = \"1\"\n",
    "    os.environ['OPENBLAS_NUM_THREADS'] = \"1\"\n",
    "    os.environ['VECLIB_MAXIMUM_THREADS'] = \"1\"\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'w'\n",
    "mpl.rcParams['savefig.facecolor'] = 'w'\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, cm\n",
    "import csky as cy\n",
    "from csky import cext\n",
    "import numpy as np\n",
    "import astropy\n",
    "#from icecube import astro\n",
    "import histlite as hl\n",
    "import healpy\n",
    "import healpy as hp\n",
    "import socket\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import copy\n",
    "healpy.disable_warnings()\n",
    "plt.rc('figure', facecolor = 'w')\n",
    "plt.rc('figure', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-nevada",
   "metadata": {},
   "source": [
    "## Define Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_version = 'version-001-p01'\n",
    "\n",
    "host_name = socket.gethostname()\n",
    "\n",
    "if 'cobalt' in host_name:\n",
    "    print('Working on Cobalts')\n",
    "    #data_prefix = '/data/user/ssclafani/data/cscd/final'\n",
    "    #ana_dir = '/data/user/ssclafani/data/analyses/'\n",
    "    plot_dir = cy.utils.ensure_dir('/data/user/mhuennefeld/data/analyses/DNNCascadeCodeReview/unblinding_checks/plots/unblinding/gp_ts_energy_range')\n",
    "    \n",
    "else:\n",
    "    raise ValueError('Unknown host:', host_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_path in [plot_dir]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        print('Creating directory:', dir_path)\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-mystery",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = cy.selections.Repository()\n",
    "specs = cy.selections.DNNCascadeDataSpecs.DNNC_10yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ana = cy.get_analysis(\n",
    "    repo, selection_version, specs, \n",
    "    #gammas=np.r_[0.1:6.01:0.125],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ana.anas[0]\n",
    "a.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.bg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-flavor",
   "metadata": {},
   "source": [
    "### Create ana with low energy only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_cut = 10000\n",
    "\n",
    "class DNNCascade_10yr_lowE(cy.selections.DNNCascadeDataSpecs.DNNCascade_10yr):\n",
    "    def dataset_modifications(self, ds):\n",
    "        print('Applying energy cut of {:3.0f} GeV'.format(energy_cut))\n",
    "        \n",
    "        ds.sig = ds.sig._subsample(ds.sig.energy < energy_cut)\n",
    "        ds.data = ds.data._subsample(ds.data.energy < energy_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ana_lowE = cy.get_analysis(\n",
    "    cy.selections.Repository(), selection_version, [DNNCascade_10yr_lowE], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_lowE = ana_lowE.anas[0]\n",
    "a_lowE.sig, a_lowE.bg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-adelaide",
   "metadata": {},
   "source": [
    "## Create ana with modified sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_factor = 1.3\n",
    "\n",
    "class DNNCascade_sigma(cy.selections.DNNCascadeDataSpecs.DNNCascade_10yr):\n",
    "    def dataset_modifications(self, ds):\n",
    "        print('Applying sigma factor of {:3.3f}'.format(sigma_factor))\n",
    "        \n",
    "        ds.sig['sigma'] = ds.sig['sigma'] * sigma_factor\n",
    "        ds.data['sigma'] = ds.data['sigma'] * sigma_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ana_sigma = cy.get_analysis(\n",
    "    cy.selections.Repository(), selection_version, [DNNCascade_sigma], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sigma = ana_sigma.anas[0]\n",
    "a_sigma.bg_data.sigma / a.bg_data.sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-decade",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycle\n",
    "from copy import deepcopy\n",
    "\n",
    "soft_colors = cy.plotting.soft_colors\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\n",
    "def get_bias_allt(tr, ntrials=200, n_sigs=np.r_[:101:10], quiet=False):\n",
    "    trials = [\n",
    "        (None if quiet else print(f'\\r{n_sig:4d} ...', end='', flush=True))\n",
    "        or\n",
    "        tr.get_many_fits(ntrials, n_sig=n_sig, logging=False, seed=n_sig)\n",
    "        for n_sig in n_sigs]\n",
    "    if not quiet:\n",
    "        print()\n",
    "    for (n_sig, t) in zip(n_sigs, trials):\n",
    "        t['ntrue'] = np.repeat(n_sig, len(t))\n",
    "    allt = cy.utils.Arrays.concatenate(trials)\n",
    "    return allt\n",
    "\n",
    "def get_color_cycler():\n",
    "    return cycle(colors)\n",
    "\n",
    "def plot_ns_bias(ax, tr, allt, label=''):\n",
    "\n",
    "    n_sigs = np.unique(allt.ntrue)\n",
    "    dns = np.mean(np.diff(n_sigs))\n",
    "    ns_bins = np.r_[n_sigs - 0.5*dns, n_sigs[-1] + 0.5*dns]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "\n",
    "    h = hl.hist((allt.ntrue, allt.ns), bins=(ns_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1),errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = ns_bins[[0, -1]]\n",
    "    ax.set_xlim(ax.set_ylim(lim))\n",
    "    ax.plot(lim, lim, **expect_kw)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ax.set_xlabel(r'$n_{inj}$')\n",
    "    ax.set_ylabel(r'$n_s$')\n",
    "    ax.grid()\n",
    "\n",
    "def plot_gamma_bias(ax, tr, allt, label=''):\n",
    "\n",
    "    n_sigs = np.unique(allt.ntrue)\n",
    "    dns = np.mean(np.diff(n_sigs))\n",
    "    ns_bins = np.r_[n_sigs - 0.5*dns, n_sigs[-1] + 0.5*dns]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "    expect_gamma = tr.sig_injs[0].flux[0].gamma\n",
    "\n",
    "    h = hl.hist((allt.ntrue, allt.gamma), bins=(ns_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1),errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = ns_bins[[0, -1]]\n",
    "    ax.set_xlim(lim)\n",
    "    ax.set_ylim(1, 4)\n",
    "    ax.axhline(expect_gamma, **expect_kw)\n",
    "\n",
    "    ax.set_xlabel(r'$n_{inj}$')\n",
    "    ax.set_ylabel(r'$\\gamma$')\n",
    "    ax.grid()\n",
    "\n",
    "def plot_bkg_trials(\n",
    "            bg, fig=None, ax=None, \n",
    "            label='{} bg trials', \n",
    "            label_fit=r'$\\chi^2[{:.2f}\\mathrm{{dof}},\\ \\eta={:.3f}]$', \n",
    "            color=colors[0],\n",
    "            density=False,\n",
    "            bins=50,\n",
    "        ):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    if density:\n",
    "        h = bg.get_hist(bins=bins).normalize(density=True)\n",
    "    else:\n",
    "        h = bg.get_hist(bins=bins)\n",
    "    if label is not None:\n",
    "        label = label.format(bg.n_total)\n",
    "    hl.plot1d(ax, h, crosses=True, color=color, label=label)\n",
    "\n",
    "    # compare with the chi2 fit:\n",
    "    if hasattr(bg, 'pdf'):\n",
    "        x = h.centers[0]\n",
    "        norm = h.integrate().values\n",
    "        if label_fit is not None:\n",
    "            label_fit = label_fit.format(bg.ndof, bg.eta)\n",
    "        if density:\n",
    "            ax.semilogy(x, bg.pdf(x), lw=1, ls='--', label=label_fit, color=color)\n",
    "        else:\n",
    "            ax.semilogy(x, norm * bg.pdf(x), lw=1, ls='--', label=label_fit, color=color)\n",
    "\n",
    "    ax.set_xlabel(r'TS')\n",
    "    if density:\n",
    "        ax.set_ylabel(r'Density')\n",
    "    else:\n",
    "        ax.set_ylabel(r'number of trials')\n",
    "    ax.legend()\n",
    "        \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-bridges",
   "metadata": {},
   "source": [
    "## Setup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import config as cg\n",
    "\n",
    "cg.base_dir = '/data/user/mhuennefeld/data/analyses/unblinding_v1.0.1_csky_bugfix_template_flux/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gp_tr(template_str, cutoff=np.inf, gamma=None, ana=ana, cpus=20):\n",
    "    cutoff_GeV = cutoff * 1e3\n",
    "    gp_conf = cg.get_gp_conf(\n",
    "        template_str=template_str, gamma=gamma, \n",
    "        cutoff_GeV=cutoff_GeV, base_dir=cg.base_dir)\n",
    "    \n",
    "    if template_str == 'pi0' and cutoff != np.inf or ana.keys != ['DNNCascade_10yr']:\n",
    "        print('Removing dir!', cutoff)\n",
    "        gp_conf.pop('dir')\n",
    "    \n",
    "    gp_conf['extra_keeps'] = ['azimuth']\n",
    "\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr\n",
    "\n",
    "def get_template_tr(template, gamma=2.7, cutoff_tev=np.inf, cpus=20):\n",
    "    cutoff_gev = cutoff_tev * 1000.\n",
    "    gp_conf = {\n",
    "        'template': template,\n",
    "        'flux': cy.hyp.PowerLawFlux(gamma, energy_cutoff=cutoff_gev),\n",
    "        'randomize': ['ra'],\n",
    "        'fitter_args': dict(gamma=gamma),\n",
    "        'sigsub': True,\n",
    "        'update_bg': True,\n",
    "        'fast_weight': False,\n",
    "    }\n",
    "    tr = cy.get_trial_runner(gp_conf, ana=ana, mp_cpus=cpus)\n",
    "    return tr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-tutorial",
   "metadata": {},
   "source": [
    "#### Get TrialRunners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dict = {\n",
    "    'pi0': get_gp_tr('pi0'),\n",
    "    'pi0_lowE': get_gp_tr('pi0', ana=ana_lowE),\n",
    "    'pi0_sigma': get_gp_tr('pi0', ana=ana_sigma),\n",
    "    'kra5': get_gp_tr('kra5'),\n",
    "    'kra50': get_gp_tr('kra50'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-replication",
   "metadata": {},
   "source": [
    "#### Get bkg fits for each template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_file_dict = {\n",
    "    'pi0': '{}/gp/trials/{}/{}/trials.dict'.format(cg.base_dir, 'DNNC', 'pi0'),\n",
    "    'kra5': '{}/gp/trials/{}/{}/trials.dict'.format(cg.base_dir, 'DNNC', 'kra5'),\n",
    "    'kra50': '{}/gp/trials/{}/{}/trials.dict'.format(cg.base_dir, 'DNNC', 'kra50'),\n",
    "    'pi0_lowE': os.path.join(plot_dir, 'trials_pi0_lowE.pkl'),\n",
    "    'pi0_sigma': os.path.join(plot_dir, 'trials_pi0_sigma.pkl'),\n",
    "}\n",
    "n_bkg_trials = 20000\n",
    "seed = 1337\n",
    "\n",
    "bkg_dict = {}\n",
    "for key, tr in tr_dict.items():\n",
    "    if key in bkg_file_dict and os.path.exists(bkg_file_dict[key]):\n",
    "        print('Loading background trials for template {}'.format(key))\n",
    "        sig = np.load(bkg_file_dict[key], allow_pickle=True)\n",
    "        if key in ['pi0', 'kra5', 'kra50']:\n",
    "            bkg_dict[key] = sig['poisson']['nsig'][0.0]['ts']\n",
    "        else:\n",
    "            bkg_dict[key] = sig.ts\n",
    "    \n",
    "    else:\n",
    "        print('Running background trials for template {}'.format(key))\n",
    "        trials = tr.get_many_fits(\n",
    "            n_trials=n_bkg_trials, seed=seed, mp_cpus=20)\n",
    "        bkg_dict[key] = trials.ts\n",
    "        \n",
    "        out_file = os.path.join(plot_dir, 'trials_{}.pkl'.format(key))\n",
    "        with open(out_file, 'wb') as f:\n",
    "            pickle.dump(trials, f, protocol=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-matter",
   "metadata": {},
   "source": [
    "#### Get Results for each template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for key in tr_dict.keys():\n",
    "    f_path = os.path.join(\n",
    "        cg.base_dir, \n",
    "        'gp/results/{}/{}_unblinded.npy'.format(key, key), \n",
    "    )\n",
    "    if os.path.exists(f_path):\n",
    "        res_dict[key] = np.load(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pi0_lowE' in tr_dict:\n",
    "    print(tr_dict['pi0'].to_E2dNdE(671.1))\n",
    "    print(tr_dict['pi0_lowE'].to_ns(tr_dict['pi0'].to_dNdE(671.1)))\n",
    "    print(tr_dict['pi0_lowE'].to_E2dNdE(559.47))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_dict = {\n",
    "    'pi0': 671.1, #748.0,  # bias corected\n",
    "    'pi0_lowE': 559.47, #623.58, # bias corected\n",
    "    'kra5': 242.9, #275.6, # bias corected\n",
    "    'kra50': 184.8, #211.1, # bias corected\n",
    "    'snr': 218.6,\n",
    "    'pwn': 279.6,\n",
    "    'unid': 238.4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-given",
   "metadata": {},
   "source": [
    "#### Compute likelihood/Ratio contribution  of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "def get_lr_from_trial(trial, tr, gamma=2.7):\n",
    "    \"\"\"Get event likelihood-ratio value of signal-subtracted likelihood\n",
    "    \n",
    "    Info here:\n",
    "        https://wiki.icecube.wisc.edu/index.php/\n",
    "        Cascade_7yr_PS_GP/Galactic_Source_Search_Methods#Signal-Subtracted_Likelihood\n",
    "    \"\"\"\n",
    "    L = tr.get_one_llh_from_trial(trial)\n",
    "    \n",
    "    res = L.fit(**tr.fitter_args)\n",
    "    ns = res[1]['ns']\n",
    "    N = float(len(trial.evss[0][0]))\n",
    "    print(res, ns, N)\n",
    "    \n",
    "    space_eval = cy.inspect.get_space_eval(L, -1, 0) # 0: background events (1 would be for signal events)\n",
    "    energy_eval = cy.inspect.get_energy_eval(L, -1, 0)\n",
    "    StoB_space_ss = space_eval(gamma=gamma)[1] \n",
    "    SoB_energy = energy_eval(gamma=gamma)[0]\n",
    "    SoB_space = space_eval(gamma=gamma)[0] \n",
    "    w = (SoB_space - StoB_space_ss) * SoB_energy\n",
    "    lr = w * ns/N + 1.\n",
    "    return lr\n",
    "    \n",
    "def get_lr(template_str, gamma=2.7):\n",
    "    \"\"\"Get event likelihood-ratio value of signal-subtracted likelihood\n",
    "    \n",
    "    Info here:\n",
    "        https://wiki.icecube.wisc.edu/index.php/\n",
    "        Cascade_7yr_PS_GP/Galactic_Source_Search_Methods#Signal-Subtracted_Likelihood\n",
    "    \"\"\"\n",
    "    trial = tr_dict[template_str].get_one_trial(TRUTH=True)\n",
    "    return get_lr_from_trial(trial=trial, tr=tr_dict[template_str], gamma=gamma)\n",
    "\n",
    "def compute_mc_lr(trial, tr, tr_trial, gamma=2.7):\n",
    "    L = tr.get_one_llh_from_trial(trial)\n",
    "    res = L.fit(**tr.fitter_args)\n",
    "    ns = res[1]['ns']\n",
    "    N = float(np.sum([len(t) for t in trial.evss[0]]))\n",
    "\n",
    "    space_eval = cy.inspect.get_space_eval(L, -1, 1) # 0: background events (1 would be for signal events)\n",
    "    energy_eval = cy.inspect.get_energy_eval(L, -1, 1)\n",
    "    StoB_space_ss = space_eval(gamma=gamma)[1] \n",
    "    SoB_energy = energy_eval(gamma=gamma)[0]\n",
    "    SoB_space = space_eval(gamma=gamma)[0] \n",
    "    w = (SoB_space - StoB_space_ss) * SoB_energy\n",
    "    lr = w * ns/N + 1.\n",
    "\n",
    "    # add true energy and append\n",
    "    mc_event_i = cy.utils.Arrays(trial.evss[0][1])\n",
    "    a = tr_trial.ana.anas[0]\n",
    "    assert np.allclose(mc_event_i.log10energy, a.sig[trial.evss[0][1].idx].log10energy)\n",
    "    mc_event_i['true_energy'] = a.sig[trial.evss[0][1].idx].true_energy\n",
    "    \n",
    "    return lr, mc_event_i\n",
    "        \n",
    "def get_mc_lr(tr, n_sig, gamma=2.7, seed=42, n_reps=100):\n",
    "    \"\"\"Get event likelihood-ratio value of signal-subtracted likelihood for injected MC events\n",
    "    \n",
    "    Info here:\n",
    "        https://wiki.icecube.wisc.edu/index.php/\n",
    "        Cascade_7yr_PS_GP/Galactic_Source_Search_Methods#Signal-Subtracted_Likelihood\n",
    "    \"\"\"\n",
    "    mc_events = []\n",
    "    lr_list = []\n",
    "    \n",
    "    for i in tqdm(range(n_reps)):\n",
    "        trial = tr.get_one_trial(n_sig=n_sig, seed=seed + i)\n",
    "        lr, mc_event_i = compute_mc_lr(trial=trial, tr=tr, tr_trial=tr, gamma=gamma)\n",
    "        \n",
    "        # append \n",
    "        mc_events.append(mc_event_i)\n",
    "        lr_list.append(lr)\n",
    "    \n",
    "    lr_list = np.concatenate(lr_list)\n",
    "    mc_events = cy.utils.Arrays.concatenate(mc_events)\n",
    "    return lr_list, mc_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dict = {\n",
    "    'pi0': get_lr('pi0'),\n",
    "    'kra5': get_lr('kra5', None),\n",
    "    'kra50': get_lr('kra50', None),\n",
    "}\n",
    "\n",
    "ts_dict = {}\n",
    "for key, lr in lr_dict.items():\n",
    "    ts = 2 * np.log(lr)\n",
    "    print(key, np.sum(ts))\n",
    "    ts_dict[key] = ts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ts_dict['pi0'], bins=1000)\n",
    "plt.yscale('log')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-pressure",
   "metadata": {},
   "source": [
    "#### Compute MC-based LR values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_mc_dict = {\n",
    "    'pi0': get_mc_lr(tr=tr_dict['pi0'], n_sig=ns_dict['pi0']),\n",
    "    'pi0_lowE': get_mc_lr(tr=tr_dict['pi0_lowE'], n_sig=ns_dict['pi0_lowE']),\n",
    "    'kra5': get_mc_lr(tr=tr_dict['kra5'], n_sig=ns_dict['kra5'], gamma=None),\n",
    "    'kra50': get_mc_lr(tr=tr_dict['kra50'], n_sig=ns_dict['kra50'], gamma=None),\n",
    "}\n",
    "\n",
    "ts_mc_dict = {}\n",
    "for key, (lr, _) in lr_mc_dict.items():\n",
    "    ts = 2 * np.log(lr)\n",
    "    print(key, np.sum(ts))\n",
    "    ts_mc_dict[key] = ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-monday",
   "metadata": {},
   "source": [
    "#### Get event weights based on TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_from_ts(ts, method='clip'):\n",
    "    if method == 'clip':\n",
    "        ts = np.clip(ts, 0., np.inf)\n",
    "    elif method == 'abs':\n",
    "        ts = np.abs(ts)\n",
    "    elif method == 'all':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('Unknown method: {}'.format(method))\n",
    "    return ts / np.sum(ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-plane",
   "metadata": {},
   "source": [
    "#### Marginalized distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_quantile(x, weights, quantile):\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(x)\n",
    "\n",
    "    sorted_indices = np.argsort(x)\n",
    "    x_sorted = x[sorted_indices]\n",
    "    weights_sorted = weights[sorted_indices]\n",
    "    cum_weights = np.cumsum(weights_sorted) / np.sum(weights)\n",
    "    idx = np.searchsorted(cum_weights, quantile)\n",
    "    return x_sorted[idx]\n",
    "\n",
    "def make_marginalized_plot(ts, method='clip', title=None, cols=['dec', 'ra', 'energy', 'sigma', 'azimuth'], n_bins=30, cl=0.68):\n",
    "    \n",
    "    weights = get_weights_from_ts(ts, method=method)\n",
    "    fig, axes = plt.subplots(len(cols), 1, figsize=(9, 3*len(cols)))\n",
    "    \n",
    "    q_tail = (1. - cl) / 2.\n",
    "    for col, ax in zip(cols, axes):\n",
    "        \n",
    "        # define defaults\n",
    "        xlabel = col\n",
    "        data = a.bg_data[col]\n",
    "        bins = n_bins\n",
    "        \n",
    "        # modify defaults\n",
    "        if col == 'dec':\n",
    "            xlabel = r'Dec: $\\delta_\\mathrm{reco}$ / °'\n",
    "            data = np.rad2deg(a.bg_data[col])\n",
    "            bins = np.linspace(-90, 90., n_bins)\n",
    "        elif col == 'energy':\n",
    "            xlabel = r'$E_\\mathrm{reco}$ / GeV'\n",
    "            bins = np.logspace(np.log10(500), 7, n_bins)\n",
    "        elif col == 'ra':\n",
    "            xlabel = r'RA: $\\alpha_\\mathrm{reco}$ / °'\n",
    "            data = np.rad2deg(a.bg_data[col])\n",
    "            bins = np.linspace(0, 360., n_bins)\n",
    "        elif col == 'sigma':\n",
    "            data = np.rad2deg(a.bg_data[col])\n",
    "            xlabel = r'$\\sigma_\\mathrm{reco}$ / °'\n",
    "            bins = np.linspace(0, 45., n_bins)\n",
    "        \n",
    "        # define what data unit to use for quantile\n",
    "        data_avg = data\n",
    "        if col == 'energy':\n",
    "            data_avg = a.bg_data[col]\n",
    "        \n",
    "        avg_q = np.average(data_avg, weights=weights)\n",
    "        med_q = weighted_quantile(data_avg, weights=weights, quantile=0.5)\n",
    "        lower_q = weighted_quantile(data_avg, weights=weights, quantile=q_tail)\n",
    "        upper_q = weighted_quantile(data_avg, weights=weights, quantile=1. - q_tail)\n",
    "        \n",
    "        label = '{}: {:3.2f} | {:.0f}% CL: [{:3.2f}, {:3.2f}])'.format(\n",
    "            col, avg_q, cl*100., lower_q, upper_q)\n",
    "        \n",
    "        ax.axvline(lower_q, ls='--', color='0.7', label='{:.0f}% CL'.format(cl*100.))\n",
    "        ax.axvline(upper_q, ls='--', color='0.7')\n",
    "        ax.axvspan(lower_q, upper_q, color='0.7', alpha=.2)\n",
    "        ax.axvline(avg_q, ls='-.', color='0.7', label='Average')\n",
    "        ax.axvline(med_q, ls='-', color='0.7', label='Median')\n",
    "            \n",
    "        ax.hist(data, weights=weights, bins=bins, label=label, density=True)\n",
    "        ax.hist(data, histtype='step', label='All Data', bins=bins, density=True)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel('Density')\n",
    "        if col == 'energy':\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "    \n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "fig, axes = make_marginalized_plot(ts_dict['kra5'], title='Model: KRA-5PeV')\n",
    "fig.savefig('{}/ts_weighted_distribution_kra5.png'.format(plot_dir))\n",
    "\n",
    "fig, axes = make_marginalized_plot(ts_dict['kra50'], title='Model: KRA-50PeV')\n",
    "fig.savefig('{}/ts_weighted_distribution_kra50.png'.format(plot_dir))\n",
    "\n",
    "fig, axes = make_marginalized_plot(ts_dict['pi0'], title='Model: $\\pi^0$')\n",
    "fig.savefig('{}/ts_weighted_distribution_pi0.png'.format(plot_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-internship",
   "metadata": {},
   "source": [
    "#### Test Effect of Energy Range on fitted Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "def run_fit_trials(n_trials, seed=42):\n",
    "    fit_flux_dict = {k: [] for k in ['pi0', 'pi0_mixed', 'pi0_lowE']}\n",
    "    fit_ns_dict = {k: [] for k in ['pi0', 'pi0_mixed', 'pi0_lowE']}\n",
    "    fit_ts_dict = {k: [] for k in ['pi0', 'pi0_mixed', 'pi0_lowE']}\n",
    "    fit_energy_dict = {k: [] for k in ['pi0', 'pi0_mixed', 'pi0_lowE']}\n",
    "    fit_ts_total_dict = {k: [] for k in ['pi0', 'pi0_mixed', 'pi0_lowE']}\n",
    "    \n",
    "    for i in tqdm(range(n_trials), total=n_trials):\n",
    "        seed_i = seed + i\n",
    "        \n",
    "        # create trial with our baseline analysis\n",
    "        trial_pi0 = tr_dict['pi0'].get_one_trial(seed=seed_i, n_sig=ns_dict['pi0'], poisson=False)\n",
    "        trial_pi0_mixed = tr_dict['pi0'].get_one_trial(seed=seed_i, n_sig=ns_dict['pi0_lowE'], poisson=False)\n",
    "\n",
    "        # create trial for lowE analaysis to steal the signal events\n",
    "        trial_pi0_lowE = tr_dict['pi0_lowE'].get_one_trial(seed=seed_i, n_sig=ns_dict['pi0_lowE'], poisson=False)\n",
    "\n",
    "        # mix-and-match: combine signal from lowE to bkg trial of baseline analysis\n",
    "        # This way we now have injected signal at lowE while lacking corresponding part at high E\n",
    "        trial_pi0_mixed.evss[0][1] = trial_pi0_lowE.evss[0][1]\n",
    "\n",
    "        # now we can perform a fit with both analyses. \n",
    "        # Both trials have the same signal injection, so difference in fitted normalization\n",
    "        # may be caused by missing high-energy signal events in baseline analysis\n",
    "        fit_dict = {\n",
    "            'pi0': (tr_dict['pi0'], tr_dict['pi0'].get_one_fit_from_trial(trial_pi0, flat=False)),\n",
    "            'pi0_mixed': (tr_dict['pi0'], tr_dict['pi0'].get_one_fit_from_trial(trial_pi0_mixed, flat=False)),\n",
    "            'pi0_lowE': (tr_dict['pi0_lowE'], tr_dict['pi0_lowE'].get_one_fit_from_trial(trial_pi0_lowE, flat=False)),\n",
    "        }\n",
    "        for key, (tr, fit) in fit_dict.items():\n",
    "            fit_flux_dict[key].append(tr.to_dNdE(fit[1]['ns']))\n",
    "            fit_ns_dict[key].append(fit[1]['ns'])\n",
    "            fit_ts_total_dict[key].append(fit[0])\n",
    "        \n",
    "        # get ts weights and energies \n",
    "        lr_pi0, mc_event_pi0 = compute_mc_lr(trial=trial_pi0, tr=tr_dict['pi0'], tr_trial=tr_dict['pi0'])\n",
    "        lr_pi0_mixed, mc_event_pi0_mixed = compute_mc_lr(trial=trial_pi0_mixed, tr=tr_dict['pi0'], tr_trial=tr_dict['pi0_lowE'])\n",
    "        lr_pi0_lowE, mc_event_pi0_lowE = compute_mc_lr(trial=trial_pi0_lowE, tr=tr_dict['pi0_lowE'], tr_trial=tr_dict['pi0_lowE'])\n",
    "        \n",
    "        fit_ts_dict['pi0'].append( 2 * np.log(lr_pi0))\n",
    "        fit_ts_dict['pi0_mixed'].append( 2 * np.log(lr_pi0_mixed))\n",
    "        fit_ts_dict['pi0_lowE'].append( 2 * np.log(lr_pi0_lowE))\n",
    "        \n",
    "        fit_energy_dict['pi0'].append(mc_event_pi0.true_energy)\n",
    "        fit_energy_dict['pi0_mixed'].append(mc_event_pi0_mixed.true_energy)\n",
    "        fit_energy_dict['pi0_lowE'].append(mc_event_pi0_lowE.true_energy)\n",
    "        \n",
    "    for key in fit_flux_dict.keys():\n",
    "        fit_flux_dict[key] = np.array(fit_flux_dict[key])\n",
    "        fit_ns_dict[key] = np.array(fit_ns_dict[key])\n",
    "        fit_ts_total_dict[key] = np.array(fit_ts_total_dict[key])\n",
    "        fit_ts_dict[key] = np.concatenate(fit_ts_dict[key])\n",
    "        fit_energy_dict[key] = np.concatenate(fit_energy_dict[key])\n",
    "    \n",
    "    return fit_flux_dict, fit_ns_dict, fit_ts_total_dict, fit_ts_dict, fit_energy_dict\n",
    "\n",
    "fit_flux_dict, fit_ns_dict, fit_ts_total_dict, fit_ts_dict, fit_energy_dict = run_fit_trials(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in fit_flux_dict.keys():\n",
    "    print('Test {}:'.format(key))\n",
    "    print('  TS: {:5.1f} +- {:3.1f} | ns: {:6.1f} +- {:3.1f}'.format(\n",
    "        np.mean(fit_ts_total_dict[key]), np.std(fit_ts_total_dict[key]),\n",
    "        np.mean(fit_ns_dict[key]), np.std(fit_ns_dict[key]),\n",
    "    ))\n",
    "    print('  dNdE at 1 GeV: {:3.2e} +- {:3.2e}'.format(\n",
    "        np.mean(fit_flux_dict[key]), np.std(fit_flux_dict[key]),\n",
    "    ))\n",
    "    weights = get_weights_from_ts(fit_ts_dict[key])\n",
    "    for cl in [0.68, 0.9]:\n",
    "        q_tail = (1. - cl) / 2.\n",
    "        print('  TS-based Range {:2.0f}% CL: [{:3.1f} GeV, {:3.1f} GeV]'.format(\n",
    "            cl*100.,\n",
    "            weighted_quantile(fit_energy_dict[key], weights=weights, quantile=q_tail),\n",
    "            weighted_quantile(fit_energy_dict[key], weights=weights, quantile=1. -q_tail),\n",
    "        ))\n",
    "        print('  True MC Range {:2.0f}% CL:  [{:3.1f} GeV, {:3.1f} GeV]'.format(\n",
    "            cl*100.,\n",
    "            weighted_quantile(fit_energy_dict[key], weights=None, quantile=q_tail),\n",
    "            weighted_quantile(fit_energy_dict[key], weights=None, quantile=1. -q_tail),\n",
    "        ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-archives",
   "metadata": {},
   "source": [
    "#### \"Unfold\" true energy\n",
    "\n",
    "Define mapping to go from: (delta_rec, E_rec) -> E_neutrino, i.e. P(E_neutrino | delta_rec, E_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "import histlite as hl\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "def get_flux(tr):\n",
    "    for key, item in tr.llh_kw['conf'].items():\n",
    "        if 'flux' == key:\n",
    "            return item\n",
    "        if isinstance(item, dict):\n",
    "            for key_sub, item_sub in item.items():\n",
    "                if key_sub == 'flux':\n",
    "                    return item_sub\n",
    "    return None\n",
    "\n",
    "   \n",
    "def get_pdf(template_str, dec=a.bg_data.dec, energy=a.bg_data.energy, sigma=a.bg_data.sigma, \n",
    "            width_dec=np.deg2rad(45), width_log_energy=0.05, width_sigma=np.deg2rad(1000),\n",
    "            ts_weighting_method='clip',\n",
    "            disable_tqdm=False,\n",
    "            ts=None, pdf_in_log=False, bins=None, cpus=15):\n",
    "    \"\"\"Get PDF in E_nu for given set of reconstructed values\n",
    "    \n",
    "    Notes: histlite's \"contain()\" method does not account for bin_width, i.e it assumes that \n",
    "    the value of particular bin is the probability of that bin. To achieve this, we must\n",
    "    normalize the hist with the `density=True` flag, which will divide bin content by the\n",
    "    bin width.\n",
    "    \"\"\"\n",
    "    tr = tr_dict[template_str]\n",
    "    if ts is None:\n",
    "        ts = ts_dict[template_str]\n",
    "    \n",
    "    if bins is None:\n",
    "        if pdf_in_log:\n",
    "            bins = np.linspace(2, 7, 100)\n",
    "        else:\n",
    "            bins = np.logspace(2, 7, 100)\n",
    "            \n",
    "    # get ts-weights\n",
    "    ts_weights = get_weights_from_ts(ts, method=ts_weighting_method)\n",
    "    total_ts_weight = np.sum(ts_weights)\n",
    "    \n",
    "    assert len(ts_weights) == len(energy)\n",
    "    assert len(ts_weights) == len(dec)\n",
    "    assert len(ts_weights) == len(sigma)\n",
    "    \n",
    "    # assume flux of fitted model for weighting of MC    \n",
    "    flux = get_flux(tr)\n",
    "    weights = a.sig.oneweight * flux(a.sig.true_energy)\n",
    "\n",
    "    log_energy = np.log10(energy)\n",
    "       \n",
    "    h_total = None\n",
    "    hist_list = []\n",
    "    sig_energy_weights = np.zeros_like(a.sig.true_energy)\n",
    "    for dec_i, log_energy_i, sigma_i, w_i in tqdm(zip(dec, log_energy, sigma, ts_weights), total=len(dec), disable=disable_tqdm):\n",
    "        mask_dec = np.logical_and(\n",
    "            a.sig.dec > dec_i - width_dec,\n",
    "            a.sig.dec < dec_i + width_dec,\n",
    "        )\n",
    "        mask_energy = np.logical_and(\n",
    "            a.sig.log10energy > log_energy_i - width_log_energy,\n",
    "            a.sig.log10energy < log_energy_i + width_log_energy,\n",
    "        )\n",
    "        mask_sigma = np.logical_and(\n",
    "            a.sig.sigma > sigma_i - width_sigma,\n",
    "            a.sig.sigma < sigma_i + width_sigma,\n",
    "        )\n",
    "        mask = np.logical_and(mask_dec, mask_energy)\n",
    "        mask = np.logical_and(mask, mask_sigma)\n",
    "        \n",
    "        if np.sum(mask) < 500:\n",
    "            print(np.sum(mask))\n",
    "        \n",
    "        # create PDF\n",
    "        if pdf_in_log:\n",
    "            data = np.log10(a.sig.true_energy[mask])\n",
    "        else:\n",
    "            data = a.sig.true_energy[mask]\n",
    "        \n",
    "        sig_energy_weights[mask] += (\n",
    "            w_i * weights[mask] / np.sum(weights[mask]) / total_ts_weight\n",
    "        )\n",
    "        \n",
    "        #hist = hl.hist(data, weights=weights[mask], bins=bins).normalize()\n",
    "        hist = hl.hist(data, weights=weights[mask], bins=bins).normalize(density=True)\n",
    "        #hist = hl.hist(data, weights=weights[mask], bins=bins) / np.diff(bins) / np.sum(weights[mask])\n",
    "        hist_list.append(hist)\n",
    "        \n",
    "        if h_total is None:\n",
    "            h_total = w_i / total_ts_weight * hist\n",
    "        else:\n",
    "            h_total += w_i / total_ts_weight * hist\n",
    "    \n",
    "    # print('SUM weights:', np.sum(sig_energy_weights))\n",
    "    \n",
    "    return h_total, hist_list, sig_energy_weights\n",
    "\n",
    "\n",
    "if True:\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    color_cycler = get_color_cycler()\n",
    "    #for dec in np.linspace(-np.pi/2., np.pi/2, 1):\n",
    "    for dec in [0.]:\n",
    "        for sigma in np.deg2rad([5]):\n",
    "            for energy in np.logspace(np.log10(500), 6, 4):\n",
    "            #for energy in np.logspace(3, 3, 1):\n",
    "                color = next(color_cycler)\n",
    "                h_total, hist_list, sig_energy_weights = get_pdf(\n",
    "                    'pi0', dec=[dec], energy=[energy], sigma=[sigma], ts=[1.])\n",
    "                hl.plot1d(h=h_total, ax=ax, color=color, label='{:3.1f} GeV | {:3.1f} ° | {:3.1f} °'.format(energy, np.rad2deg(dec), np.rad2deg(sigma)))\n",
    "                ax.axvline(energy, color=color, ls='--')\n",
    "                ax.axvline(weighted_quantile(a.sig.true_energy, weights=sig_energy_weights, quantile=0.5), color=color, ls='-')\n",
    "                ax.axvline(h_total.contain(axis=0, frac=0.5).values, color=color, ls='-.')\n",
    "    ax.legend()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-metallic",
   "metadata": {},
   "source": [
    "#### Test Quantile Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_quantile2(values, quantiles, sample_weight=None, \n",
    "                      values_sorted=False, old_style=False):\n",
    "    \"\"\" Very close to numpy.percentile, but supports weights.\n",
    "    NOTE: quantiles should be in [0, 1]!\n",
    "    :param values: numpy.array with data\n",
    "    :param quantiles: array-like with many quantiles needed\n",
    "    :param sample_weight: array-like of the same length as `array`\n",
    "    :param values_sorted: bool, if True, then will avoid sorting of\n",
    "        initial array\n",
    "    :param old_style: if True, will correct output to be consistent\n",
    "        with numpy.percentile.\n",
    "    :return: numpy.array with computed quantiles.\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    quantiles = np.array(quantiles)\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(len(values))\n",
    "    sample_weight = np.array(sample_weight)\n",
    "    assert np.all(quantiles >= 0) and np.all(quantiles <= 1), \\\n",
    "        'quantiles should be in [0, 1]'\n",
    "\n",
    "    if not values_sorted:\n",
    "        sorter = np.argsort(values)\n",
    "        values = values[sorter]\n",
    "        sample_weight = sample_weight[sorter]\n",
    "\n",
    "    weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight\n",
    "    if old_style:\n",
    "        # To be convenient with numpy.percentile\n",
    "        weighted_quantiles -= weighted_quantiles[0]\n",
    "        weighted_quantiles /= weighted_quantiles[-1]\n",
    "    else:\n",
    "        weighted_quantiles /= np.sum(sample_weight)\n",
    "    return np.interp(quantiles, weighted_quantiles, values)\n",
    "\n",
    "for q in np.linspace(0., 0.99, 10):\n",
    "    print('q: {:3.2f}: {:8.1f} [True] | {:8.1f} [Hists] | {:8.1f} [Events]'.format(\n",
    "        q, \n",
    "        np.quantile(a.sig.true_energy, q),\n",
    "        weighted_quantile(a.sig.true_energy, quantile=q, weights=None),\n",
    "        weighted_quantile2(a.sig.true_energy, quantiles=q, sample_weight=None),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "h_total, hist_list, sig_energy_weights = get_pdf(\n",
    "    'pi0', dec=a.sig.dec[:n], energy=a.sig.energy[:n], sigma=a.sig.sigma[:n], ts=np.ones_like(ts_dict['pi0'][:n]),\n",
    "    #bins=np.logspace(1, 9, 50), width_log_energy=0.1,\n",
    ")\n",
    "hl.plot1d(h=h_total, ax=ax, label='h_total', ls=':')\n",
    "h2 = hl.hist(a.sig.true_energy, weights=sig_energy_weights, bins=h_total.bins).normalize(density=True)\n",
    "hl.plot1d(h=h2, ax=ax, label='h_events', ls='--')\n",
    "ax.hist(a.sig.true_energy[:n], bins=h_total.bins[0], density=True, histtype='step', label='True')\n",
    "\n",
    "for q in np.linspace(0., 0.99, 10):\n",
    "    print('q: {:3.2f}: {:8.1f} [True] | {:8.1f} [Hists] | {:8.1f} [Events]'.format(\n",
    "        q, \n",
    "        np.quantile(a.sig.true_energy[:n], q),\n",
    "        #weighted_quantile(a.sig.true_energy[:n], quantile=q, weights=None),\n",
    "        #weighted_quantile2(a.sig.true_energy[:n], quantiles=q, sample_weight=None),\n",
    "        h_total.contain(0, frac=q).values,\n",
    "        weighted_quantile(a.sig.true_energy, weights=sig_energy_weights, quantile=q),\n",
    "    ))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-jonathan",
   "metadata": {},
   "source": [
    "#### Compute Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import timeit\n",
    "\n",
    "cpus = 3\n",
    "ts_weighting_method = 'clip'  # GP paper uses 'clip'\n",
    "#ts_weighting_method = 'all'\n",
    "\n",
    "print('Starting pool with {} cpus'.format(cpus))\n",
    "\n",
    "arg_list = ['pi0', 'kra5', 'kra50']\n",
    "if cpus == 1:\n",
    "    result = [get_pdf(k) for k in arg_list]\n",
    "else:\n",
    "    def get_pdf_multiprocessing(template_str):\n",
    "        return get_pdf(template_str=template_str, ts_weighting_method=ts_weighting_method, disable_tqdm=True)\n",
    "    \n",
    "    with Pool(cpus) as p:\n",
    "        start_t = timeit.default_timer()\n",
    "        result = p.map(get_pdf_multiprocessing, arg_list)\n",
    "        end_t = timeit.default_timer()\n",
    "        print(end_t - start_t)\n",
    "\n",
    "h_total_dict = {}\n",
    "hist_list_dict = {}\n",
    "sig_energy_weights_dict = {}\n",
    "for i, key in enumerate(arg_list):\n",
    "    h_total_dict[key] = result[i][0]\n",
    "    hist_list_dict[key] = result[i][1]\n",
    "    sig_energy_weights_dict[key] = result[i][2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-confusion",
   "metadata": {},
   "source": [
    "#### Compute Quantiles for Models [data derived energy range via TS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-certification",
   "metadata": {},
   "source": [
    "The GP paper uses the data derived range via the positive TS-distribution of the experimental data events. \n",
    "Via the above generated PDFs, for every data event we can determine likely true neutrino energies for a given reconstructed declination and energy: $(\\delta_\\mathrm{rec}, E_\\mathrm{rec}) \\rightarrow E_\\nu$, i.e. $P(E_\\nu | \\delta_\\mathrm{rec}, E_\\mathrm{rec})$. \n",
    "Essentially we are accumulating the weighted PDFs in true neutrino energies. The PDFs are weighted by the event's contribution to the overall analysis TS value. We use the \"clipped\" method, so events with TS contributions $<0$ are set to have weights equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sig_energy_weights_dict.keys():\n",
    "    print('Model {} [Data Derived TS]:'.format(key))\n",
    "    for cl in [.68, 0.9]:\n",
    "        q_tail = (1. - cl) / 2.\n",
    "        print('  {:3.0f} GeV | {:3.1f}% CL: [{:3.0f}, {:3.0f}]'.format(\n",
    "            np.average(a.sig.true_energy, weights=sig_energy_weights_dict[key]), cl*100.,\n",
    "            weighted_quantile(a.sig.true_energy, weights=sig_energy_weights_dict[key], quantile=q_tail),\n",
    "            weighted_quantile(a.sig.true_energy, weights=sig_energy_weights_dict[key], quantile=1-q_tail),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-merchandise",
   "metadata": {},
   "source": [
    "#### Formatting for table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 1000.\n",
    "for key in sig_energy_weights_dict.keys():\n",
    "    print('Model {} [Data Derived TS]:'.format(key))\n",
    "    msg = '  '\n",
    "    for q in [0.05, 0.16, 0.5, 0.84, 0.95]:\n",
    "        msg += '{:3.2f} & '.format(\n",
    "            weighted_quantile(a.sig.true_energy, weights=sig_energy_weights_dict[key], quantile=q) / unit)\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-watson",
   "metadata": {},
   "source": [
    "The remainder of this notebook computes alternative MC-based variations to determine the energy range of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-anniversary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dental-southwest",
   "metadata": {},
   "source": [
    "#### Compute MC expectation\n",
    "\n",
    "Note that this is not exactly the same as the data-derived TS-based energy range, since the MC range here is only calculated on the injected signal events. To make this equal, the MC range here should be calculated based on the entire sample, i.e. background MC plus injected signal events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, (lr, mc_events) in lr_mc_dict.items():\n",
    "    \n",
    "    weights = get_weights_from_ts(ts_mc_dict[key])\n",
    "        \n",
    "    print('Model {} [MC expectation]:'.format(key))\n",
    "    for cl in [.68, 0.9]:\n",
    "        q_tail = (1. - cl) / 2.\n",
    "        print('  {:3.0f} GeV | {:3.1f}% CL: [{:3.0f}, {:3.0f}]'.format(\n",
    "            np.average(mc_events.true_energy, weights=weights), cl*100.,\n",
    "            weighted_quantile(mc_events.true_energy, weights=weights, quantile=q_tail),\n",
    "            weighted_quantile(mc_events.true_energy, weights=weights, quantile=1-q_tail),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-entity",
   "metadata": {},
   "source": [
    "#### Compute MC expectation (neglects ts-weighting of MC distribution)\n",
    "\n",
    "This is just the true distribution of injected signal MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tr_dict.keys():\n",
    "    \n",
    "    flux = get_flux(tr_dict[key])\n",
    "    weights = a.sig.oneweight * flux(a.sig.true_energy)\n",
    "        \n",
    "    print('Model {} [MC expectation neglecting TS-weights]:'.format(key))\n",
    "    for cl in [.68, 0.9]:\n",
    "        q_tail = (1. - cl) / 2.\n",
    "        print('  {:3.0f} GeV | {:3.1f}% CL: [{:3.0f}, {:3.0f}]'.format(\n",
    "            np.average(a.sig.true_energy, weights=weights), cl*100.,\n",
    "            weighted_quantile(a.sig.true_energy, weights=weights, quantile=q_tail),\n",
    "            weighted_quantile(a.sig.true_energy, weights=weights, quantile=1-q_tail),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-lightweight",
   "metadata": {},
   "source": [
    "#### Helper functions for Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_injected_events(tr, n_sig, seed=None):\n",
    "    injector = tr.sig_injs[0]\n",
    "    original_keep = [k for k in injector.keep]\n",
    "    if 'true_energy' not in injector.keep:\n",
    "        injector.keep += ['true_energy']\n",
    "    injected_events = injector.inject(n_sig, seed=seed)[0][0]\n",
    "    injector.keep = original_keep\n",
    "    return injected_events\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_energy_dist(h_total, sig_energy_weights, cl=0.68, fig=None, ax=None, is_log=False, mc_tr=None, add_mc=False, mc_seed=42):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "    q_tail = (1. - cl) / 2.\n",
    "    med_q = weighted_quantile(a.sig.true_energy, weights=sig_energy_weights, quantile=0.5)\n",
    "    lower_q = weighted_quantile(a.sig.true_energy, weights=sig_energy_weights, quantile=q_tail)\n",
    "    upper_q = weighted_quantile(a.sig.true_energy, weights=sig_energy_weights, quantile=1. - q_tail)\n",
    "    \n",
    "    if is_log:\n",
    "        label = r'$E_\\nu$: {:3.2f} GeV | {:.0f}% CL: [{:3.0f} GeV, {:3.0f} GeV])'.format(\n",
    "            10**med_q, cl*100., 10**lower_q, 10**upper_q)\n",
    "    else:\n",
    "        label = r'$E_\\nu$: {:3.2f} GeV | {:.0f}% CL: [{:3.0f} GeV, {:3.0f} GeV])'.format(\n",
    "            med_q, cl*100., lower_q, upper_q)\n",
    "\n",
    "    ax.axvline(lower_q, ls='--', color='0.7', label='{:.0f}% CL'.format(cl*100.))\n",
    "    ax.axvline(upper_q, ls='--', color='0.7')\n",
    "    ax.axvspan(lower_q, upper_q, color='0.7', alpha=.2)\n",
    "    ax.axvline(med_q, ls='-', color='0.7', label='Median')\n",
    "\n",
    "    hl.plot1d(ax=ax, h=h_total, label=label)\n",
    "    #ax.hist(a.sig.true_energy, weights=sig_energy_weights, bins=h_total.bins[0], density=True, label='MPL')\n",
    "    \n",
    "    if mc_tr is not None and add_mc:\n",
    "        if False:\n",
    "            mc_events = get_injected_events(mc_tr, 1000000, seed=mc_seed)\n",
    "\n",
    "            if is_log:\n",
    "                h_mc = hl.hist(np.log10(mc_events.true_energy), bins=h_total.bins).normalize(density=True)\n",
    "            else:\n",
    "                h_mc = hl.hist(mc_events.true_energy, bins=h_total.bins).normalize(density=True)\n",
    "            print(\n",
    "                'MC:', \n",
    "                np.quantile(mc_events.true_energy, q_tail),\n",
    "                np.quantile(mc_events.true_energy, 0.5),\n",
    "                np.quantile(mc_events.true_energy, 1. - q_tail),\n",
    "            )\n",
    "            hl.plot1d(ax=ax, h=h_mc, label='MC [template sampled] (assumes uniform TS-weights)')\n",
    "\n",
    "            flux = get_flux(mc_tr)\n",
    "            weights = a.sig.oneweight * flux(a.sig.true_energy)\n",
    "            h_mc_all = hl.hist(a.sig.true_energy, bins=h_total.bins, weights=weights).normalize(density=True)\n",
    "            hl.plot1d(ax=ax, h=h_mc_all, label='MC [total MC] (assumes uniform TS-weights)')\n",
    "        \n",
    "        if True:\n",
    "            lr, mc_events_ts = lr_mc_dict[key]\n",
    "            weights = get_weights_from_ts(ts_mc_dict[key])\n",
    "            print(\n",
    "                'MC [injection trials]:', \n",
    "                weighted_quantile(mc_events_ts.true_energy, weights=weights, quantile=0.5),\n",
    "                weighted_quantile(mc_events_ts.true_energy, weights=weights, quantile=q_tail),\n",
    "                weighted_quantile(mc_events_ts.true_energy, weights=weights, quantile=1. - q_tail),\n",
    "            )\n",
    "            h_mc_ts = hl.hist(mc_events_ts.true_energy, bins=h_total.bins, weights=weights).normalize(density=True)\n",
    "            hl.plot1d(ax=ax, h=h_mc_ts, label='MC [injection trials]')\n",
    "        \n",
    "        if False:\n",
    "            if key + '_lowE' in lr_mc_dict:\n",
    "                lr, mc_events_ts_lowE = lr_mc_dict[key + '_lowE']\n",
    "                weights = get_weights_from_ts(ts_mc_dict[key + '_lowE'])\n",
    "                h_mc_ts = hl.hist(mc_events_ts_lowE.true_energy, bins=h_total.bins, weights=weights).normalize(density=True)\n",
    "                hl.plot1d(ax=ax, h=h_mc_ts, label='MC [injection trials] [lowE]')\n",
    "            \n",
    "    if is_log:\n",
    "        ax.set_xlabel(r'$\\log_{10}(E_{\\nu} / \\mathrm{GeV})$')\n",
    "        ax.set_ylabel(r'$P \\, ( \\, \\log_{10}[E_{\\nu} \\, / \\, \\mathrm{GeV}] \\, )$')\n",
    "    else:\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlabel(r'$E_{\\nu} / \\mathrm{GeV}$')\n",
    "        ax.set_ylabel(r'$P \\, ( \\, E_{\\nu} \\, / \\, \\mathrm{GeV} \\, )$')\n",
    "        \n",
    "    ax.legend()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Make Plot\n",
    "for add_mc in [True, False]:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(9, 9), sharex=True)\n",
    "    \n",
    "    for ax, key, name in zip(axes, ['pi0', 'kra5', 'kra50'], ['$\\pi^0$', 'KRA-5PeV', 'KRA-50PeV']):\n",
    "        plot_energy_dist(h_total=h_total_dict[key], sig_energy_weights=sig_energy_weights_dict[key], \n",
    "                         fig=fig, ax=ax, mc_tr=tr_dict[key], add_mc=add_mc)\n",
    "        ax.set_title('Model: {}'.format(name))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if add_mc:\n",
    "        for ax in axes:\n",
    "            ax.set_yscale('log')\n",
    "            ax.set_ylim(1e-8)\n",
    "        fig.savefig('{}/ts_weighted_energy_with_mc.png'.format(plot_dir))\n",
    "    else:\n",
    "        fig.savefig('{}/ts_weighted_energy.png'.format(plot_dir))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-minneapolis",
   "metadata": {},
   "source": [
    "## Compute MC-based TS Range\n",
    "\n",
    "This is an approach to calculate an equivalent TS-based range for MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mc_pseudo_trial(template_str, ns_dict=ns_dict, seed=None):\n",
    "    trial = tr_dict[template_str].get_one_trial(n_sig=ns_dict[template_str],seed=seed)\n",
    "    \n",
    "    # move injected events to original data events to immitate\n",
    "    # real data\n",
    "    bkg_events = trial[0][0][0]\n",
    "    sig_events = trial[0][0][1]\n",
    "    \n",
    "    # make keys match\n",
    "    sig_events['log10energy'] = sig_events.log10energy\n",
    "    \n",
    "    # append signal evens\n",
    "    events = cy.utils.Events.concatenate([bkg_events, sig_events])\n",
    "    \n",
    "    trial[0][0] = [events]\n",
    "    \n",
    "    return trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 10\n",
    "\n",
    "# get pseudo data\n",
    "pseudo_trials = {}\n",
    "for i in range(n_reps):\n",
    "    pseudo_trials['pi0_{:02d}'.format(i)] = get_mc_pseudo_trial(template_str='pi0', seed=i)\n",
    "    pseudo_trials['kra5_{:02d}'.format(i)] = get_mc_pseudo_trial(template_str='kra5', seed=i)\n",
    "    pseudo_trials['kra50_{:02d}'.format(i)] = get_mc_pseudo_trial(template_str='kra50', seed=i)\n",
    "    \n",
    "lr_pseudo_dict = {}\n",
    "for i in range(n_reps):\n",
    "    for k in ['pi0', 'kra5', 'kra50']:\n",
    "        key = '{}_{:02d}'.format(k, i)\n",
    "        lr_pseudo_dict[key] = get_lr_from_trial(trial=pseudo_trials[key], tr=tr_dict[k])\n",
    "\n",
    "ts_pseudo_dict = {}\n",
    "for key, lr in lr_pseudo_dict.items():\n",
    "    ts = 2 * np.log(lr)\n",
    "    print(key, np.sum(ts))\n",
    "    ts_pseudo_dict[key] = ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = 5 #3 * n_reps\n",
    "\n",
    "print('Starting pool with {} cpus'.format(cpus))\n",
    "\n",
    "# define method to compute PDF for these pseudo trials\n",
    "def get_pdf_from_mc_pseudo_data(key):\n",
    "    template_str = key.split('_')[0]\n",
    "    pseudo_data = pseudo_trials[key][0][0][0]\n",
    "\n",
    "    return get_pdf(\n",
    "        template_str=template_str, \n",
    "        dec=pseudo_data.dec, \n",
    "        energy=pseudo_data.energy, \n",
    "        sigma=pseudo_data.sigma, \n",
    "        ts=ts_pseudo_dict[key],\n",
    "        ts_weighting_method=ts_weighting_method,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "arg_list = list(ts_pseudo_dict.keys())\n",
    "if cpus == 1:\n",
    "    result = [get_pdf_from_mc_pseudo_data(k) for k in arg_list]\n",
    "else:\n",
    "    with Pool(cpus) as p:\n",
    "        start_t = timeit.default_timer()\n",
    "        result = p.map(get_pdf_from_mc_pseudo_data, arg_list)\n",
    "        end_t = timeit.default_timer()\n",
    "        print(end_t - start_t)\n",
    "\n",
    "h_total_dict_pseudo = {}\n",
    "hist_list_dict_pseudo = {}\n",
    "sig_energy_weights_dict_pseudo = {}\n",
    "for i, key in enumerate(arg_list):\n",
    "    h_total_dict_pseudo[key] = result[i][0]\n",
    "    hist_list_dict_pseudo[key] = result[i][1]\n",
    "    sig_energy_weights_dict_pseudo[key] = result[i][2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-jones",
   "metadata": {},
   "source": [
    "##### Compute Quantiles for MC TS (without caveats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['pi0', 'kra5', 'kra50']:\n",
    "    print('Model {} [MC ts-equivalent]:'.format(key))\n",
    "    for cl in [.68, 0.9]:\n",
    "        q_tail = (1. - cl) / 2.\n",
    "        \n",
    "        averages = []\n",
    "        lower_bounds = []\n",
    "        upper_bounds = []\n",
    "        for k in sig_energy_weights_dict_pseudo.keys():\n",
    "            if key == k.split('_')[0]:\n",
    "                averages.append(np.average(a.sig.true_energy, weights=sig_energy_weights_dict_pseudo[k]))\n",
    "                lower_bounds.append(weighted_quantile(a.sig.true_energy, weights=sig_energy_weights_dict_pseudo[k], quantile=q_tail))\n",
    "                upper_bounds.append(weighted_quantile(a.sig.true_energy, weights=sig_energy_weights_dict_pseudo[k], quantile=1-q_tail))\n",
    "        print('  {:3.0f} +- {:3.0f} GeV | {:3.1f}% CL: [{:3.0f} +- {:3.0f}, {:3.0f} +- {:3.0f}]'.format(\n",
    "            np.median(averages), np.std(averages, ddof=1), cl*100.,\n",
    "            np.median(lower_bounds), np.std(lower_bounds, ddof=1),\n",
    "            np.median(upper_bounds), np.std(upper_bounds, ddof=1),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-congo",
   "metadata": {},
   "source": [
    "##### Formatting for Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 1000.\n",
    "for key in ['pi0', 'kra5', 'kra50']:\n",
    "    print('Model {} [MC ts-equivalent]:'.format(key))\n",
    "    msg = '  '\n",
    "    for q in [0.05, 0.16, 0.5, 0.84, 0.95]:\n",
    "        \n",
    "        values = []\n",
    "        for k in sig_energy_weights_dict_pseudo.keys():\n",
    "            if key == k.split('_')[0]:\n",
    "                values.append(weighted_quantile(a.sig.true_energy, weights=sig_energy_weights_dict_pseudo[k], quantile=q))\n",
    "        \n",
    "        values = np.array(values) / unit\n",
    "        \n",
    "        std = np.std(values, ddof=1)\n",
    "        if std > 0.1:\n",
    "            msg += '${:3.1f} \\pm {:3.1f}$ & '.format(np.median(values), std)\n",
    "        elif std > 0.01:\n",
    "            msg += '${:3.2f} \\pm {:3.2f}$ & '.format(np.median(values), std)\n",
    "        else:\n",
    "            msg += '${:3.3f} \\pm {:3.3f}$ & '.format(np.median(values), std)\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sig_energy_weights_dict_pseudo.keys():\n",
    "    print('Model {} [MC ts-equivalent]:'.format(key))\n",
    "    for cl in [.68, 0.9]:\n",
    "        q_tail = (1. - cl) / 2.\n",
    "        print('  {:3.0f} GeV | {:3.1f}% CL: [{:3.0f}, {:3.0f}]'.format(\n",
    "            np.average(a.sig.true_energy, weights=sig_energy_weights_dict_pseudo[key]), cl*100.,\n",
    "            weighted_quantile(a.sig.true_energy, weights=sig_energy_weights_dict_pseudo[key], quantile=q_tail),\n",
    "            weighted_quantile(a.sig.true_energy, weights=sig_energy_weights_dict_pseudo[key], quantile=1-q_tail),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-serial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.3_py3-v4.1.0_csky",
   "language": "python",
   "name": "tensorflow2.3_py3-v4.1.0_csky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
