{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "# set env flags to catch BLAS used for scipy/numpy \n",
    "# to only use 1 cpu, n_cpus will be totally controlled by csky\n",
    "if False:\n",
    "    os.environ['MKL_NUM_THREADS'] = \"1\"\n",
    "    os.environ['NUMEXPR_NUM_THREADS'] = \"1\"\n",
    "    os.environ['OMP_NUM_THREADS'] = \"1\"\n",
    "    os.environ['OPENBLAS_NUM_THREADS'] = \"1\"\n",
    "    os.environ['VECLIB_MAXIMUM_THREADS'] = \"1\"\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'w'\n",
    "mpl.rcParams['savefig.facecolor'] = 'w'\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, cm\n",
    "import csky as cy\n",
    "from csky import cext\n",
    "import numpy as np\n",
    "import astropy\n",
    "#from icecube import astro\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import histlite as hl\n",
    "import healpy\n",
    "import healpy as hp\n",
    "import socket\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import copy\n",
    "healpy.disable_warnings()\n",
    "plt.rc('figure', facecolor = 'w')\n",
    "plt.rc('figure', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-nevada",
   "metadata": {},
   "source": [
    "## Define Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_version = 'version-001-p00'\n",
    "\n",
    "host_name = socket.gethostname()\n",
    "\n",
    "if 'cobalt' in host_name:\n",
    "    print('Working on Cobalts')\n",
    "    #data_prefix = '/data/user/ssclafani/data/cscd/final'\n",
    "    #ana_dir = '/data/user/ssclafani/data/analyses/'\n",
    "    plot_dir = cy.utils.ensure_dir('/data/user/mhuennefeld/data/analyses/DNNCascadeCodeReview/unblinding_checks/plots/unblinding/confidence_intervals_stacking_catalogs_2d')\n",
    "    \n",
    "else:\n",
    "    raise ValueError('Unknown host:', host_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_path in [plot_dir]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        print('Creating directory:', dir_path)\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-mystery",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = cy.selections.Repository()\n",
    "specs = cy.selections.DNNCascadeDataSpecs.DNNC_10yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ana = cy.get_analysis(\n",
    "    repo, selection_version, specs, \n",
    "    #gammas=np.r_[0.1:6.01:0.125],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ana.anas[0]\n",
    "a.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.bg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-decade",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycle\n",
    "from copy import deepcopy\n",
    "\n",
    "soft_colors = cy.plotting.soft_colors\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\n",
    "def get_bias_allt(tr, n_trials=200, n_sigs=np.r_[:101:10], quiet=False):\n",
    "    trials = [\n",
    "        (None if quiet else print(f'\\r{n_sig:4d} ...', end='', flush=True))\n",
    "        or\n",
    "        tr.get_many_fits(n_trials, n_sig=n_sig, logging=False, seed=n_sig)\n",
    "        for n_sig in n_sigs]\n",
    "    if not quiet:\n",
    "        print()\n",
    "    for (n_sig, t) in zip(n_sigs, trials):\n",
    "        t['ntrue'] = np.repeat(n_sig, len(t))\n",
    "    allt = cy.utils.Arrays.concatenate(trials)\n",
    "    return allt\n",
    "\n",
    "def get_color_cycler():\n",
    "    return cycle(colors)\n",
    "\n",
    "def plot_bias(ax, x_fit, y_true, label=''):\n",
    "    \n",
    "    y_unique = np.unique(y_true)\n",
    "    dy = np.mean(np.diff(y_unique))\n",
    "    y_bins = np.r_[y_unique - 0.5*dy, y_unique[-1] + 0.5*dy]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "\n",
    "    h = hl.hist((y_true, x_fit), bins=(y_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1), errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = y_bins[[0, -1]]\n",
    "    ax.set_xlim(ax.set_ylim(lim))\n",
    "    ax.plot(lim, lim, **expect_kw)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ax.grid()\n",
    "    return h\n",
    "    \n",
    "def plot_ns_bias(ax, tr, allt, label=''):\n",
    "\n",
    "    n_sigs = np.unique(allt.ntrue)\n",
    "    dns = np.mean(np.diff(n_sigs))\n",
    "    ns_bins = np.r_[n_sigs - 0.5*dns, n_sigs[-1] + 0.5*dns]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "\n",
    "    h = hl.hist((allt.ntrue, allt.ns), bins=(ns_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1),errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = ns_bins[[0, -1]]\n",
    "    ax.set_xlim(ax.set_ylim(lim))\n",
    "    ax.plot(lim, lim, **expect_kw)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ax.set_xlabel(r'$n_{inj}$')\n",
    "    ax.set_ylabel(r'$n_s$')\n",
    "    ax.grid()\n",
    "\n",
    "def plot_gamma_bias(ax, tr, allt, label=''):\n",
    "\n",
    "    n_sigs = np.unique(allt.ntrue)\n",
    "    dns = np.mean(np.diff(n_sigs))\n",
    "    ns_bins = np.r_[n_sigs - 0.5*dns, n_sigs[-1] + 0.5*dns]\n",
    "    expect_kw = dict(color='C0', ls='--', lw=1, zorder=-10)\n",
    "    expect_gamma = tr.sig_injs[0].flux[0].gamma\n",
    "\n",
    "    h = hl.hist((allt.ntrue, allt.gamma), bins=(ns_bins, 100))\n",
    "    hl.plot1d(ax, h.contain_project(1),errorbands=True, \n",
    "              drawstyle='default', label=label)\n",
    "    lim = ns_bins[[0, -1]]\n",
    "    ax.set_xlim(lim)\n",
    "    ax.set_ylim(1, 4)\n",
    "    ax.axhline(expect_gamma, **expect_kw)\n",
    "\n",
    "    ax.set_xlabel(r'$n_{inj}$')\n",
    "    ax.set_ylabel(r'$\\gamma$')\n",
    "    ax.grid()\n",
    "\n",
    "def plot_bkg_trials(\n",
    "            bg, fig=None, ax=None, \n",
    "            label='{} bg trials', \n",
    "            label_fit=r'$\\chi^2[{:.2f}\\mathrm{{dof}},\\ \\eta={:.3f}]$', \n",
    "            color=colors[0],\n",
    "            density=False,\n",
    "            bins=50,\n",
    "        ):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    if density:\n",
    "        h = bg.get_hist(bins=bins).normalize()\n",
    "    else:\n",
    "        h = bg.get_hist(bins=bins)\n",
    "    if label is not None:\n",
    "        label = label.format(bg.n_total)\n",
    "    hl.plot1d(ax, h, crosses=True, color=color, label=label)\n",
    "\n",
    "    # compare with the chi2 fit:\n",
    "    if hasattr(bg, 'pdf'):\n",
    "        x = h.centers[0]\n",
    "        norm = h.integrate().values\n",
    "        if label_fit is not None:\n",
    "            label_fit = label_fit.format(bg.ndof, bg.eta)\n",
    "        if density:\n",
    "            ax.semilogy(x, bg.pdf(x), lw=1, ls='--', label=label_fit, color=color)\n",
    "        else:\n",
    "            ax.semilogy(x, norm * bg.pdf(x), lw=1, ls='--', label=label_fit, color=color)\n",
    "\n",
    "    ax.set_xlabel(r'TS')\n",
    "    if density:\n",
    "        ax.set_ylabel(r'Density')\n",
    "    else:\n",
    "        ax.set_ylabel(r'number of trials')\n",
    "    ax.legend()\n",
    "        \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-bridges",
   "metadata": {},
   "source": [
    "## Setup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import config as cg\n",
    "\n",
    "cg.base_dir = '/data/user/mhuennefeld/data/analyses/unblinding_v1.0.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_catalog_tr(ana, catalog, gamma, cutoff=np.inf, cpus=20):\n",
    "    catalog_file = os.path.join(\n",
    "        cg.catalog_dir, '{}_ESTES_12.pickle'.format(catalog))\n",
    "    cat = np.load(catalog_file, allow_pickle=True)\n",
    "    src = cy.utils.Sources(dec=cat['dec_deg'], ra=cat['ra_deg'], deg=True)\n",
    "    cutoff_GeV = cutoff * 1e3\n",
    "\n",
    "    conf = cg.get_ps_conf(src=src, gamma=gamma, cutoff_GeV=cutoff_GeV)\n",
    "    tr = cy.get_trial_runner(ana=ana, conf=conf, mp_cpus=cpus)\n",
    "    return tr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-converter",
   "metadata": {},
   "source": [
    "#### SnowStorm Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_dir = '/data/ana/PointSource/DNNCascade/analysis/{}/'.format(selection_version)\n",
    "df = pd.read_hdf(\n",
    "    df_dir + '/systematics/SnowStorm_Spice321/MC_NuGen_snowstorm_214xx.hdf', key='df',\n",
    ")\n",
    "df = df[['SnowstormParameters_{:05d}'.format(i) for i in range(6)] + ['run', 'energy', 'ow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from IPython.utils import io\n",
    "\n",
    "sim_ranges = {\n",
    "    'Scattering': [0.9, 1.1],\n",
    "    'Absorption': [0.9, 1.1],\n",
    "    'AnisotropyScale': [0., 2.],\n",
    "    'DOMEfficiency': [0.9, 1.1],\n",
    "    'HoleIceForward_Unified_00': [-1.0, 1.0],\n",
    "    'HoleIceForward_Unified_01': [-0.2, 0.2],\n",
    "}\n",
    "\n",
    "allowed_ranges = {\n",
    "    'Scattering': [0.9, 1.1],\n",
    "    'Absorption': [0.9, 1.1],\n",
    "    'AnisotropyScale': [0., 2.],\n",
    "    'DOMEfficiency': [0.9, 1.1],\n",
    "    \n",
    "    # slightly increase range from recommendation to not have too little stats\n",
    "    'HoleIceForward_Unified_00': [-0.75, 0.45], #[-0.5, 0.3],\n",
    "    'HoleIceForward_Unified_01': [-0.15, 0.075], #[-0.1, 0.05],\n",
    "}\n",
    "\n",
    "\n",
    "def get_snowstorm_ana(sys_ranges, sim_ranges=sim_ranges):\n",
    "    \n",
    "    # define SnowStorm dataset with reduced range\n",
    "    class DNNCascade_10yr_sys_reduced(cy.selections.DNNCascadeDataSpecs.DNNCascade_10yr_snowstorm_fullrange):\n",
    "        def dataset_modifications(self, ds):\n",
    "            print('Adding SnowStorm Parameters to MC')\n",
    "            path_sig_df = (\n",
    "                '/data/ana/PointSource/DNNCascade/analysis/' + \n",
    "                self._path_sig.format(version=self._version).replace('dnn_cascades/', '').replace('.npy', '.hdf')\n",
    "            )\n",
    "            # (use global df to avoid loading multiple times)\n",
    "            #if df is None:\n",
    "            #    df = pd.read_hdf(path_sig_df, key='df')\n",
    "            assert np.allclose(df['run'], ds.sig.run)\n",
    "            assert np.allclose(df['energy'], ds.sig.energy)\n",
    "            assert np.allclose(df['ow'], ds.sig.oneweight)\n",
    "\n",
    "            # load and rename SnowStorm parameters\n",
    "            parameter_names=[\n",
    "                'Scattering', 'Absorption', 'AnisotropyScale', \n",
    "                'DOMEfficiency', 'HoleIceForward_Unified_00', \n",
    "                'HoleIceForward_Unified_01',\n",
    "            ]\n",
    "            for i, param in enumerate(parameter_names):\n",
    "                ds.sig[param] = np.array(df['SnowstormParameters_{:05d}'.format(i)])\n",
    "            \n",
    "            print('Reducing Dataset')\n",
    "            mask = np.ones(len(ds.sig), dtype=bool)\n",
    "            factor = 1.\n",
    "            for param, sys_range in sys_ranges.items():\n",
    "                if sys_range != sim_ranges[param]:\n",
    "                    assert sys_range[0] < sys_range[1], sys_range\n",
    "                    factor *= (sys_range[1] - sys_range[0]) / (sim_ranges[param][1] - sim_ranges[param][0])\n",
    "                    mask_i = np.logical_and(\n",
    "                        ds.sig[param] >= sys_range[0],\n",
    "                        ds.sig[param] < sys_range[1],\n",
    "                    )\n",
    "                    mask = np.logical_and(mask, mask_i)\n",
    "            \n",
    "            print('Reduction factor: {:3.3f}'.format(factor))\n",
    "            ds.sig = ds.sig._subsample(mask)\n",
    "            ds.sig.oneweight[:] = ds.sig.oneweight/factor\n",
    "            \n",
    "    ana_sys = cy.get_analysis(\n",
    "        cy.selections.Repository(), selection_version, [DNNCascade_10yr_sys_reduced], \n",
    "        #_quiet=True,\n",
    "    )\n",
    "    return ana_sys\n",
    "\n",
    "def sample_snowstorm_ranges(\n",
    "            seed=None, \n",
    "            sim_ranges=sim_ranges, \n",
    "            allowed_ranges=allowed_ranges, \n",
    "            min_red_factor=0.05,\n",
    "            max_k=3,\n",
    "        ):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # sample number of parameters to perturb\n",
    "    k = rng.randint(1, 1 + max_k)\n",
    "    \n",
    "    # sample which parameters to perturb\n",
    "    parameter_names=[\n",
    "        'Scattering', 'Absorption', 'AnisotropyScale', \n",
    "        'DOMEfficiency', 'HoleIceForward_Unified_00', \n",
    "        'HoleIceForward_Unified_01',\n",
    "    ]\n",
    "    params = rng.choice(parameter_names, size=k, replace=False)\n",
    "    \n",
    "    # compute reduction fraction from allowed range\n",
    "    fractions = []\n",
    "    allowed_fraction = 1.\n",
    "    for param, allowed_range in allowed_ranges.items():\n",
    "        if allowed_range != sim_ranges[param]:\n",
    "            fraction_i = (allowed_range[1] - allowed_range[0]) / (sim_ranges[param][1] - sim_ranges[param][0])\n",
    "        else:\n",
    "            fraction_i = 1.\n",
    "        allowed_fraction *= fraction_i\n",
    "        if param in params:\n",
    "            fractions.append(fraction_i)\n",
    "            \n",
    "    # define relative reduction fraction of allowed range\n",
    "    rel_fr = np.power(min_red_factor / allowed_fraction , 1./k)\n",
    "    \n",
    "    # sample intervals\n",
    "    sys_range = deepcopy(allowed_ranges)\n",
    "\n",
    "    current_factor = 1.\n",
    "    for param, fraction_i in zip(params, fractions):\n",
    "        allowed_range = allowed_ranges[param]\n",
    "        \n",
    "        interval_width = (allowed_range[1] - allowed_range[0]) * rel_fr / 2.\n",
    "        sample_range = [allowed_range[0] + interval_width, allowed_range[1] - interval_width]\n",
    "        \n",
    "        assert sample_range[1] > sample_range[0], sample_range\n",
    "        \n",
    "        mid_point = rng.uniform(*sample_range)\n",
    "        sys_range[str(param)] = [mid_point - interval_width, mid_point + interval_width]\n",
    "    \n",
    "    return sys_range, params\n",
    "\n",
    "def get_snowstorm_tr(\n",
    "            catalog,\n",
    "            gamma,\n",
    "            seed=None, \n",
    "            sim_ranges=sim_ranges, \n",
    "            allowed_ranges=allowed_ranges, \n",
    "            min_red_factor=0.05,\n",
    "            max_k=3,\n",
    "        ):\n",
    "    \n",
    "    # sample SnowStorm parameters\n",
    "    sys_ranges, params = sample_snowstorm_ranges(seed=seed, min_red_factor=min_red_factor, max_k=max_k)\n",
    "    \n",
    "    # get snowstorm ana object\n",
    "    with io.capture_output() as captured:\n",
    "        ana_sys = get_snowstorm_ana(sys_ranges=sys_ranges)\n",
    "\n",
    "        # get trial runner\n",
    "        tr_sys = get_catalog_tr(ana=ana_sys, catalog=catalog, gamma=gamma)\n",
    "    \n",
    "    return tr_sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-secret",
   "metadata": {},
   "source": [
    "##### Test Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "min_red_factor = 0.02\n",
    "max_k = 3\n",
    "\n",
    "mids = {k: [] for k in sim_ranges.keys()}\n",
    "mids_all = {k: [] for k in sim_ranges.keys()}\n",
    "for i in tqdm(range(n_samples), total=n_samples):\n",
    "    sys_ranges, params = sample_snowstorm_ranges(min_red_factor=min_red_factor, max_k=max_k)\n",
    "    for k, sys_range in sys_ranges.items():\n",
    "        if k in params:\n",
    "            mids[k].append(np.mean(sys_range))\n",
    "        mids_all[k].append(np.mean(sys_range))\n",
    "    \n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    key = sorted(sim_ranges.keys())[i]\n",
    "    ax.set_xlabel(key)\n",
    "    ax.set_ylabel('Number of samples')\n",
    "    ax.hist(mids[key], bins=30)\n",
    "    ax.axvline(sim_ranges[key][0], color='0.3', ls='--', label='Simulation Range')\n",
    "    ax.axvline(sim_ranges[key][1], color='0.3', ls='--')\n",
    "    ax.axvline(allowed_ranges[key][0], color='0.7', ls='-', label='Allowed Range')\n",
    "    ax.axvline(allowed_ranges[key][1], color='0.7', ls='-')\n",
    "axes[0, 0].legend()\n",
    "fig.suptitle('Min Reduction: {:1.3f} | Max k: {}'.format(min_red_factor, max_k))\n",
    "fig.tight_layout()\n",
    "fig.savefig('{}/snowstorm_sampling_check.png'.format(plot_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_ranges, params = sample_snowstorm_ranges(min_red_factor=0.02)\n",
    "sys_ranges, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ana_sys = get_snowstorm_ana(\n",
    "    #sys_ranges={\n",
    "    #    'Scattering': [1.0, 1.1],\n",
    "    #    'Absorption': [0.9, 1.0],\n",
    "    #    'AnisotropyScale': [0., 1.],\n",
    "    #},\n",
    "    sys_ranges=sys_ranges,\n",
    "    #sys_ranges=sample_snowstorm_ranges(),\n",
    ")\n",
    "tr_sys = get_catalog_tr(\n",
    "    ana=ana_sys, catalog='snr', gamma=2.0,\n",
    ")\n",
    "print(len(ana_sys.anas[0].sig)/len(df), len(ana_sys.anas[0].sig))\n",
    "print('ana', np.sum(a.sig.oneweight * a.sig.true_energy**-2.5))\n",
    "print('sys', np.sum(ana_sys.anas[0].sig.oneweight * ana_sys.anas[0].sig.true_energy**-2.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ana_sys.anas[0].sig) / len(ana.anas[0].sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-matter",
   "metadata": {},
   "source": [
    "#### Get Results for each template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for key in ['snr', 'pwn', 'unid']:\n",
    "    f_path = os.path.join(\n",
    "        cg.base_dir, \n",
    "        'stacking/results/{}/{}_unblinded.npy'.format(key, key), \n",
    "    )\n",
    "    res_dict[key] = np.load(f_path)\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, res in res_dict.items():\n",
    "    ts, ns, gamma, pval, nsigma = res\n",
    "    tr = get_catalog_tr(ana=ana, catalog=key, gamma=gamma)\n",
    "    print('{} | ns: {:3.1f} | gamma: {:3.2f} | flux: {}'.format(\n",
    "        key, ns, gamma, tr.to_E2dNdE(ns, E0=100, unit=1e3)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-citation",
   "metadata": {},
   "source": [
    "#### Spot-check ns/gamma bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "recalculate = False\n",
    "allt_dict_file = '{}/allt_dict_file.pkl'.format(plot_dir)\n",
    "\n",
    "if os.path.exists(allt_dict_file) and not recalculate:\n",
    "    with open(allt_dict_file, 'rb') as handle:\n",
    "        allt_dict = pickle.load(handle)\n",
    "else:\n",
    "    allt_dict = {}\n",
    "    \n",
    "for catalog in ['snr']:\n",
    "#for catalog in ['snr', 'pwn', 'unid']:\n",
    "    for gamma in [2.0, 2.5, 3.0]:\n",
    "        if (catalog, gamma) not in allt_dict:\n",
    "            print('Computing for {} and gamma: {}'.format(catalog, gamma))\n",
    "\n",
    "            # get trial runner (for baseline MC)\n",
    "            tr = get_catalog_tr(ana=ana, catalog=catalog, gamma=gamma)\n",
    "\n",
    "            # compute bias\n",
    "            allt_dict[(catalog, gamma)] = get_bias_allt(tr, n_trials=200, n_sigs=np.r_[:301:10])       \n",
    "\n",
    "            # save file\n",
    "            with open(allt_dict_file, 'wb') as f:\n",
    "                pickle.dump(allt_dict, f, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-lucas",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (catalog, gamma), allt in allt_dict.items():\n",
    "    # plot bias\n",
    "    tr = get_catalog_tr(ana=ana, catalog=catalog, gamma=gamma)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 4))\n",
    "    plot_ns_bias(ax=axes[0], tr=tr, allt=allt)\n",
    "    plot_gamma_bias(ax=axes[1], tr=tr, allt=allt)\n",
    "    axes[0].set_title('{} | $\\gamma$= {:3.2f}'.format(catalog, gamma))\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('{}/bias_check_{}_{:3.2f}.png'.format(plot_dir, catalog, gamma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-clothing",
   "metadata": {},
   "source": [
    "#### Get ns and gamma-bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_poisson = False # GP paper uses: False\n",
    "add_sys = False  # GP paper uses: False\n",
    "\n",
    "if add_sys:\n",
    "    sys_suffix = '_sys'\n",
    "else:\n",
    "    sys_suffix = ''\n",
    "    \n",
    "if use_poisson:\n",
    "    bias_file = os.path.join(plot_dir, 'bias_poisson{}.pkl'.format(sys_suffix))\n",
    "else:\n",
    "    bias_file = os.path.join(plot_dir, 'bias{}.pkl'.format(sys_suffix))\n",
    "\n",
    "\n",
    "def get_bias_dict_from_sys(bias_dict_sys):\n",
    "    bias_dict = {}\n",
    "    for key, gamma_ns_trials_dict in bias_dict_sys.items():\n",
    "        bias_dict[key] = {}\n",
    "        \n",
    "        for gamma, ns_trials_dict in gamma_ns_trials_dict.items():\n",
    "\n",
    "            # get ns_values\n",
    "            ns_values = [sorted(ns_trials_dict[trial_i].keys()) for trial_i in ns_trials_dict.keys()]\n",
    "            for ns_values_i in ns_values:\n",
    "                assert np.allclose(ns_values_i, ns_values[0])\n",
    "            ns_values = ns_values[0]\n",
    "\n",
    "            trials_dict = {ns: [] for ns in ns_values}\n",
    "            for trial_i in sorted(ns_trials_dict.keys()):\n",
    "                for ns in ns_values:\n",
    "                    trials_dict[ns].append(ns_trials_dict[trial_i][ns])\n",
    "\n",
    "            bias_dict[key][gamma] = {\n",
    "                ns: cy.utils.Arrays.concatenate(trials_dict[ns]) for ns in ns_values\n",
    "            }\n",
    "    return bias_dict\n",
    "\n",
    "if os.path.exists(bias_file):\n",
    "    print('Loading from file')\n",
    "    if add_sys:\n",
    "        with open(bias_file, 'rb') as handle:\n",
    "            bias_dict_sys = pickle.load(handle)\n",
    "        \n",
    "        # restructure dict\n",
    "        bias_dict = get_bias_dict_from_sys(bias_dict_sys)\n",
    "    else:\n",
    "        with open(bias_file, 'rb') as handle:\n",
    "            bias_dict = pickle.load(handle)\n",
    "        \n",
    "        bias_dict_sys = {}\n",
    "else:\n",
    "    print('Creating new dict')\n",
    "    bias_dict = {}\n",
    "    bias_dict_sys = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-person",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "ns_bias_range = {\n",
    "    'snr': [0, 2000, 30],  # GP paper uses: [0, 2000, 30]\n",
    "    'unid': [0, 2000, 30],  # GP paper uses: [0, 2000, 30]\n",
    "    'pwn': [0, 2000, 30],  # GP paper uses: [0, 2000, 30]\n",
    "}\n",
    "gammas = np.r_[1:4.01:0.25]  # GP paper uses: np.r_[1:4.01:0.25]\n",
    "\n",
    "n_trials = 100 # GP paper uses: 100\n",
    "recalculate = False\n",
    "cpus = 25\n",
    "\n",
    "for key, ns_range in ns_bias_range.items():\n",
    "    \n",
    "    for dictionary in [bias_dict, bias_dict_sys]:\n",
    "        if key not in dictionary:\n",
    "            dictionary[key] = {gamma:{} for gamma in gammas}\n",
    "        \n",
    "    print('Submitting {} with {} gammas for ns: {}:{}:{}'.format(key, len(gammas), *ns_range))\n",
    "    \n",
    "    for gamma in gammas:\n",
    "        print('  ... at gamma: {:3.2f}'.format(gamma))\n",
    "        \n",
    "        if add_sys:\n",
    "            for trial_i in tqdm(range(n_trials), total=n_trials):\n",
    "\n",
    "                if trial_i not in bias_dict_sys[key][gamma] or recalculate:\n",
    "                    bias_dict_sys[key][gamma][trial_i] = {}\n",
    "\n",
    "                    print('Getting trial runner for {}'.format(key))\n",
    "                    tr = get_snowstorm_tr(\n",
    "                        catalog=key, gamma=gamma, seed=trial_i, \n",
    "                    )\n",
    "\n",
    "                    print('Starting pool with {} cpus'.format(cpus))\n",
    "                    def compute_trial_i(ns):\n",
    "                        trials = tr.get_many_fits(1, n_sig=ns, logging=False, seed=ns, poisson=use_poisson, cpus=1)\n",
    "                        trials['ntrue'] = np.repeat(ns, len(trials))\n",
    "                        return trials\n",
    "\n",
    "                    arg_list = list(range(*ns_range))\n",
    "                    with Pool(cpus) as p:\n",
    "                        trials = list(tqdm(p.imap(compute_trial_i, arg_list), total=len(arg_list)))\n",
    "\n",
    "                    for j, ns in enumerate(range(*ns_range)):\n",
    "                        bias_dict_sys[key][gamma][trial_i][ns] = trials[j]\n",
    "\n",
    "                    with open(bias_file, 'wb') as f:\n",
    "                        pickle.dump(bias_dict_sys, f, protocol=-1)\n",
    "\n",
    "        else:\n",
    "            tr = get_catalog_tr(ana=ana, catalog=key, gamma=gamma)\n",
    "\n",
    "            for ns in tqdm(range(*ns_range), total=len(range(*ns_range))):\n",
    "\n",
    "                if ns not in bias_dict[key][gamma] or recalculate:\n",
    "                    trials = tr.get_many_fits(n_trials, n_sig=ns, logging=False, seed=ns, cpus=cpus, poisson=use_poisson)\n",
    "                    trials['ntrue'] = np.repeat(ns, len(trials))\n",
    "\n",
    "                    bias_dict[key][gamma][ns] = trials\n",
    "\n",
    "                    with open(bias_file, 'wb') as f:\n",
    "                        pickle.dump(bias_dict, f, protocol=-1)\n",
    "\n",
    "if add_sys:\n",
    "    bias_dict = get_bias_dict_from_sys(bias_dict_sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-transsexual",
   "metadata": {},
   "source": [
    "#### Compute ns and gamma-bias correction\n",
    "\n",
    "Goal is to obtain a bias-correction function of the form:\n",
    "    $f(n_\\mathrm{inj}, \\gamma_\\mathrm{inj}) \\rightarrow (\\overline{n}_\\mathrm{fit}, \\overline{\\gamma}_\\mathrm{fit})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import SmoothBivariateSpline\n",
    "from copy import deepcopy\n",
    "\n",
    "bias_corr_funcs_non_pickable = {}\n",
    "\n",
    "smoothing_settings = {\n",
    "    'unid': {\n",
    "        'gamma': dict(s=0.87), # GP paper uses: dict(s=0.87)\n",
    "        'ns': dict(s=174200), # GP paper uses: dict(s=174200)\n",
    "    },\n",
    "    'snr': {\n",
    "        'gamma': dict(s=1.5, kx=3, ky=1), # GP paper uses: dict(s=1.5, kx=3, ky=1)\n",
    "        'ns': dict(s=174200), # GP paper uses: dict(s=174200)\n",
    "    },\n",
    "    'pwn': {\n",
    "        'gamma': dict(s=1.5, kx=3, ky=1), # GP paper uses: dict(s=1.5, kx=3, ky=1)\n",
    "        'ns': dict(s=174200), # GP paper uses: dict(s=174200)\n",
    "    },\n",
    "}\n",
    "\n",
    "for key, bias_dict_i in bias_dict.items():\n",
    "    #if key != 'pwn': continue\n",
    "    \n",
    "    # create bias correction function\n",
    "    inj_n = []\n",
    "    inj_gamma = []\n",
    "    fit_gamma = []\n",
    "    fit_ns = []\n",
    "    \n",
    "    for gamma, ns_dict_i in bias_dict_i.items():\n",
    "        for n_inj in sorted(ns_dict_i.keys()):\n",
    "            inj_n.append(n_inj)\n",
    "            inj_gamma.append(gamma)\n",
    "            fit_gamma.append(np.median(ns_dict_i[n_inj].gamma))\n",
    "            fit_ns.append(np.median(ns_dict_i[n_inj].ns))\n",
    "    \n",
    "    spline_gamma = SmoothBivariateSpline(\n",
    "        x=inj_n, y=inj_gamma, z=fit_gamma, **smoothing_settings[key]['gamma'])\n",
    "    spline_ns = SmoothBivariateSpline(\n",
    "        x=inj_n, y=inj_gamma, z=fit_ns, **smoothing_settings[key]['ns'])\n",
    "\n",
    "    def bias_corr(n_inj, gamma_inj):\n",
    "        corr_gamma = spline_gamma.ev(n_inj, gamma_inj)\n",
    "        corr_gamma = np.clip(corr_gamma, 1., 4.)\n",
    "        corr_ns = spline_ns.ev(n_inj, gamma_inj)\n",
    "        corr_ns = np.clip(corr_ns, 0., np.inf)\n",
    "        \n",
    "        return np.stack([corr_ns, corr_gamma], axis=0)\n",
    "    \n",
    "    bias_corr_funcs_non_pickable[key] = bias_corr\n",
    "    \n",
    "    # -------------------------------------\n",
    "    # make 2D plot to check smoothing value\n",
    "    # -------------------------------------\n",
    "    cmap = plt.cm.viridis\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "        'Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "    # define the bins and normalize\n",
    "    bounds_gamma = np.linspace(1, 4, 17)\n",
    "    norm_gamma = mpl.colors.BoundaryNorm(bounds_gamma, cmap.N)\n",
    "    \n",
    "    max_ns = max(max(fit_ns), max(inj_n))\n",
    "    bounds_ns = np.linspace(0, max_ns, 17)\n",
    "    norm_ns = mpl.colors.BoundaryNorm(bounds_ns, cmap.N)\n",
    "\n",
    "    x_bins = np.arange(0, max_ns, 1)\n",
    "    y_bins = np.arange(1, 4, 0.01)\n",
    "    test_x = x_bins[:-1] + 0.5*np.diff(x_bins)\n",
    "    test_y = y_bins[:-1] + 0.5*np.diff(y_bins)\n",
    "    grid_x, grid_y = np.meshgrid(x_bins, y_bins)\n",
    "\n",
    "    gamma_result = spline_gamma(test_x, test_y)\n",
    "    ns_result = spline_ns(test_x, test_y)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9, 6))\n",
    "    pc_gamma = axes[0].pcolormesh(grid_x, grid_y, gamma_result.T, cmap=cmap, norm=norm_gamma)\n",
    "    cb_gamma = fig.colorbar(pc_gamma, ax=axes[0])\n",
    "    cb_gamma.set_label('$\\gamma$')\n",
    "    pc_ns = axes[1].pcolormesh(grid_x, grid_y, ns_result.T, cmap=cmap, norm=norm_ns)\n",
    "    cb_ns = fig.colorbar(pc_ns, ax=axes[1], orientation='horizontal', ticks=np.arange(0, max_ns, 500))\n",
    "    cb_ns.set_label('$n_s$')\n",
    "    \n",
    "    axes[0].set_title('Model: {} | $\\gamma$-Interpolation'.format(key))\n",
    "    axes[1].set_title('$n_s$-Interpolation'.format(key))\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('$n_\\mathrm{inj}$')\n",
    "        ax.set_ylabel('$\\gamma_\\mathrm{inj}$')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('{}/biascorrection_2d_{}{}.png'.format(plot_dir, key, sys_suffix))\n",
    "    \n",
    "    # ---------------\n",
    "    # 1D slices in ns\n",
    "    # ---------------\n",
    "    if True:            \n",
    "        ns_values = sorted(np.unique(inj_n))\n",
    "        n_axes = len(ns_values)\n",
    "        n_1d = int(np.ceil(np.sqrt(n_axes)))\n",
    "        fig, axes = plt.subplots(n_1d, n_1d, figsize=(3*n_1d, 3*n_1d))\n",
    "        axes = axes.flatten()\n",
    "        for ax, ns in zip(axes, ns_values):\n",
    "            trials = []\n",
    "            for gamma, ns_dict_i in bias_dict_i.items():\n",
    "                trials_i = cy.utils.Arrays(deepcopy(ns_dict_i[ns]))\n",
    "                trials_i['gamma_true'] = np.ones(len(trials_i)) * gamma\n",
    "                trials.append(trials_i)\n",
    "            allt = cy.utils.Arrays.concatenate(trials)\n",
    "\n",
    "            plot_bias(ax, x_fit=allt.gamma, y_true=allt.gamma_true, label='')\n",
    "\n",
    "            x_gamma = np.linspace(1, 4, 100)\n",
    "            x_ns = np.ones_like(x_gamma) * ns\n",
    "\n",
    "            ax.plot(x_gamma, bias_corr_funcs_non_pickable[key](x_ns, x_gamma)[1], label='Spline Fit')\n",
    "            ax.set_title('Model: {} | {} = {:3.2f}'.format(key, r'$n_\\mathrm{inj}$', ns))\n",
    "            ax.set_xlabel(r'$\\gamma_\\mathrm{inj}$')\n",
    "            ax.set_ylabel(r'$\\gamma_\\mathrm{fit}$')\n",
    "            ax.legend()\n",
    "            \n",
    "        fig.tight_layout()\n",
    "        fig.savefig('{}/biascorrection_1d_ns_slices_{}{}.png'.format(plot_dir, key, sys_suffix))\n",
    "    \n",
    "    \n",
    "    # ------------------\n",
    "    # 1D slices in gamma\n",
    "    # ------------------\n",
    "    if True:\n",
    "        n_axes = len(bias_dict_i.keys())\n",
    "        n_1d = int(np.ceil(np.sqrt(n_axes)))\n",
    "        fig, axes = plt.subplots(n_1d, n_1d, figsize=(3*n_1d, 3*n_1d))\n",
    "        axes = axes.flatten()\n",
    "        for ax, (gamma, ns_dict_i) in zip(axes, bias_dict_i.items()):\n",
    "            allt = cy.utils.Arrays.concatenate([t for t in ns_dict_i.values()])\n",
    "\n",
    "            tr = get_catalog_tr(ana=ana, catalog=key, gamma=gamma)\n",
    "\n",
    "            plot_ns_bias(ax=ax, tr=tr, allt=allt)\n",
    "            \n",
    "            x_ns = np.linspace(0, 2000, 100)\n",
    "            x_gamma = np.ones_like(x_ns) * gamma\n",
    "            \n",
    "            ax.plot(x_ns, bias_corr_funcs_non_pickable[key](x_ns, x_gamma)[0], label='Spline Fit')\n",
    "            ax.set_title('Model: {} | $\\gamma$ = {:3.2f}'.format(key, gamma))\n",
    "            ax.set_xlabel('$n_\\mathrm{inj}$')\n",
    "            ax.set_ylabel('$n_s$')\n",
    "            ax.legend()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        fig.savefig('{}/biascorrection_1d_gamma_slices_{}{}.png'.format(plot_dir, key, sys_suffix))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-midwest",
   "metadata": {},
   "source": [
    "#### Make top-level, pickable functions for Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_corr_funcs = {}\n",
    "\n",
    "if 'unid' in bias_corr_funcs_non_pickable:\n",
    "    \n",
    "    def bias_corr_func_unid(n_inj, gamma_inj):\n",
    "        return bias_corr_funcs_non_pickable['unid'](n_inj, gamma_inj)\n",
    "    \n",
    "    bias_corr_funcs['unid'] = bias_corr_func_unid\n",
    "\n",
    "if 'snr' in bias_corr_funcs_non_pickable:\n",
    "    \n",
    "    def bias_corr_func_snr(n_inj, gamma_inj):\n",
    "        return bias_corr_funcs_non_pickable['snr'](n_inj, gamma_inj)\n",
    "    \n",
    "    bias_corr_funcs['snr'] = bias_corr_func_snr\n",
    "\n",
    "if 'pwn' in bias_corr_funcs_non_pickable:\n",
    "    \n",
    "    def bias_corr_func_pwn(n_inj, gamma_inj):\n",
    "        return bias_corr_funcs_non_pickable['pwn'](n_inj, gamma_inj)\n",
    "    \n",
    "    bias_corr_funcs['pwn'] = bias_corr_func_pwn\n",
    "\n",
    "print('Got the following keys:', bias_corr_funcs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-butterfly",
   "metadata": {},
   "source": [
    "#### Some Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, gamma=2.84):\n",
    "    return bias_corr_funcs['unid'](x, gamma)[0]\n",
    "\n",
    "x = np.linspace(0, 3000, 100)\n",
    "for gamma in np.linspace(1, 4, 5):\n",
    "    plt.plot(x, f(x, gamma=gamma), label='$\\gamma=${:3.2f}'.format(gamma))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_corr_funcs['unid'](200, 2.80) #2.37906900e+02, 2.84580387e+00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-diana",
   "metadata": {},
   "source": [
    "#### Get Critical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "def get_critical_value_trial(\n",
    "            E2dNdE, gamma, tr, tr_inj, bias_corr_func=None,\n",
    "            E0=100, unit=1e3, seed=None, TRUTH=False, \n",
    "        ):\n",
    "    \n",
    "    # get number of ns corresponding to flux\n",
    "    n_sig = tr_inj.to_ns(E2dNdE, E0=E0, unit=unit)\n",
    "    \n",
    "    if TRUTH:\n",
    "        n_inj = 0\n",
    "    else:\n",
    "        n_inj = n_sig\n",
    "    \n",
    "    # get trial\n",
    "    trial = tr_inj.get_one_trial(n_sig=n_inj, poisson=True, seed=seed, TRUTH=TRUTH)\n",
    "    \n",
    "    # get best fit ts and ns for this trial\n",
    "    fit = tr.get_one_fit_from_trial(trial)\n",
    "    ts_fit, ns_fit, gamma_fit = fit\n",
    "    \n",
    "    # apply bias correction for tested nsig?\n",
    "    if bias_corr_func is not None:\n",
    "        n_sig, gamma = bias_corr_func(n_sig, gamma)\n",
    "        \n",
    "    # get Likelihood object\n",
    "    L = tr.get_one_llh_from_trial(trial)\n",
    "    fitter_kwargs = deepcopy(tr.fitter_args)\n",
    "    fitter_kwargs.pop('gamma')\n",
    "    ts_test = L.get_ts(ns=n_sig, gamma=gamma, **fitter_kwargs)\n",
    "    \n",
    "    # compute test-statistic tau for critical value definition\n",
    "    # tau = -2 log llh-ratio = - 2 log {L_0(ns_test) / L_1(ns=n_fit)}\n",
    "    # In this case, we want to test against ns_test = ns(E2dNdE)\n",
    "    # tau = (-2 log LR(ns=0) - (-2 log LR(ns=ns_test))\n",
    "    #     = -2 log L(ns=0) + 2 log L(ns=n_fit) + 2 log L(ns=0) - 2 log L(ns=ns_test)\n",
    "    #     = -2 log L(ns=ns_test) + 2 log L(ns=ns_fit)\n",
    "    #     = -2 log {L(ns_ns_test) / L(ns=ns_fit)}\n",
    "    tau = ts_fit - ts_test\n",
    "    \n",
    "    return tau\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# define global functions for multiprocessing (pickle has issues with local functions)\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "def compute_trial_i(args):\n",
    "    i, E2dNdEs, gamma, catalog, E0, unit, bias_corr_func, sys_seed, min_red_factor, max_k, TRUTH = args\n",
    "    if sys_seed is None:\n",
    "        tr_inj = get_catalog_tr(ana=ana, catalog=catalog, gamma=gamma)\n",
    "    else:\n",
    "        tr_inj = get_snowstorm_tr(\n",
    "            catalog=catalog, gamma=gamma, seed=sys_seed, min_red_factor=min_red_factor, max_k=max_k)\n",
    "    \n",
    "    tr = get_catalog_tr(ana=ana, catalog=catalog, gamma=gamma)\n",
    "    \n",
    "    tau_values = [] \n",
    "    for E2dNdE in E2dNdEs:\n",
    "        tau_values_i = get_critical_value_trial(E2dNdE=E2dNdE, gamma=gamma, tr=tr, tr_inj=tr_inj, bias_corr_func=bias_corr_func, E0=E0, unit=unit, seed=i, TRUTH=TRUTH)\n",
    "        tau_values.append(tau_values_i)\n",
    "    return tau_values\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "def run_critical_value_trials_for_one_gamma(\n",
    "            n_trials, E2dNdE_list, gamma, key, add_systematics=False, TRUTH=False,\n",
    "            bias_corr_funcs=None, min_red_factor=0.02, max_k=3,\n",
    "            E0=100, unit=1e3, seed=0, cpus=20,\n",
    "        ):\n",
    "    \n",
    "    if bias_corr_funcs is not None:\n",
    "        bias_corr_func = bias_corr_funcs[key]\n",
    "    else:\n",
    "        bias_corr_func = None\n",
    "        \n",
    "    tau_values = [[] for norm in E2dNdE_list]\n",
    "    seed_values = list(range(seed, seed + n_trials))\n",
    "    \n",
    "    if add_systematics:\n",
    "        arg_list = [(i, E2dNdE_list, gamma, key, E0, unit, bias_corr_func, i, min_red_factor, max_k, TRUTH) \n",
    "                    for i in seed_values]\n",
    "    else:\n",
    "        arg_list = [(i, E2dNdE_list, gamma, key, E0, unit, bias_corr_func, None, min_red_factor, max_k, TRUTH) \n",
    "                    for i in seed_values]\n",
    "\n",
    "    if cpus > 1:\n",
    "        print('Running pool with {} cpus'.format(cpus))\n",
    "        \n",
    "        with Pool(cpus) as p:\n",
    "            tau_values_map = list(tqdm(p.imap(compute_trial_i, arg_list), total=n_trials))\n",
    "        print('tau_values_map.shape', np.array(tau_values_map).shape)\n",
    "        for j, tau_values_i in enumerate(tau_values_map):\n",
    "            for i, values in enumerate(tau_values_i):\n",
    "                tau_values[i].append(values)\n",
    "        p.close()\n",
    "    else:\n",
    "        for args in tqdm(arg_list, total=n_trials):\n",
    "            tau_values_i = compute_trial_i(args)\n",
    "            for i, values in enumerate(tau_values_i):\n",
    "                tau_values[i].append(values)\n",
    "            \n",
    "    return np.array(tau_values)\n",
    "\n",
    "def run_critical_value_trials(n_trials, E2dNdE_list, gamma_list, key, **kwargs):\n",
    "    tau_matrix = []\n",
    "    for gamma in gamma_list:\n",
    "        print('  ... runnning for gamma = {:3.2f}'.format(gamma))\n",
    "        tau_values = run_critical_value_trials_for_one_gamma(\n",
    "            n_trials=n_trials, E2dNdE_list=E2dNdE_list, gamma=gamma, key=key, **kwargs\n",
    "        )\n",
    "        tau_matrix.append(tau_values)\n",
    "    return np.array(tau_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-lodge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed = 0  # GP paper uses: seeds x-y\n",
    "n_trials = 100  # GP paper uses: N trials in total\n",
    "apply_correction = True # GP paper uses: True\n",
    "add_systematics = True # GP paper uses: True\n",
    "min_red_factor = 0.02 # GP paper uses: 0.02\n",
    "max_k = 3 # GP paper uses: 3\n",
    "\n",
    "cpus = 25\n",
    "recalculate = False\n",
    "\n",
    "E2dNdE_dict = {\n",
    "    'unid': np.linspace(1e-13, 13e-12, 50), # GP paper uses: np.linspace(1e-13, 13e-12, 50)\n",
    "    'snr': np.linspace(1e-13, 16e-12, 61), # GP paper uses: np.linspace(1e-13, 16e-12, 61)\n",
    "    'pwn': np.linspace(1e-13, 12e-12, 45), # GP paper uses: np.linspace(1e-13, 12e-12, 45)\n",
    "}\n",
    "gamma_dict = {\n",
    "    'unid': np.linspace(2.2, 3.35, 24), # GP paper uses: np.linspace(2.2, 3.35, 24)\n",
    "    'snr': np.linspace(2.1, 3.35, 26), # GP paper uses: np.linspace(2.1, 3.35, 26)\n",
    "    'pwn': np.linspace(2.45, 3.55, 23), # GP paper uses: np.linspace(2.45, 3.55, 23)\n",
    "}\n",
    "\n",
    "    \n",
    "tau_dict = {}\n",
    "for key in E2dNdE_dict.keys():\n",
    "        \n",
    "    print('Running {} trials for {} with {} normalizations and {} gammas'.format(\n",
    "        n_trials, key, len(E2dNdE_dict[key]), len(gamma_dict[key])))\n",
    "    if apply_correction:\n",
    "        print('Applying correction')\n",
    "        bias_corr_funcs_kw = bias_corr_funcs\n",
    "    else:\n",
    "        bias_corr_funcs_kw = None\n",
    "    print('Adding Systematic:', add_systematics)\n",
    "    \n",
    "    if add_systematics:\n",
    "        sys_str = '{}_red_{:0.3f}_k_{}'.format(add_systematics, min_red_factor, max_k)\n",
    "    else:\n",
    "        sys_str = '{}'.format(add_systematics)\n",
    "        \n",
    "    file_path = os.path.join(plot_dir, 'trials_{}_corr_{}_sys_{}_seeds_{}_{}.pkl'.format(\n",
    "        key, apply_correction, sys_str, seed, seed+n_trials))\n",
    "    \n",
    "    if not os.path.exists(file_path) or recalculate:\n",
    "        tau_matrix = run_critical_value_trials(\n",
    "            n_trials, seed=seed, E2dNdE_list=E2dNdE_dict[key], gamma_list=gamma_dict[key], \n",
    "            key=key, add_systematics=add_systematics, cpus=cpus, bias_corr_funcs=bias_corr_funcs_kw)\n",
    "        \n",
    "        # save trials\n",
    "        with open(file_path, 'wb') as f:\n",
    "            seeds = list(range(seed, seed+n_trials))\n",
    "            pickle.dump((E2dNdE_dict, gamma_dict, tau_matrix, seeds), f, protocol=-1)\n",
    "    else:\n",
    "        print('Skipping because file already exists...')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-august",
   "metadata": {},
   "source": [
    "#### Load trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "tau_dict = {}\n",
    "for key in ['unid', 'snr', 'pwn']:\n",
    "#for key in ['unid']:\n",
    "    \n",
    "    print('Loading trials for {} with {} normalizations and {} gammas'.format(\n",
    "        key, len(E2dNdE_dict[key]), len(gamma_dict[key])))\n",
    "    key_s = (key, apply_correction)\n",
    "    \n",
    "    if key_s not in tau_dict:\n",
    "        tau_dict[key_s] = {gamma: {norm: [] for norm in E2dNdE_dict[key]} for gamma in gamma_dict[key]}\n",
    "    \n",
    "    # find a list of files\n",
    "    if add_systematics:\n",
    "        sys_str = '{}_red_{:0.3f}_k_{}'.format(add_systematics, min_red_factor, max_k)\n",
    "    else:\n",
    "        sys_str = '{}'.format(add_systematics)\n",
    "        \n",
    "    file_pattern = os.path.join(plot_dir, 'trials_{}_corr_{}_sys_{}_seeds_*_*.pkl'.format(\n",
    "        key, apply_correction, sys_str))\n",
    "    file_list = sorted(glob.glob(file_pattern))\n",
    "    print('Found {} files...'.format(len(file_list)))\n",
    "    \n",
    "    # load files and check for overlapping seeds\n",
    "    seed_values = set([])\n",
    "    for file_i in file_list:\n",
    "        with open(file_i, 'rb') as handle:\n",
    "            E2dNdE_dict_loaded, gamma_dict_loaded, tau_matrix, seeds = pickle.load(handle)\n",
    "        \n",
    "        # make sure model norms match\n",
    "        assert np.all([k in E2dNdE_dict for k in E2dNdE_dict_loaded.keys()])\n",
    "        for k, norms in E2dNdE_dict_loaded.items():\n",
    "            assert np.allclose(norms, E2dNdE_dict[k]), (norms, E2dNdE_dict[k])\n",
    "        \n",
    "        # make sure gammas match\n",
    "        assert np.all([k in gamma_dict for k in gamma_dict_loaded.keys()])\n",
    "        for k, gammas in gamma_dict_loaded.items():\n",
    "            assert np.allclose(gammas, gamma_dict[k]), (gammas, gamma_dict[k])\n",
    "\n",
    "        # make sure seeds do not overlap\n",
    "        overlapping_seeds = seed_values.intersection(set(seeds))\n",
    "        if overlapping_seeds:\n",
    "            raise ValueError('Found overlapping seeds: {}!'.format(overlapping_seeds))\n",
    "        seed_values = seed_values.union(set(seeds))\n",
    "        \n",
    "        # append tau values from this file\n",
    "        for i, gamma in enumerate(gamma_dict[key]):\n",
    "            for j, E2dNdE in enumerate(E2dNdE_dict[key]):\n",
    "                tau_dict[key_s][gamma][E2dNdE].append(tau_matrix[i, j])\n",
    "    \n",
    "    # concatenate into single array\n",
    "    for i, gamma in enumerate(gamma_dict[key]):\n",
    "        for j, E2dNdE in enumerate(E2dNdE_dict[key]):\n",
    "            tau_dict[key_s][gamma][E2dNdE] = np.concatenate(tau_dict[key_s][gamma][E2dNdE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-oxford",
   "metadata": {},
   "source": [
    "#### Investigate boundary issues\n",
    "\n",
    "When applying correction, there is a boundary effect when ns goes beyond extrapolation end, at which the splines provide a constant value. In this range, the trials for the confidence value calculation are meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "E0 = 100\n",
    "\n",
    "def get_flux_norm(ns_values, catalog, gamma, E0=100, unit=1e3):\n",
    "    ns_values = np.atleast_1d(ns_values)\n",
    "    tr = get_catalog_tr(ana=ana, catalog=catalog, gamma=gamma)\n",
    "    norm_values = [tr.to_E2dNdE(ns, E0=E0, unit=unit) for ns in ns_values]\n",
    "    return np.array(norm_values)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "flux_gammas = np.linspace(2., 4, 20)\n",
    "flux_norms = []\n",
    "ns_values = [600, 2000, 3000]\n",
    "for gamma in tqdm(flux_gammas, total=len(flux_gammas)):\n",
    "    flux_norms.append(get_flux_norm(ns_values, catalog='unid', gamma=gamma, E0=E0))\n",
    "flux_norms = np.stack(flux_norms, axis=1)\n",
    "\n",
    "for i, ns in enumerate(ns_values):\n",
    "    ax.plot(flux_gammas, flux_norms[i], label='$n_s$ = {}'.format(ns))\n",
    "\n",
    "ax.set_ylim(0, 1.2e-11)\n",
    "ax.axvline(3.3, ls='--', color='0.7', label='$\\gamma$ = 3.3')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Spectral index $\\gamma$')\n",
    "units_label = ' $\\cdot 10^{-11}$'\n",
    "ax.set_ylabel(\n",
    "    '$\\mathrm{E}^2 \\cdot \\mathrm{dN/dE}$'+ units_label + ' at {:.0f} TeV'.format(E0)  + \n",
    "    ' [$\\mathrm{TeV} \\, \\mathrm{s}^{-1} \\, \\mathrm{cm}^{-2}$]')\n",
    "ax.grid()\n",
    "fig.savefig('{}/spline_boundary_effect.png'.format(plot_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-correspondence",
   "metadata": {},
   "source": [
    "#### Make critical value plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-somerset",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_correction(bias_corr_funcs, catalog):\n",
    "    if bias_corr_funcs is None:\n",
    "        print('Not applying correction')\n",
    "        apply_correction = False\n",
    "        bias_corr_func_kw = None\n",
    "    else:\n",
    "        print('Applying correction')\n",
    "        apply_correction = True\n",
    "        bias_corr_func_kw = bias_corr_funcs[catalog]\n",
    "    return apply_correction, bias_corr_func_kw\n",
    "    \n",
    "def compute_tau_observed_matrix(\n",
    "            catalog, gammas, norms,\n",
    "            bias_corr_funcs=None,\n",
    "            cpus=20,\n",
    "            E0=100, unit=1e3,\n",
    "        ):\n",
    "    \n",
    "    apply_correction, bias_corr_func_kw = check_correction(bias_corr_funcs, catalog)\n",
    "        \n",
    "    # create matrix of tau values: [gammas, norm, trials]\n",
    "    n_gammas = len(gammas)\n",
    "    n_norms = len(norms)\n",
    "    \n",
    "    tau_observed_matrix = np.zeros((n_gammas, n_norms))\n",
    "    \n",
    "    for i, gamma in enumerate(tqdm(gammas, total=n_gammas)):\n",
    "                           \n",
    "        \n",
    "        # get trial runner\n",
    "        tr = get_catalog_tr(ana=ana, catalog=catalog, gamma=gamma)\n",
    "        \n",
    "        arg_list = [(42, [E2dNdE], gamma, catalog, E0, unit, bias_corr_func_kw, None, min_red_factor, max_k, True) \n",
    "                    for E2dNdE in norms]\n",
    "        if cpus > 1:\n",
    "            with Pool(cpus) as p:\n",
    "                tau_observed_i = list(tqdm(p.imap(compute_trial_i, arg_list), total=len(arg_list)))\n",
    "            tau_observed_matrix[i] =  np.squeeze(tau_observed_i)\n",
    "        else:            \n",
    "            for j, norm in enumerate(tqdm(norms, total=n_norms)):\n",
    "\n",
    "                # get tau value for observed data\n",
    "                tau_observed_i = get_critical_value_trial(\n",
    "                    E2dNdE=norm, gamma=gamma, tr=tr, tr_inj=tr, E0=E0, unit=unit, seed=42, TRUTH=True, \n",
    "                    bias_corr_func=bias_corr_func_kw,\n",
    "                )\n",
    "                tau_observed_matrix[i, j] =  tau_observed_i\n",
    "            \n",
    "    return tau_observed_matrix, gammas, norms\n",
    "\n",
    "\n",
    "def make_critical_value_plot(\n",
    "            tau_dict, catalog,\n",
    "            bias_corr_funcs=None,\n",
    "            confidence_levels=[0.68, 0.9, 0.95],\n",
    "            E0=100, unit=1e3,\n",
    "            mask_ns_region_above=2000,\n",
    "        ):\n",
    "    \n",
    "    apply_correction, bias_corr_func_kw = check_correction(bias_corr_funcs, catalog)\n",
    "        \n",
    "    key_s = (catalog, apply_correction)\n",
    "    tau_dict_i = tau_dict[key_s]\n",
    "    \n",
    "    # create matrix of tau values: [gammas, norm, trials]\n",
    "    gammas = np.sort(list(tau_dict_i.keys()))\n",
    "    norms = np.sort(list(tau_dict_i[gammas[0]].keys()))\n",
    "    \n",
    "    n_gammas = len(gammas)\n",
    "    n_norms = len(norms)\n",
    "    n_trial = len(tau_dict_i[gammas[0]][norms[0]])\n",
    "    \n",
    "    tau_matrix = np.zeros((n_gammas, n_norms, n_trial))\n",
    "    pval_matrix = np.zeros((n_gammas, n_norms))\n",
    "    \n",
    "    # compute observed tau for each bin\n",
    "    tau_observed_matrix, _, _ = compute_tau_observed_matrix(\n",
    "        catalog=catalog, bias_corr_funcs=bias_corr_funcs, \n",
    "        gammas=gammas, norms=norms,\n",
    "    )\n",
    "    \n",
    "    for i, gamma in enumerate(gammas):\n",
    "        \n",
    "        if mask_ns_region_above is None:\n",
    "            valid_region = np.ones_like(norms, dtype=bool)\n",
    "        else:\n",
    "            norm_bound = get_flux_norm(\n",
    "                ns_values=mask_ns_region_above, catalog=catalog, gamma=gamma, E0=E0, unit=unit)\n",
    "            valid_region = np.array([n < norm_bound for n in norms])\n",
    "            \n",
    "        for j, (norm, valid) in enumerate(zip(norms, valid_region)):\n",
    "            if valid:\n",
    "                tau_matrix[i, j, :] = tau_dict_i[gamma][norm]\n",
    "                pval_matrix[i, j] = np.sum(tau_matrix[i, j] < tau_observed_matrix[i, j]) * 1. / n_trial\n",
    "            else:\n",
    "                tau_matrix[i, j, :] = np.nan\n",
    "                pval_matrix[i, j] = np.nan\n",
    "            \n",
    "    return tau_matrix, tau_observed_matrix, pval_matrix, gammas, norms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-province",
   "metadata": {},
   "source": [
    "#### Compute Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-flash",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrices_dict = {}\n",
    "for catalog in ['unid', 'pwn', 'snr']:\n",
    "#for catalog in ['unid']:\n",
    "    tau_matrix, tau_observed_matrix, pval_matrix, gammas, norms = (\n",
    "        make_critical_value_plot(tau_dict=tau_dict, catalog=catalog, bias_corr_funcs=bias_corr_funcs)\n",
    "    )\n",
    "    \n",
    "    matrices_dict[catalog] = {\n",
    "        'tau_matrix': tau_matrix, \n",
    "        'tau_observed_matrix': tau_observed_matrix, \n",
    "        'pval_matrix': pval_matrix, \n",
    "        'gammas': gammas, \n",
    "        'norms': norms,\n",
    "    }\n",
    "    \n",
    "    if False:\n",
    "        tau_observed_matrix_full, gammas_full, norms_full = compute_tau_observed_matrix(\n",
    "            catalog=catalog, bias_corr_funcs=bias_corr_funcs, \n",
    "            gammas=np.linspace(2.2, 3.35, 2), norms=np.linspace(1e-13, 13e-12, 2),\n",
    "            cpus=30,\n",
    "        )\n",
    "        \n",
    "        matrices_dict[catalog]['tau_observed_matrix_full'] = tau_observed_matrix_full\n",
    "        matrices_dict[catalog]['gammas_full'] = gammas_full\n",
    "        matrices_dict[catalog]['norms_full'] = norms_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-parade",
   "metadata": {},
   "source": [
    "#### Add another Wilk's based Contour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for catalog in ['unid', 'pwn', 'snr']:\n",
    "for catalog in ['pwn', 'snr']:\n",
    "    if False:\n",
    "        tau_observed_matrix_full, gammas_full, norms_full = compute_tau_observed_matrix(\n",
    "            catalog=catalog, bias_corr_funcs=bias_corr_funcs, \n",
    "            gammas=np.linspace(2.2, 3.7, 24), norms=np.linspace(1e-13, 18e-12, 50),\n",
    "            cpus=30,\n",
    "        )\n",
    "        \n",
    "        matrices_dict[catalog]['tau_observed_matrix_full'] = tau_observed_matrix_full\n",
    "        matrices_dict[catalog]['gammas_full'] = gammas_full\n",
    "        matrices_dict[catalog]['norms_full'] = norms_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-metabolism",
   "metadata": {},
   "source": [
    "#### Make Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from scipy.interpolate import UnivariateSpline, interp2d\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "def draw_critical_value_plot(\n",
    "            pval_matrix, gammas, norms, res_dict,\n",
    "            E0=100, unit=1e3, unit_multiplier=1e-11,\n",
    "            fig=None, ax=None,\n",
    "            confidence_levels=[0.68, 0.9, 0.95],\n",
    "            bounds=np.linspace(0, 1, 21),\n",
    "            confidence_colors=['1.', '1.', '1.'],\n",
    "            confidence_ls=['-', '--', '-.'],\n",
    "            confidence_labels=None,\n",
    "            cb_label=r'Fraction of $\\tau_\\mathrm{fit} < \\tau_\\mathrm{observed}$',\n",
    "            cax=None,\n",
    "            xlabel='Spectral index $\\gamma$',\n",
    "            ylabel='<FILL>',\n",
    "            plot_best_fit=True, plot_minimum=True,\n",
    "        ):\n",
    "    cmap = plt.cm.viridis\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)[::-1]]\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "        'Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "    # define the bins and normalize\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    if unit_multiplier == 1e-11:\n",
    "        units_label = ' $\\cdot 10^{-11}$'\n",
    "        norms = np.array(norms) / 1e-11\n",
    "    elif unit_multiplier is None:\n",
    "        units_label = ''\n",
    "    else:\n",
    "        raise ValueError(unit_multiplier)\n",
    "    \n",
    "    gamma_width = np.diff(gammas) * 0.5\n",
    "    assert np.allclose(gamma_width, gamma_width[0]), gamma_width\n",
    "    gamma_width = gamma_width[0]\n",
    "    \n",
    "    norm_width = np.diff(norms) * 0.5\n",
    "    assert np.allclose(norm_width, norm_width[0]), gamma_width\n",
    "    norm_width = norm_width[0]\n",
    "    \n",
    "    x_bins = np.array([g - gamma_width for g in gammas] + [gammas[-1] + gamma_width])\n",
    "    y_bins = np.array([n - norm_width for n in norms] + [norms[-1] + norm_width])\n",
    "    grid_x, grid_y = np.meshgrid(x_bins, y_bins)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    pc_gamma = ax.pcolormesh(grid_x, grid_y, pval_matrix.T, cmap=cmap, norm=norm)\n",
    "    contours = ax.contour(\n",
    "        gammas, norms, pval_matrix.T, levels=confidence_levels, colors=confidence_colors, \n",
    "        linestyles=confidence_ls,\n",
    "    )\n",
    "    ax.clabel(contours, inline=1, fontsize=10)\n",
    "    if confidence_labels is not None:\n",
    "        for color, ls, label, in zip(confidence_colors, confidence_ls, confidence_labels):\n",
    "            ax.plot(np.nan, np.nan, color=color, ls=ls, label=label)\n",
    "    if cax is None:\n",
    "        cb_gamma = fig.colorbar(pc_gamma, ax=ax)\n",
    "    else:\n",
    "        cb_gamma = fig.colorbar(pc_gamma, cax=cax)\n",
    "    cb_gamma.set_label(cb_label)\n",
    "    \n",
    "    # plot analysis best fit\n",
    "    tr = get_catalog_tr(ana=ana, catalog=catalog, gamma=res_dict[catalog][2])\n",
    "    best_fit_norm = tr.to_E2dNdE(res_dict[catalog][1], E0=E0, unit=unit) / unit_multiplier\n",
    "    if plot_best_fit:\n",
    "        ax.scatter(\n",
    "            res_dict[catalog][2], best_fit_norm, marker='x', color='red', \n",
    "            label='$\\gamma$: {:3.2f} | $\\Phi$: {:3.2e} | uncorrected'.format(\n",
    "                res_dict[catalog][2], best_fit_norm * unit_multiplier),\n",
    "        )\n",
    "    \n",
    "    # compute minimum\n",
    "    spline2d = interp2d(x=gammas, y=norms, z=pval_matrix.T, kind='cubic')\n",
    "    \n",
    "    def loss(args):\n",
    "        gamma, norm = args\n",
    "        return spline2d(gamma, norm)\n",
    "    res = optimize.minimize(loss, x0=(res_dict[catalog][2], best_fit_norm))\n",
    "    min_gamma = res.x[0]\n",
    "    min_norm = res.x[1] * unit_multiplier\n",
    "\n",
    "    if plot_minimum:\n",
    "        ax.scatter(\n",
    "            res.x[0], res.x[1], marker='^', color='white', \n",
    "            label='$\\gamma$: {:3.2f} | $\\Phi$: {:3.2e} | bias-corrected'.format(min_gamma, min_norm),\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    if ylabel == '<FILL>':\n",
    "        ylabel = (\n",
    "            '$\\mathrm{E}^2 \\cdot \\mathrm{dN/dE}$'+ units_label + ' at {:.0f} TeV'.format(E0)  + \n",
    "            ' [$\\mathrm{TeV} \\, \\mathrm{s}^{-1} \\, \\mathrm{cm}^{-2}$]')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    return fig, ax, min_gamma, min_norm\n",
    "\n",
    "\n",
    "def find_intersection2(spline, x0, cl):\n",
    "    \"\"\"Find intersection points based on splines\"\"\"\n",
    "    x0 = np.atleast_1d(x0)\n",
    "    def fun(x):\n",
    "        return spline(x) - cl\n",
    "    sol = optimize.root(fun, x0=x0)\n",
    "    return np.sort(np.unique(sol.x.round(decimals=4)))\n",
    "\n",
    "def find_intersection(spline, x0, x_bounds, cl, n_steps=10000, eps=1e-4):\n",
    "    \"\"\"Find intersection points based on splines\n",
    "    \n",
    "    Brute force in provided range, assumes one intersection\n",
    "    is left and one right of x0\n",
    "    \"\"\"\n",
    "    x_bounds = np.sort(x_bounds)\n",
    "    intersections = []\n",
    "    for bound in x_bounds:\n",
    "        x = np.linspace(bound, x0, n_steps)\n",
    "        y = (spline(x) - cl)**2\n",
    "        y_min = np.min(y)\n",
    "        if y_min < eps:\n",
    "            intersections.append(x[np.argmin(y)])\n",
    "        else:\n",
    "            intersections.append(np.nan)\n",
    "    return np.sort(intersections)\n",
    "\n",
    "def draw_combined_confidence_plot(\n",
    "            pval_matrix, gammas, norms, res_dict,\n",
    "            E0=100, unit=1e3, unit_multiplier=1e-11,\n",
    "            fig=None, ax=None, figsize=(12, 9),\n",
    "            confidence_levels=[0.68, 0.9, 0.95],\n",
    "            confidence_names=['68% CL', '90% CL', '95% CL'],\n",
    "            bounds=np.linspace(0, 1, 21),\n",
    "            confidence_colors=['1.', '1.', '1.'],\n",
    "            confidence_ls=['-', '--', '-.'],\n",
    "            confidence_labels=None,\n",
    "            cb_label=r'Fraction of $\\tau_\\mathrm{fit} < \\tau_\\mathrm{obs}$',\n",
    "            xlabel='Spectral index $\\gamma$',\n",
    "            plot_spline=True,\n",
    "            replace_pval_matrix_nans=1.0,\n",
    "        ):\n",
    "    \n",
    "    if replace_pval_matrix_nans is not None:\n",
    "        pval_matrix = np.array(pval_matrix)\n",
    "        pval_matrix[~np.isfinite(pval_matrix)] = 1.0\n",
    "        \n",
    "    if unit_multiplier == 1e-11:\n",
    "        units_label = ' $\\cdot 10^{-11}$'\n",
    "        norms_mod = np.array(norms) / 1e-11\n",
    "    elif unit_multiplier is None:\n",
    "        units_label = ''\n",
    "        norms_mod = np.array(norms)\n",
    "    else:\n",
    "        raise ValueError(unit_multiplier)\n",
    "        \n",
    "    if unit == 1e3:\n",
    "        ylabel = (\n",
    "            '$\\mathrm{E}^2 \\cdot \\mathrm{dN/dE}$'+ units_label + ' at {:.0f} TeV'.format(E0)  + \n",
    "            ' [$\\mathrm{TeV} \\, \\mathrm{s}^{-1} \\, \\mathrm{cm}^{-2}$]')\n",
    "    else:\n",
    "        raise ValueError(unit)\n",
    "        \n",
    "    norm_bounds = (np.min(norms_mod), np.max(norms_mod))\n",
    "    gamma_bounds = (np.min(gammas), np.max(gammas))\n",
    "        \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 3, width_ratios=[3, 10, 0.5], height_ratios=[10, 3])\n",
    "    ax_2d = fig.add_subplot(gs[0, 1])\n",
    "    ax_cb = fig.add_subplot(gs[0, 2])\n",
    "    ax_norm = fig.add_subplot(gs[0, 0], sharey=ax_2d)\n",
    "    ax_gamma = fig.add_subplot(gs[1, 1], sharex=ax_2d)\n",
    "    ax_text = fig.add_subplot(gs[1, 0])\n",
    "    axes = [ax_2d, ax_gamma, ax_norm, ax_cb, ax_text]\n",
    "    \n",
    "    ax_text.axis('off')\n",
    "    result_str = ''\n",
    "    \n",
    "    plt.setp(ax_2d.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax_2d.get_xticklabels(), visible=False)\n",
    "    #ax_2d.axes.xaxis.set_ticklabels([]) # note due to share axis, this will apply to all\n",
    "    #ax_2d.axes.yaxis.set_ticklabels([]) # note due to share axis, this will apply to all\n",
    "    \n",
    "    _, _, min_gamma, min_norm = draw_critical_value_plot(\n",
    "        pval_matrix=pval_matrix, \n",
    "        gammas=gammas, \n",
    "        norms=norms, \n",
    "        res_dict=res_dict,\n",
    "        E0=E0, unit=unit, unit_multiplier=unit_multiplier,\n",
    "        fig=fig, ax=ax_2d, cax=ax_cb,\n",
    "        confidence_levels=confidence_levels,\n",
    "        bounds=bounds,\n",
    "        confidence_colors=confidence_colors,\n",
    "        confidence_ls=confidence_ls,\n",
    "        confidence_labels=confidence_labels,\n",
    "        cb_label=cb_label,\n",
    "        xlabel=None,\n",
    "        ylabel=None,\n",
    "    )\n",
    "    ax_2d.legend()\n",
    "    \n",
    "    \n",
    "    # ----------------------------\n",
    "    # 1D contour for normalization\n",
    "    # ----------------------------\n",
    "    matrix_norm = np.min(pval_matrix, axis=0)\n",
    "    ax_norm.plot(matrix_norm, norms_mod)\n",
    "    ax_norm.set_xlabel(cb_label)\n",
    "    \n",
    "    # fit spline\n",
    "    spline = UnivariateSpline(x=norms_mod, y=matrix_norm, s=len(norms)*0.0002)\n",
    "    if plot_spline:\n",
    "        ax_norm.plot(spline(norms_mod), norms_mod, ls='--', label='Spline-Fit')\n",
    "    \n",
    "    # find intersections\n",
    "    result_str += '$\\Phi_\\mathrm{corrected}$ = ' + '{:3.2e}\\n'.format(min_norm)\n",
    "    for cl, ls, cname in zip(confidence_levels, confidence_ls, confidence_names):\n",
    "        intersections = find_intersection(\n",
    "            spline=spline, x0=min_norm / unit_multiplier, x_bounds=norm_bounds, cl=cl)\n",
    "        for intersection_i in intersections:\n",
    "            ax_norm.axhline(intersection_i, color='0.8', ls=ls)\n",
    "        result_str += '  {} | +{:3.2e} -{:3.2e}\\n'.format(\n",
    "            cname, \n",
    "            (intersections[1] * unit_multiplier - min_norm),\n",
    "            (min_norm - intersections[0]*unit_multiplier),\n",
    "        )\n",
    "        ax_norm.axhline(\n",
    "            np.inf, color='0.8', ls=ls, label='{}: [{:3.2e}, {:3.2e}]'.format(\n",
    "                cname, *intersections * unit_multiplier))\n",
    "    ax_norm.legend(loc='center right', fontsize=6)\n",
    "    result_str += '\\n'\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 1D contour for gamma\n",
    "    # ----------------------------\n",
    "    matrix_gamma = np.min(pval_matrix, axis=1)\n",
    "    ax_gamma.plot(gammas, matrix_gamma)\n",
    "    ax_gamma.set_ylabel(cb_label)\n",
    "    \n",
    "    # fit spline\n",
    "    spline = UnivariateSpline(x=gammas, y=matrix_gamma, s=len(norms)*0.00005)\n",
    "    if plot_spline:\n",
    "        ax_gamma.plot(gammas, spline(gammas), ls='--', label='Spline-Fit')\n",
    "    \n",
    "    # find intersections\n",
    "    result_str += '$\\gamma_\\mathrm{corrected}$ = ' + '{:3.2f}\\n'.format(min_gamma)\n",
    "    for cl, ls, cname in zip(confidence_levels, confidence_ls, confidence_names):\n",
    "        intersections = find_intersection(spline=spline, x0=min_gamma, x_bounds=gamma_bounds, cl=cl)\n",
    "        for intersection_i in intersections:\n",
    "            ax_gamma.axvline(intersection_i, color='0.8', ls=ls)\n",
    "        result_str += '  {} | +{:3.2f} -{:3.2f}\\n'.format(\n",
    "            cname, intersections[1] - min_gamma, min_gamma - intersections[0],\n",
    "        )\n",
    "        ax_gamma.axvline(\n",
    "            np.inf, color='0.8', ls=ls, label='{}: [{:3.2f}, {:3.2f}]'.format(\n",
    "                cname, *intersections))\n",
    "    ax_gamma.legend(loc='upper center', fontsize=6)\n",
    "    # ----------------------------\n",
    "    \n",
    "    ax_text.text( \n",
    "        -.2, 0, result_str,\n",
    "        ha='left', va='bottom',\n",
    "        transform=ax_text.transAxes,\n",
    "        fontsize=8,\n",
    "        bbox=dict(facecolor='none', edgecolor='0.3', boxstyle='round,pad=1'),\n",
    "    )\n",
    "    \n",
    "    ax_gamma.set_xlabel(xlabel)\n",
    "    ax_norm.set_ylabel(ylabel)\n",
    "    ax_norm.set_ylim(norm_bounds)\n",
    "    ax_gamma.set_xlim(gamma_bounds)\n",
    "    \n",
    "    return fig, axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_correction:\n",
    "    bias_corr_funcs_kw = bias_corr_funcs\n",
    "    file_suffix = '_corrected'\n",
    "else:\n",
    "    bias_corr_funcs_kw = None\n",
    "    file_suffix = ''\n",
    "\n",
    "if add_systematics:\n",
    "    file_suffix += '_sys_red_{:1.3f}_k_{}'.format(min_red_factor, max_k)\n",
    "\n",
    "for catalog, res in matrices_dict.items():\n",
    "    fig, axes = draw_combined_confidence_plot(\n",
    "        pval_matrix=res['pval_matrix'], gammas=res['gammas'], norms=res['norms'], res_dict=res_dict)\n",
    "    axes[0].set_title('Stacking Catalog: {}'.format(catalog))\n",
    "    fig.savefig('{}/confidence_2d_{}{}.png'.format(plot_dir, catalog, file_suffix))\n",
    "\n",
    "    fig, axes = draw_combined_confidence_plot(\n",
    "        pval_matrix=res['tau_observed_matrix'], gammas=res['gammas'], norms=res['norms'],\n",
    "        bounds=np.linspace(0, 10, 21), confidence_levels=[2.28, 4.6, 6.],\n",
    "        confidence_labels=[\"Wilk's 68%\", \"Wilk's 90%\", \"Wilk's 95%\"],\n",
    "        cb_label=r'Test-statistic $\\tau$',\n",
    "        res_dict=res_dict,\n",
    "    )\n",
    "    axes[0].set_title(\"Stacking Catalog: {} | Utilizing Wilk's Theorem\".format(catalog))\n",
    "    axes[0].legend()\n",
    "    fig.savefig('{}/confidence_2d_{}{}_wilks.png'.format(plot_dir, catalog, file_suffix))\n",
    "    \n",
    "    if 'tau_observed_matrix_full' in res:\n",
    "        fig, axes = draw_combined_confidence_plot(\n",
    "            pval_matrix=res['tau_observed_matrix_full'], gammas=res['gammas_full'], norms=res['norms_full'],\n",
    "            bounds=np.linspace(0, 10, 21), confidence_levels=[2.28, 4.6, 6.],\n",
    "            confidence_labels=[\"Wilk's 68%\", \"Wilk's 90%\", \"Wilk's 95%\"],\n",
    "            cb_label=r'Test-statistic $\\tau$',\n",
    "            res_dict=res_dict,\n",
    "        )\n",
    "        axes[0].legend()\n",
    "        axes[0].set_title(\"Stacking Catalog: {} | Utilizing Wilk's Theorem\".format(catalog))\n",
    "        fig.savefig('{}/confidence_2d_{}{}_wilks_full.png'.format(plot_dir, catalog, file_suffix))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-brook",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-custody",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 20, 10)\n",
    "bins = 10\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "for i, gamma in enumerate(gammas):\n",
    "    if i < len(gammas) - 2: continue\n",
    "    for j, norm in enumerate(norms):\n",
    "        if j % 5 != 0: continue\n",
    "        ax.hist(\n",
    "            tau_matrix[i, j], bins=bins, \n",
    "            label='$\\gamma$ = {:3.2f} | $\\Phi$ = {:3.3e}'.format(gamma, norm), \n",
    "            histtype='step',\n",
    "        )\n",
    "ax.legend(fontsize=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-tribe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-insulation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-plaza",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-walker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-cambodia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.3_py3-v4.1.0_csky",
   "language": "python",
   "name": "tensorflow2.3_py3-v4.1.0_csky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
